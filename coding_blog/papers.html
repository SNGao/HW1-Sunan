<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Feed | Coding Blog</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <style>
        .papers-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 100px 20px 40px;
        }
        .papers-header {
            text-align: center;
            margin-bottom: 40px;
        }
        .papers-header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .update-info {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 8px;
        }
        .auto-update-info {
            color: #22c55e;
            font-size: 0.85rem;
            margin-bottom: 15px;
        }
        .config-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-bottom: 10px;
        }
        .config-hint code {
            background: #334155;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: var(--font-code);
            color: #f472b6;
        }
        .search-box {
            max-width: 500px;
            margin: 20px auto;
        }
        .search-box input {
            width: 100%;
            padding: 12px 20px;
            border: 2px solid var(--border);
            border-radius: 25px;
            background: var(--card-bg);
            color: var(--text-primary);
            font-size: 1rem;
            outline: none;
            transition: border-color 0.3s;
        }
        .search-box input:focus {
            border-color: var(--primary);
        }
        .search-box input::placeholder {
            color: var(--text-muted);
        }
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 25px;
        }
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 25px;
            transition: all 0.3s ease;
        }
        .paper-card:hover {
            border-color: var(--primary);
            transform: translateY(-3px);
            box-shadow: 0 10px 30px rgba(236, 72, 153, 0.15);
        }
        .paper-card.hidden {
            display: none;
        }
        .paper-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-bottom: 10px;
            line-height: 1.4;
        }
        .paper-title a {
            color: inherit;
            text-decoration: none;
            transition: color 0.3s;
        }
        .paper-title a:hover {
            color: var(--primary);
        }
        .paper-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 12px;
        }
        .paper-date {
            background: var(--primary);
            color: white;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
            font-weight: 500;
        }
        .paper-category {
            background: #334155;
            color: #e2e8f0;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
        }
        .paper-authors {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 12px;
            line-height: 1.5;
        }
        .paper-abstract {
            color: #94a3b8;
            font-size: 0.9rem;
            line-height: 1.7;
            margin-bottom: 15px;
        }
        .paper-abstract.expanded {
            max-height: none;
        }
        .expand-btn {
            background: none;
            border: none;
            color: var(--primary);
            cursor: pointer;
            font-size: 0.85rem;
            padding: 0;
            margin-bottom: 15px;
        }
        .expand-btn:hover {
            text-decoration: underline;
        }
        .paper-actions {
            display: flex;
            gap: 10px;
        }
        .pdf-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            transition: all 0.3s;
        }
        .pdf-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 15px rgba(236, 72, 153, 0.3);
        }
        .arxiv-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: var(--surface);
            color: var(--text-primary);
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            border: 1px solid var(--border);
            transition: all 0.3s;
        }
        .arxiv-btn:hover {
            border-color: var(--primary);
            color: var(--primary);
        }
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: var(--text-muted);
        }
        .no-results h3 {
            font-size: 1.5rem;
            margin-bottom: 10px;
            color: var(--text-secondary);
        }
        .keywords-info {
            background: #1e293b;
            padding: 20px;
            border-radius: 12px;
            margin-bottom: 30px;
            text-align: center;
            border: 1px solid #334155;
        }
        .keywords-info > span {
            color: #e2e8f0;
            font-size: 0.95rem;
            font-weight: 500;
        }
        .keyword-tag {
            display: inline-block;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: #ffffff;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin: 4px;
            text-shadow: 0 1px 2px rgba(0,0,0,0.2);
        }
        .keyword-editor {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #334155;
        }
        .keyword-input-group {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .keyword-input {
            padding: 10px 16px;
            border: 2px solid #334155;
            border-radius: 25px;
            background: #0f172a;
            color: #f8fafc;
            font-size: 0.9rem;
            width: 250px;
            outline: none;
            transition: border-color 0.3s;
        }
        .keyword-input:focus {
            border-color: #6366f1;
        }
        .keyword-input::placeholder {
            color: #64748b;
        }
        .fetch-btn {
            padding: 10px 24px;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: white;
            border: none;
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .fetch-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 20px rgba(99, 102, 241, 0.4);
        }
        .fetch-btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }
        .editor-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-top: 8px;
        }
        @media (max-width: 768px) {
            .papers-grid {
                grid-template-columns: 1fr;
            }
            .papers-header h1 {
                font-size: 1.8rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="index.html" class="nav-logo">
                <span class="logo-icon">ğŸ’»</span>
                <span class="logo-text">Coding Blog</span>
            </a>
            <ul class="nav-menu">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li><a href="index.html#projects" class="nav-link">Projects</a></li>
                <li><a href="papers.html" class="nav-link active">Papers</a></li>
                <li><a href="index.html#about" class="nav-link">About</a></li>
            </ul>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <div class="papers-container">
        <div class="papers-header">
            <h1>ğŸ“š arXiv Paper Feed</h1>
            <p class="update-info">Last updated: 2026-02-26 01:42:12 UTC</p>
            <p class="auto-update-info">â° Auto-updates daily at midnight UTC via GitHub Actions</p>
            <div class="keywords-info">
                <span>Current keywords: </span>
                <div id="currentKeywords">
                    <span class="keyword-tag">large language model</span><span class="keyword-tag">machine learning</span><span class="keyword-tag">biostatistics</span><span class="keyword-tag">deep learning</span>
                </div>
                <div class="keyword-editor">
                    <p class="editor-hint">ğŸ”„ Try different keywords (fetches live from arXiv):</p>
                    <div class="keyword-input-group">
                        <input type="text" id="customKeywords" class="keyword-input" 
                               placeholder="e.g., reinforcement learning, NLP" 
                               value="large language model, machine learning, biostatistics, deep learning">
                        <button onclick="fetchCustomPapers()" class="fetch-btn" id="fetchBtn">
                            Fetch Papers
                        </button>
                    </div>
                    <p class="editor-hint">Separate multiple keywords with commas</p>
                </div>
            </div>
            <div class="search-box">
                <input type="text" id="searchInput" placeholder="ğŸ” Filter papers by title, author, or abstract..." oninput="filterPapers()">
            </div>
        </div>

        <div class="papers-grid" id="papersGrid">

            <article class="paper-card" data-search="test-time training with kv binding is secretly linear attention junchen liu sven elflein or litany zan gojcic ruilong li test-time training (ttt) with kv binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. however, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. motivated by these findings, we revisit the formulation of ttt and show that a broad class of ttt architectures can be expressed as a form of learned linear attention operator. beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse ttt variants to a standard linear attention form. overall, our results reframe ttt not as test-time memorization, but as learned linear attention with enhanced representational capacity.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21204v1" target="_blank" rel="noopener">Test-Time Training with KV Binding Is Secretly Linear Attention</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Junchen Liu, Sven Elflein, Or Litany, Zan Gojcic, Ruilong Li</p>
                <p class="paper-abstract" id="abstract-0">Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis...</p>
                <button class="expand-btn" onclick="toggleAbstract(0, 'Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.', 'Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis...')" id="expand-btn-0">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21204v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21204v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="squint: fast visual reinforcement learning for sim-to-real robotics abdulaziz almuzairee henrik i. christensen visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. to address these challenges, we introduce squint, a visual soft actor critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. we evaluate on the so-101 task set, a new suite of eight manipulation tasks in maniskill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real so-101 robot. we train policies for 15 minutes on a single rtx 3090 gpu, with most tasks converging in under 6 minutes.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21203v1" target="_blank" rel="noopener">Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.RO</span><span class="paper-category">cs.CV</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Abdulaziz Almuzairee, Henrik I. Christensen</p>
                <p class="paper-abstract" id="abstract-1">Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown...</p>
                <button class="expand-btn" onclick="toggleAbstract(1, 'Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.', 'Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown...')" id="expand-btn-1">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21203v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21203v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="aletheia tackles firstproof autonomously tony feng junehyuk jung sang-hyun kim carlo pagano sergei gukov chiang-chiang tsai david woodruff adel javanmard aryan mokhtari dawsen hwang yuri chervonyi jonathan n. lee garrett bingham trieu h. trinh vahab mirrokni quoc v. le thang luong we report the performance of aletheia (feng et al., 2026b), a mathematics research agent powered by gemini 3 deep think, on the inaugural firstproof challenge. within the allowed timeframe of the challenge, aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on problem 8 (only). for full transparency, we explain our interpretation of firstproof and disclose details about our experiments as well as our evaluation. raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21201v1" target="_blank" rel="noopener">Aletheia tackles FirstProof autonomously</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.AI</span><span class="paper-category">cs.CL</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Tony Feng, Junehyuk Jung, Sang-hyun Kim, Carlo Pagano, Sergei Gukov <em>(+12 more)</em></p>
                <p class="paper-abstract" id="abstract-2">We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the...</p>
                <button class="expand-btn" onclick="toggleAbstract(2, 'We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.', 'We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the...')" id="expand-btn-2">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21201v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21201v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="learning from trials and errors: reflective test-time planning for embodied llms yining hong huang huang manling li li fei-fei jiajun wu yejin choi embodied llms endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. drawing upon human reflective practitioners, we introduce reflective test-time planning, which integrates two modes of reflection: \textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. we also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. experiments on our newly-designed long-horizon household benchmark and mujoco cupboard fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21198v1" target="_blank" rel="noopener">Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Yining Hong, Huang Huang, Manling Li, Li Fei-Fei, Jiajun Wu <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-3">Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather...</p>
                <button class="expand-btn" onclick="toggleAbstract(3, 'Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.', 'Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather...')" id="expand-btn-3">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21198v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21198v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="untied ulysses: memory-efficient context parallelism via headwise chunking ravi ghadia maksim abraham sergei vorobyov max ryabinin efficiently processing long sequences with transformer models usually requires splitting the computations across accelerators via context parallelism. the dominant approaches in this family of methods, such as ring attention or deepspeed ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. more advanced techniques, such as fully pipelined distributed transformer or activation offloading, can further extend the possible context length at the cost of training throughput. in this paper, we present upipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. this technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\%$ for 32b transformers, while matching previous context parallelism techniques in terms of training speed. upipe can support the context length of 5m tokens when training llama3-8b on a single 8$\times$h100 node, improving upon prior methods by over 25$\%$.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21196v1" target="_blank" rel="noopener">Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.DC</span>
                </div>
                <p class="paper-authors">Ravi Ghadia, Maksim Abraham, Sergei Vorobyov, Max Ryabinin</p>
                <p class="paper-abstract" id="abstract-4">Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of...</p>
                <button class="expand-btn" onclick="toggleAbstract(4, 'Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\times$H100 node, improving upon prior methods by over 25$\%$.', 'Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of...')" id="expand-btn-4">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21196v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21196v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="region of interest segmentation and morphological analysis for membranes in cryo-electron tomography xingyi cheng julien maufront aurÃ©lie di cicco daniÃ«l m. pelt manuela dezi daniel lÃ©vy cryo-electron tomography (cryo-et) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. identification of regions of interest (rois) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. in practice, however, rois are typically derived indirectly through full structure segmentation followed by post hoc analysis. this limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. here, we developed tomorois-surfora, a two step framework for direct, shape-agnostic roi segmentation and morphological surface analysis. tomorois performs deep learning-based roi segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. surfora processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. it supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-et due to the missing wedge effect. we demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. while demonstrated here on cryo-et membrane data, the combined approach is applicable to roi detection and surface analysis in broader scientific imaging contexts.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21195v1" target="_blank" rel="noopener">Region of Interest Segmentation and Morphological Analysis for Membranes in Cryo-Electron Tomography</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Xingyi Cheng, Julien Maufront, AurÃ©lie Di Cicco, DaniÃ«l M. Pelt, Manuela Dezi <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-5">Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest...</p>
                <button class="expand-btn" onclick="toggleAbstract(5, 'Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest (ROIs) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. In practice, however, ROIs are typically derived indirectly through full structure segmentation followed by post hoc analysis. This limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. Here, we developed TomoROIS-SurfORA, a two step framework for direct, shape-agnostic ROI segmentation and morphological surface analysis. TomoROIS performs deep learning-based ROI segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. SurfORA processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. It supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-ET due to the missing wedge effect. We demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. While demonstrated here on cryo-ET membrane data, the combined approach is applicable to ROI detection and surface analysis in broader scientific imaging contexts.', 'Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest...')" id="expand-btn-5">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21195v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21195v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="on data engineering for scaling llm terminal capabilities renjie pi grace lam mohammad shoeybi pooya jannaty bryan catanzaro wei ping despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. we address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) terminal-task-gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. our pipeline yields terminal-corpus, a large-scale open-source dataset for terminal tasks. using this dataset, we train nemotron-terminal, a family of models initialized from qwen3(8b, 14b, 32b) that achieve substantial gains on terminal-bench 2.0: nemotron-terminal-8b improves from 2.5% to 13.0% nemotron-terminal-14b improves from 4.0% to 20.2%, and nemotron-terminal-32b improves from 3.4% to 27.4%, matching the performance of significantly larger models. to accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21193v1" target="_blank" rel="noopener">On Data Engineering for Scaling LLM Terminal Capabilities</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Renjie Pi, Grace Lam, Mohammad Shoeybi, Pooya Jannaty, Bryan Catanzaro <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-6">Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this...</p>
                <button class="expand-btn" onclick="toggleAbstract(6, 'Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.', 'Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this...')" id="expand-btn-6">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21193v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21193v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="statistical query lower bounds for smoothed agnostic learning ilias diakonikolas daniel m. kane we study the complexity of smoothed agnostic learning, recently introduced by~\cite{ckkms24}, in which the learner competes with the best classifier in a target class under slight gaussian perturbations of the inputs. specifically, we focus on the prototypical task of agnostically learning halfspaces under subgaussian distributions in the smoothed model. the best known upper bound for this problem relies on $l_1$-polynomial regression and has complexity $d^{\tilde{o}(1/Ïƒ^2) \log(1/Îµ)}$, where $Ïƒ$ is the smoothing parameter and $Îµ$ is the excess error. our main result is a statistical query (sq) lower bound providing formal evidence that this upper bound is close to best possible. in more detail, we show that (even for gaussian marginals) any sq algorithm for smoothed agnostic learning of halfspaces requires complexity $d^{Ï‰(1/Ïƒ^{2}+\log(1/Îµ))}$. this is the first non-trivial lower bound on the complexity of this task and nearly matches the known upper bound. roughly speaking, we show that applying $l_1$-polynomial regression to a smoothed version of the function is essentially best possible. our techniques involve finding a moment-matching hard distribution by way of linear programming duality. this dual program corresponds exactly to finding a low-degree approximating polynomial to the smoothed version of the target function (which turns out to be the same condition required for the $l_1$-polynomial regression to work). our explicit sq lower bound then comes from proving lower bounds on this approximation degree for the class of halfspaces.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21191v1" target="_blank" rel="noopener">Statistical Query Lower Bounds for Smoothed Agnostic Learning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.DS</span><span class="paper-category">stat.ML</span>
                </div>
                <p class="paper-authors">Ilias Diakonikolas, Daniel M. Kane</p>
                <p class="paper-abstract" id="abstract-7">We study the complexity of smoothed agnostic learning, recently introduced by~\cite{CKKMS24}, in which the learner competes with the best classifier in a target class under slight Gaussian...</p>
                <button class="expand-btn" onclick="toggleAbstract(7, 'We study the complexity of smoothed agnostic learning, recently introduced by~\cite{CKKMS24}, in which the learner competes with the best classifier in a target class under slight Gaussian perturbations of the inputs. Specifically, we focus on the prototypical task of agnostically learning halfspaces under subgaussian distributions in the smoothed model. The best known upper bound for this problem relies on $L_1$-polynomial regression and has complexity $d^{\tilde{O}(1/Ïƒ^2) \log(1/Îµ)}$, where $Ïƒ$ is the smoothing parameter and $Îµ$ is the excess error. Our main result is a Statistical Query (SQ) lower bound providing formal evidence that this upper bound is close to best possible. In more detail, we show that (even for Gaussian marginals) any SQ algorithm for smoothed agnostic learning of halfspaces requires complexity $d^{Î©(1/Ïƒ^{2}+\log(1/Îµ))}$. This is the first non-trivial lower bound on the complexity of this task and nearly matches the known upper bound. Roughly speaking, we show that applying $L_1$-polynomial regression to a smoothed version of the function is essentially best possible. Our techniques involve finding a moment-matching hard distribution by way of linear programming duality. This dual program corresponds exactly to finding a low-degree approximating polynomial to the smoothed version of the target function (which turns out to be the same condition required for the $L_1$-polynomial regression to work). Our explicit SQ lower bound then comes from proving lower bounds on this approximation degree for the class of halfspaces.', 'We study the complexity of smoothed agnostic learning, recently introduced by~\cite{CKKMS24}, in which the learner competes with the best classifier in a target class under slight Gaussian...')" id="expand-btn-7">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21191v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21191v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="why pass@k optimization can degrade pass@1: prompt interference in llm post-training anas barakat souradip chakraborty khushbu pahwa amrit singh bedi pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. it defines success if any of $k$ independently sampled solutions passes a verifier. this multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. however, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. this trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. we study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. we show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. we illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21189v1" target="_blank" rel="noopener">Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Anas Barakat, Souradip Chakraborty, Khushbu Pahwa, Amrit Singh Bedi</p>
                <p class="paper-abstract" id="abstract-8">Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$...</p>
                <button class="expand-btn" onclick="toggleAbstract(8, 'Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.', 'Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$...')" id="expand-btn-8">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21189v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21189v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="the diffusion duality, chapter ii: $Ïˆ$-samplers and efficient curriculum justin deschenaux caglar gulcehre subham sekhar sahoo uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or masked diffusion models in these settings. however, their sampling quality plateaus with ancestral samplers as the number of steps increases. we introduce a family of predictor-corrector (pc) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. when paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on openwebtext and better fid/is scores on cifar10. crucially, unlike conventional samplers, our pc methods continue to improve with more sampling steps. taken together, these findings call into question the assumption that masked diffusion is the inevitable future of diffusion-based language modeling. beyond sampling, we develop a memory-efficient curriculum for the gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to duo while maintaining comparable perplexity on openwebtext and lm1b and strong downstream performance. we release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21185v1" target="_blank" rel="noopener">The Diffusion Duality, Chapter II: $Î¨$-Samplers and Efficient Curriculum</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Justin Deschenaux, Caglar Gulcehre, Subham Sekhar Sahoo</p>
                <p class="paper-abstract" id="abstract-9">Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these...</p>
                <button class="expand-btn" onclick="toggleAbstract(9, 'Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2', 'Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these...')" id="expand-btn-9">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21185v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21185v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="memory undone: between knowing and not knowing in data systems viktoriia makovska george fletcher julia stoyanovich tetiana zakharchenko machine learning and data systems increasingly function as infrastructures of memory: they ingest, store, and operationalize traces of personal, political, and cultural life. yet contemporary governance demands credible forms of forgetting, from gdpr-backed deletion to harm-mitigation and the removal of manipulative content, while technical infrastructures are optimized to retain, replicate, and reuse. this work argues that &quot;forgetting&quot; in computational systems cannot be reduced to a single operation (e.g., record deletion) and should instead be treated as a sociotechnical practice with distinct mechanisms and consequences. we clarify a vocabulary that separates erasure (removing or disabling access to data artifacts), unlearning (interventions that bound or remove a data point influence on learned parameters and outputs), exclusion (upstream non-collection and omission), and forgetting as an umbrella term spanning agency, temporality, reversibility, and scale. building on examples from machine unlearning, semantic dependencies in data management, participatory data modeling, and manipulation at scale, we show how forgetting can simultaneously protect rights and enable silencing. we propose reframing unlearning as a first-class capability in knowledge infrastructures, evaluated not only by compliance or utility retention, but by its governance properties: transparency, accountability, and epistemic justice.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21180v1" target="_blank" rel="noopener">Memory Undone: Between Knowing and Not Knowing in Data Systems</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.CY</span>
                </div>
                <p class="paper-authors">Viktoriia Makovska, George Fletcher, Julia Stoyanovich, Tetiana Zakharchenko</p>
                <p class="paper-abstract" id="abstract-10">Machine learning and data systems increasingly function as infrastructures of memory: they ingest, store, and operationalize traces of personal, political, and cultural life. Yet contemporary...</p>
                <button class="expand-btn" onclick="toggleAbstract(10, 'Machine learning and data systems increasingly function as infrastructures of memory: they ingest, store, and operationalize traces of personal, political, and cultural life. Yet contemporary governance demands credible forms of forgetting, from GDPR-backed deletion to harm-mitigation and the removal of manipulative content, while technical infrastructures are optimized to retain, replicate, and reuse. This work argues that &quot;forgetting&quot; in computational systems cannot be reduced to a single operation (e.g., record deletion) and should instead be treated as a sociotechnical practice with distinct mechanisms and consequences. We clarify a vocabulary that separates erasure (removing or disabling access to data artifacts), unlearning (interventions that bound or remove a data point influence on learned parameters and outputs), exclusion (upstream non-collection and omission), and forgetting as an umbrella term spanning agency, temporality, reversibility, and scale. Building on examples from machine unlearning, semantic dependencies in data management, participatory data modeling, and manipulation at scale, we show how forgetting can simultaneously protect rights and enable silencing. We propose reframing unlearning as a first-class capability in knowledge infrastructures, evaluated not only by compliance or utility retention, but by its governance properties: transparency, accountability, and epistemic justice.', 'Machine learning and data systems increasingly function as infrastructures of memory: they ingest, store, and operationalize traces of personal, political, and cultural life. Yet contemporary...')" id="expand-btn-10">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21180v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21180v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="xmorph: explainable brain tumor analysis via llm-assisted hybrid deep intelligence sepehr salem ghahfarokhi m. moein esfahani raj sunderraman vince calhoun mohammed alser deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. conventional models often act as opaque &#x27;&#x27;black boxes&#x27;&#x27; and fail to quantify the complex, irregular tumor boundaries that characterize malignant growth. to address these challenges, we present xmorph, an explainable and computationally efficient framework for fine-grained classification of three prominent brain tumor types: glioma, meningioma, and pituitary tumors. we propose an information-weighted boundary normalization (iwbn) mechanism that emphasizes diagnostically relevant boundary regions alongside nonlinear chaotic and clinically validated features, enabling a richer morphological representation of tumor growth. a dual-channel explainable ai module combines gradcam++ visual cues with llm-generated textual rationales, translating model reasoning into clinically interpretable insights. the proposed framework achieves a classification accuracy of 96.0%, demonstrating that explainability and high performance can co-exist in ai-based medical imaging systems. the source code and materials for xmorph are all publicly available at: https://github.com/alser-lab/xmorph.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21178v1" target="_blank" rel="noopener">XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.CV</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Sepehr Salem Ghahfarokhi, M. Moein Esfahani, Raj Sunderraman, Vince Calhoun, Mohammed Alser</p>
                <p class="paper-abstract" id="abstract-11">Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as...</p>
                <button class="expand-btn" onclick="toggleAbstract(11, 'Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as opaque &#x27;&#x27;black boxes&#x27;&#x27; and fail to quantify the complex, irregular tumor boundaries that characterize malignant growth. To address these challenges, we present XMorph, an explainable and computationally efficient framework for fine-grained classification of three prominent brain tumor types: glioma, meningioma, and pituitary tumors. We propose an Information-Weighted Boundary Normalization (IWBN) mechanism that emphasizes diagnostically relevant boundary regions alongside nonlinear chaotic and clinically validated features, enabling a richer morphological representation of tumor growth. A dual-channel explainable AI module combines GradCAM++ visual cues with LLM-generated textual rationales, translating model reasoning into clinically interpretable insights. The proposed framework achieves a classification accuracy of 96.0%, demonstrating that explainability and high performance can co-exist in AI-based medical imaging systems. The source code and materials for XMorph are all publicly available at: https://github.com/ALSER-Lab/XMorph.', 'Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as...')" id="expand-btn-11">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21178v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21178v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="sequential counterfactual inference for temporal clinical data: addressing the time traveler dilemma jingya cheng alaleh azhir jiazi tian hossein estiri counterfactual inference enables clinicians to ask &quot;what if&quot; questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. we introduce the sequential counterfactual framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time. applied to 2,723 covid-19 patients (383 long covid heart failure cases, 2,340 matched controls), we demonstrate that 38-67% of patients with chronic conditions would require biologically impossible counterfactuals under naive methods. we identify a cardiorenal cascade (ckd -&gt; aki -&gt; hf) with relative risks of 2.27 and 1.19 at each step, illustrating temporal propagation that sequential -- but not naive -- counterfactuals can capture. our framework transforms counterfactual explanation from &quot;what if this feature were different?&quot; to &quot;what if we had intervened earlier, and how would that propagate forward?&quot; -- yielding clinically actionable insights grounded in biological plausibility.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21168v1" target="_blank" rel="noopener">Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Jingya Cheng, Alaleh Azhir, Jiazi Tian, Hossein Estiri</p>
                <p class="paper-abstract" id="abstract-12">Counterfactual inference enables clinicians to ask &quot;what if&quot; questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated...</p>
                <button class="expand-btn" onclick="toggleAbstract(12, 'Counterfactual inference enables clinicians to ask &quot;what if&quot; questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. We introduce the Sequential Counterfactual Framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time. Applied to 2,723 COVID-19 patients (383 Long COVID heart failure cases, 2,340 matched controls), we demonstrate that 38-67% of patients with chronic conditions would require biologically impossible counterfactuals under naive methods. We identify a cardiorenal cascade (CKD -&gt; AKI -&gt; HF) with relative risks of 2.27 and 1.19 at each step, illustrating temporal propagation that sequential -- but not naive -- counterfactuals can capture. Our framework transforms counterfactual explanation from &quot;what if this feature were different?&quot; to &quot;what if we had intervened earlier, and how would that propagate forward?&quot; -- yielding clinically actionable insights grounded in biological plausibility.', 'Counterfactual inference enables clinicians to ask &quot;what if&quot; questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated...')" id="expand-btn-12">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21168v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21168v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="pvminer: a domain-specific tool to detect the patient voice in patient generated data samah fodeh linhai ma yan wang srivani talakokkul ganesh puthiaraju afshan khan ashley hagaman sarah lowe aimee roundtree patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (pv), reflecting communicative behaviors and social determinants of health (sdoh). traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. existing machine learning (ml) and natural language processing (nlp) approaches provide partial solutions but often treat patient-centered communication (pcc) and sdoh as separate tasks or rely on models not well suited to patient-facing language. we introduce pvminer, a domain-adapted nlp framework for structuring patient voice in secure patient-provider communication. pvminer formulates pv detection as a multi-label, multi-class prediction task integrating patient-specific bert encoders (pv-bert-base and pv-bert-large), unsupervised topic modeling for thematic augmentation (pv-topic-bert), and fine-tuned classifiers for code, subcode, and combo-level labels. topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. pvminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving f1 scores of 82.25% (code), 80.14% (subcode), and up to 77.87% (combo). an ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21165v1" target="_blank" rel="noopener">PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Samah Fodeh, Linhai Ma, Yan Wang, Srivani Talakokkul, Ganesh Puthiaraju <em>(+4 more)</em></p>
                <p class="paper-abstract" id="abstract-13">Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health...</p>
                <button class="expand-btn" onclick="toggleAbstract(13, 'Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.', 'Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health...')" id="expand-btn-13">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21165v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21165v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="actionreasoning: robot action reasoning in 3d space with llm for robotic brick stacking guangming wang qizhen ying yixiong jing olaf wysocki brian sheil classical robotic systems typically rely on custom planners designed for constrained environments. while effective in restricted settings, these systems lack generalization capabilities, limiting the scalability of embodied ai and general-purpose robots. recent data-driven vision-language-action (vla) approaches aim to learn policies from large-scale simulation and real-world data. however, the continuous action space of the physical world significantly exceeds the representational capacity of linguistic tokens, making it unclear if scaling data alone can yield general robotic intelligence. to address this gap, we propose actionreasoning, an llm-driven framework that performs explicit action reasoning to produce physics-consistent, prior-guided decisions for robotic manipulation. actionreasoning leverages the physical priors and real-world knowledge already encoded in large language models (llms) and structures them within a multi-agent architecture. we instantiate this framework on a tractable case study of brick stacking, where the environment states are assumed to be already accurately measured. the environmental states are then serialized and passed to a multi-agent llm framework that generates physics-aware action plans. the experiments demonstrate that the proposed multi-agent llm framework enables stable brick placement while shifting effort from low-level domain-specific coding to high-level tool invocation and prompting, highlighting its potential for broader generalization. this work introduces a promising approach to bridging perception and execution in robotic manipulation by integrating physical reasoning with llms.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21161v1" target="_blank" rel="noopener">ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.RO</span>
                </div>
                <p class="paper-authors">Guangming Wang, Qizhen Ying, Yixiong Jing, Olaf Wysocki, Brian Sheil</p>
                <p class="paper-abstract" id="abstract-14">Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the...</p>
                <button class="expand-btn" onclick="toggleAbstract(14, 'Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the scalability of embodied AI and general-purpose robots. Recent data-driven Vision-Language-Action (VLA) approaches aim to learn policies from large-scale simulation and real-world data. However, the continuous action space of the physical world significantly exceeds the representational capacity of linguistic tokens, making it unclear if scaling data alone can yield general robotic intelligence. To address this gap, we propose ActionReasoning, an LLM-driven framework that performs explicit action reasoning to produce physics-consistent, prior-guided decisions for robotic manipulation. ActionReasoning leverages the physical priors and real-world knowledge already encoded in Large Language Models (LLMs) and structures them within a multi-agent architecture. We instantiate this framework on a tractable case study of brick stacking, where the environment states are assumed to be already accurately measured. The environmental states are then serialized and passed to a multi-agent LLM framework that generates physics-aware action plans. The experiments demonstrate that the proposed multi-agent LLM framework enables stable brick placement while shifting effort from low-level domain-specific coding to high-level tool invocation and prompting, highlighting its potential for broader generalization. This work introduces a promising approach to bridging perception and execution in robotic manipulation by integrating physical reasoning with LLMs.', 'Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the...')" id="expand-btn-14">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21161v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21161v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="not just how much, but where: decomposing epistemic uncertainty into per-class contributions mame diarra toure david a. stephens in safety-critical classification, the cost of failure is often asymmetric, yet bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (mi), that cannot distinguish whether a model&#x27;s ignorance involves a benign or safety-critical class. we decompose mi into a per-class vector $c_k(x)=Ïƒ_k^{2}/(2Î¼_k)$, with $Î¼_k{=}\mathbb{e}[p_k]$ and $Ïƒ_k^2{=}\mathrm{var}[p_k]$ across posterior samples. the decomposition follows from a second-order taylor expansion of the entropy; the $1/Î¼_k$ weighting corrects boundary suppression and makes $c_k$ comparable across rare and common classes. by construction $\sum_k c_k \approx \mathrm{mi}$, and a companion skewness diagnostic flags inputs where the approximation degrades. after characterising the axiomatic properties of $c_k$, we validate it on three tasks: (i) selective prediction for diabetic retinopathy, where critical-class $c_k$ reduces selective risk by 34.7\% over mi and 56.2\% over variance baselines; (ii) out-of-distribution detection on clinical and image benchmarks, where $\sum_k c_k$ achieves the highest auroc and the per-class view exposes asymmetric shifts invisible to mi; and (iii) a controlled label-noise study in which $\sum_k c_k$ shows less sensitivity to injected aleatoric noise than mi under end-to-end bayesian training, while both metrics degrade under transfer learning. across all tasks, the quality of the posterior approximation shapes uncertainty at least as strongly as the choice of metric, suggesting that how uncertainty is propagated through the network matters as much as how it is measured.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21160v1" target="_blank" rel="noopener">Not Just How Much, But Where: Decomposing Epistemic Uncertainty into Per-Class Contributions</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">stat.ML</span><span class="paper-category">cs.LG</span><span class="paper-category">stat.AP</span>
                </div>
                <p class="paper-authors">Mame Diarra Toure, David A. Stephens</p>
                <p class="paper-abstract" id="abstract-15">In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot...</p>
                <button class="expand-btn" onclick="toggleAbstract(15, 'In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot distinguish whether a model&#x27;s ignorance involves a benign or safety-critical class. We decompose MI into a per-class vector $C_k(x)=Ïƒ_k^{2}/(2Î¼_k)$, with $Î¼_k{=}\mathbb{E}[p_k]$ and $Ïƒ_k^2{=}\mathrm{Var}[p_k]$ across posterior samples. The decomposition follows from a second-order Taylor expansion of the entropy; the $1/Î¼_k$ weighting corrects boundary suppression and makes $C_k$ comparable across rare and common classes. By construction $\sum_k C_k \approx \mathrm{MI}$, and a companion skewness diagnostic flags inputs where the approximation degrades. After characterising the axiomatic properties of $C_k$, we validate it on three tasks: (i) selective prediction for diabetic retinopathy, where critical-class $C_k$ reduces selective risk by 34.7\% over MI and 56.2\% over variance baselines; (ii) out-of-distribution detection on clinical and image benchmarks, where $\sum_k C_k$ achieves the highest AUROC and the per-class view exposes asymmetric shifts invisible to MI; and (iii) a controlled label-noise study in which $\sum_k C_k$ shows less sensitivity to injected aleatoric noise than MI under end-to-end Bayesian training, while both metrics degrade under transfer learning. Across all tasks, the quality of the posterior approximation shapes uncertainty at least as strongly as the choice of metric, suggesting that how uncertainty is propagated through the network matters as much as how it is measured.', 'In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot...')" id="expand-btn-15">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21160v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21160v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="selaur: self evolving llm agent via uncertainty-aware rewards dengjia zhang xiaoou liu lu cheng yaqing wang kenton murray hua wei large language models (llms) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of llms. uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. we introduce selaur: self evolving llm agent via uncertainty-aware rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. selaur integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. experiments on two benchmarks, alfworld and webshop, show that our method consistently improves success rates over strong baselines. ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21158v1" target="_blank" rel="noopener">SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Dengjia Zhang, Xiaoou Liu, Lu Cheng, Yaqing Wang, Kenton Murray <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-16">Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various...</p>
                <button class="expand-btn" onclick="toggleAbstract(16, 'Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.', 'Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various...')" id="expand-btn-16">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21158v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21158v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="kan-koopman based rapid detection of battery thermal anomalies with diagnostics guarantees sanchita ghosh tanushree roy early diagnosis of battery thermal anomalies is crucial to ensure safe and reliable battery operation by preventing catastrophic thermal failures. battery diagnostics primarily rely on battery surface temperature measurements and/or estimation of core temperatures. however, aging-induced changes in the battery model and limited training data remain major challenges for model-based and machine-learning based battery state estimation and diagnostics. to address these issues, we propose a kolomogorov-arnold network (kan) in conjunction with a koopman-based detection algorithm that leverages the unique advantages of both methods. firstly, the lightweight kan provides a model-free estimation of the core temperature to ensure rapid detection of battery thermal anomalies. secondly, the koopman operator is learned in real time using the estimated core temperature from kan and the measured surface temperature of the battery to provide a prediction for diagnostic residual generation. this online learning approach overcomes the challenges of model changes, while the integrated structure reduces the dependence on large datasets. furthermore, we derive analytical conditions that provide diagnostic guarantees on our kan-koopman detection scheme. our simulation results illustrate a significant reduction in detection time with the proposed algorithm compared to the baseline koopman-only algorithm.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21155v1" target="_blank" rel="noopener">KAN-Koopman Based Rapid Detection Of Battery Thermal Anomalies With Diagnostics Guarantees</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">eess.SY</span>
                </div>
                <p class="paper-authors">Sanchita Ghosh, Tanushree Roy</p>
                <p class="paper-abstract" id="abstract-17">Early diagnosis of battery thermal anomalies is crucial to ensure safe and reliable battery operation by preventing catastrophic thermal failures. Battery diagnostics primarily rely on battery...</p>
                <button class="expand-btn" onclick="toggleAbstract(17, 'Early diagnosis of battery thermal anomalies is crucial to ensure safe and reliable battery operation by preventing catastrophic thermal failures. Battery diagnostics primarily rely on battery surface temperature measurements and/or estimation of core temperatures. However, aging-induced changes in the battery model and limited training data remain major challenges for model-based and machine-learning based battery state estimation and diagnostics. To address these issues, we propose a Kolomogorov-Arnold network (KAN) in conjunction with a Koopman-based detection algorithm that leverages the unique advantages of both methods. Firstly, the lightweight KAN provides a model-free estimation of the core temperature to ensure rapid detection of battery thermal anomalies. Secondly, the Koopman operator is learned in real time using the estimated core temperature from KAN and the measured surface temperature of the battery to provide a prediction for diagnostic residual generation. This online learning approach overcomes the challenges of model changes, while the integrated structure reduces the dependence on large datasets. Furthermore, we derive analytical conditions that provide diagnostic guarantees on our KAN-Koopman detection scheme. Our simulation results illustrate a significant reduction in detection time with the proposed algorithm compared to the baseline Koopman-only algorithm.', 'Early diagnosis of battery thermal anomalies is crucial to ensure safe and reliable battery operation by preventing catastrophic thermal failures. Battery diagnostics primarily rely on battery...')" id="expand-btn-17">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21155v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21155v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="scaling state-space models on multiple gpus with tensor parallelism anurag dutt nimit shah hazem masarani anshul gandhi selective state space models (ssms) have rapidly become a compelling backbone for large language models, especially for long-context workloads. yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single gpu, making multi-gpu execution increasingly necessary. although tensor parallelism (tp) is widely used to scale transformer inference, applying it to selective ssm blocks is non-trivial because the ssm mixer couples large projections with a sequence-wise recurrent state update and local mixing whose efficiency depends on preserving locality and avoiding synchronization in the critical path. this paper presents a communication-efficient tp design for selective ssm inference that addresses three practical engineering challenges: enabling ttft improvements via an ssm state cache across prefill and decode, partitioning the mixer&#x27;s packed parameter tensor so that recurrent updates remain local while minimizing communication, and reducing tp aggregation overhead with quantized allreduce. we evaluate on three representative ssm-based llms spanning pure-ssm and hybrid architectures - mamba, falcon-mamba, and zamba - on nvidia a6000 and a100 clusters. our experiments show substantial throughput gains from tensor-parallel ssm inference, improving batch-request throughput by ~1.6-2.1x on 2 gpus and ~2.6-4.0x on 4 gpus for mamba, with the largest benefits at long context lengths, and achieving a further ~10-18% throughput improvement from quantized all-reduce by lowering synchronization bandwidth overhead.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21144v1" target="_blank" rel="noopener">Scaling State-Space Models on Multiple GPUs with Tensor Parallelism</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.DC</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Anurag Dutt, Nimit Shah, Hazem Masarani, Anshul Gandhi</p>
                <p class="paper-abstract" id="abstract-18">Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often...</p>
                <button class="expand-btn" onclick="toggleAbstract(18, 'Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large projections with a sequence-wise recurrent state update and local mixing whose efficiency depends on preserving locality and avoiding synchronization in the critical path. This paper presents a communication-efficient TP design for selective SSM inference that addresses three practical engineering challenges: enabling TTFT improvements via an SSM state cache across prefill and decode, partitioning the mixer&#x27;s packed parameter tensor so that recurrent updates remain local while minimizing communication, and reducing TP aggregation overhead with quantized AllReduce. We evaluate on three representative SSM-based LLMs spanning pure-SSM and hybrid architectures - Mamba, Falcon-Mamba, and Zamba - on NVIDIA A6000 and A100 clusters. Our experiments show substantial throughput gains from tensor-parallel SSM inference, improving batch-request throughput by ~1.6-2.1x on 2 GPUs and ~2.6-4.0x on 4 GPUs for Mamba, with the largest benefits at long context lengths, and achieving a further ~10-18% throughput improvement from quantized all-reduce by lowering synchronization bandwidth overhead.', 'Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often...')" id="expand-btn-18">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21144v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21144v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="a benchmark for deep information synthesis debjit paul daniel murphy milan gritta ronald cardenas victor prokhorov lena sophia bolliger aysim toker roy miles andreea-maria oncescu jasivan alex sivakumar philipp borchert ismail elezi meiru zhang ka yiu lee guchun zhang jun wang gerasimos lampouras large language model (llm)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. however, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. to address this, we introduce deepsynth, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. deepsynth contains 120 tasks collected across 7 domains and data sources covering 67 countries. deepsynth is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. when evaluated on deepsynth, 11 state-of-the-art llms and deep research agents achieve a maximum f1 score of 8.97 and 17.5 on the llm-judge metric, underscoring the difficulty of the benchmark. our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting deepsynth as a crucial benchmark for guiding future research.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.21143v1" target="_blank" rel="noopener">A Benchmark for Deep Information Synthesis</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-24</span>
                    <span class="paper-category">cs.AI</span><span class="paper-category">cs.CL</span><span class="paper-category">cs.IR</span>
                </div>
                <p class="paper-authors">Debjit Paul, Daniel Murphy, Milan Gritta, Ronald Cardenas, Victor Prokhorov <em>(+12 more)</em></p>
                <p class="paper-abstract" id="abstract-19">Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation...</p>
                <button class="expand-btn" onclick="toggleAbstract(19, 'Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.', 'Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation...')" id="expand-btn-19">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.21143v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.21143v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

        </div>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2026 Coding Blog | BST 236 Computing I | Harvard University</p>
            <p class="footer-links">
                <a href="https://github.com">GitHub</a>
            </p>
        </div>
    </footer>

    <script src="js/main.js"></script>
    <script>
        function filterPapers() {
            const query = document.getElementById('searchInput').value.toLowerCase();
            const cards = document.querySelectorAll('.paper-card');
            
            cards.forEach(card => {
                const searchText = card.getAttribute('data-search');
                if (searchText.includes(query)) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });
        }
        
        function toggleAbstract(index, fullText, shortText) {
            const abstractEl = document.getElementById('abstract-' + index);
            const btnEl = document.getElementById('expand-btn-' + index);
            
            if (abstractEl.classList.contains('expanded')) {
                abstractEl.textContent = shortText;
                abstractEl.classList.remove('expanded');
                btnEl.textContent = 'Show more â–¼';
            } else {
                abstractEl.textContent = fullText;
                abstractEl.classList.add('expanded');
                btnEl.textContent = 'Show less â–²';
            }
        }
        
        // Client-side arXiv fetching for custom keywords
        // CORS proxies to try in order
        const corsProxies = [
            url => `https://corsproxy.io/?${encodeURIComponent(url)}`,
            url => `https://api.codetabs.com/v1/proxy?quest=${encodeURIComponent(url)}`,
            url => `https://api.allorigins.win/raw?url=${encodeURIComponent(url)}`
        ];
        
        async function tryFetchWithProxies(url, proxies) {
            for (let i = 0; i < proxies.length; i++) {
                const proxyUrl = proxies[i](url);
                try {
                    const response = await fetch(proxyUrl, { timeout: 10000 });
                    if (response.ok) {
                        return await response.text();
                    }
                } catch (e) {
                    console.log(`Proxy ${i + 1} failed:`, e.message);
                }
            }
            throw new Error('All CORS proxies failed. Please try again later or edit scripts/config.json directly.');
        }
        
        async function fetchCustomPapers() {
            const input = document.getElementById('customKeywords').value.trim();
            const btn = document.getElementById('fetchBtn');
            const grid = document.getElementById('papersGrid');
            const keywordsDisplay = document.getElementById('currentKeywords');
            
            if (!input) {
                alert('Please enter at least one keyword');
                return;
            }
            
            const keywords = input.split(',').map(k => k.trim()).filter(k => k);
            
            btn.disabled = true;
            btn.textContent = 'Fetching...';
            
            // Update displayed keywords
            keywordsDisplay.innerHTML = keywords.map(kw => 
                `<span class="keyword-tag">${escapeHtml(kw)}</span>`
            ).join('');
            
            // Build arXiv query
            const searchQuery = keywords.map(kw => `all:"${kw}"`).join(' OR ');
            const url = `https://export.arxiv.org/api/query?search_query=${encodeURIComponent(searchQuery)}&start=0&max_results=20&sortBy=submittedDate&sortOrder=descending`;
            
            try {
                const xmlText = await tryFetchWithProxies(url, corsProxies);
                
                // Parse XML
                const parser = new DOMParser();
                const xmlDoc = parser.parseFromString(xmlText, 'text/xml');
                const entries = xmlDoc.querySelectorAll('entry');
                
                if (entries.length === 0) {
                    grid.innerHTML = `
                        <div class="no-results" style="grid-column: 1 / -1;">
                            <h3>No papers found</h3>
                            <p>Try different keywords.</p>
                        </div>
                    `;
                } else {
                    let html = '';
                    entries.forEach((entry, i) => {
                        const title = entry.querySelector('title')?.textContent?.replace(/\s+/g, ' ').trim() || 'Untitled';
                        const abstract = entry.querySelector('summary')?.textContent?.replace(/\s+/g, ' ').trim() || 'No abstract';
                        const published = entry.querySelector('published')?.textContent?.substring(0, 10) || 'Unknown';
                        const id = entry.querySelector('id')?.textContent || '';
                        const pdfUrl = id.replace('/abs/', '/pdf/') + '.pdf';
                        
                        const authors = [];
                        entry.querySelectorAll('author name').forEach(n => authors.push(n.textContent));
                        const authorsStr = authors.length > 5 
                            ? authors.slice(0, 5).join(', ') + ` <em>(+${authors.length - 5} more)</em>`
                            : authors.join(', ');
                        
                        const categories = [];
                        entry.querySelectorAll('category').forEach(c => {
                            const term = c.getAttribute('term');
                            if (term && categories.length < 3) categories.push(term);
                        });
                        
                        const abstractShort = abstract.length > 200 
                            ? abstract.substring(0, 200).replace(/\s+\S*$/, '') + '...'
                            : abstract;
                        
                        html += `
                            <article class="paper-card" data-search="${escapeHtml(title.toLowerCase())} ${escapeHtml(authors.join(' ').toLowerCase())} ${escapeHtml(abstract.toLowerCase())}">
                                <h2 class="paper-title">
                                    <a href="${escapeHtml(id)}" target="_blank" rel="noopener">${escapeHtml(title)}</a>
                                </h2>
                                <div class="paper-meta">
                                    <span class="paper-date">ğŸ“… ${escapeHtml(published)}</span>
                                    ${categories.map(c => `<span class="paper-category">${escapeHtml(c)}</span>`).join('')}
                                </div>
                                <p class="paper-authors">${authorsStr}</p>
                                <p class="paper-abstract" id="abstract-dyn-${i}">${escapeHtml(abstractShort)}</p>
                                <button class="expand-btn" onclick="toggleAbstract('dyn-${i}', '${escapeHtml(abstract).replace(/'/g, "\\'")}', '${escapeHtml(abstractShort).replace(/'/g, "\\'")}')">Show more â–¼</button>
                                <div class="paper-actions">
                                    <a href="${escapeHtml(pdfUrl)}" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                                    <a href="${escapeHtml(id)}" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                                </div>
                            </article>
                        `;
                    });
                    grid.innerHTML = html;
                }
                
                // Update timestamp
                const now = new Date().toISOString().replace('T', ' ').substring(0, 19) + ' (live fetch)';
                document.querySelector('.update-info').textContent = 'Last updated: ' + now;
                
            } catch (error) {
                console.error('Fetch error:', error);
                grid.innerHTML = `
                    <div class="no-results" style="grid-column: 1 / -1;">
                        <h3>Error fetching papers</h3>
                        <p>${escapeHtml(error.message)}</p>
                        <p style="margin-top: 15px; font-size: 0.9rem;">
                            <strong>Alternative:</strong> Edit <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">scripts/config.json</code> 
                            and run <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">python scripts/fetch_arxiv.py</code> locally, 
                            or push to GitHub to trigger the auto-update.
                        </p>
                    </div>
                `;
            }
            
            btn.disabled = false;
            btn.textContent = 'Fetch Papers';
        }
        
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }
        
        // Allow Enter key to trigger fetch
        document.getElementById('customKeywords').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') fetchCustomPapers();
        });
    </script>
</body>
</html>
