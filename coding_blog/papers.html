<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Feed | Coding Blog</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <style>
        .papers-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 100px 20px 40px;
        }
        .papers-header {
            text-align: center;
            margin-bottom: 40px;
        }
        .papers-header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .update-info {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 8px;
        }
        .auto-update-info {
            color: #22c55e;
            font-size: 0.85rem;
            margin-bottom: 15px;
        }
        .config-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-bottom: 10px;
        }
        .config-hint code {
            background: #334155;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: var(--font-code);
            color: #f472b6;
        }
        .search-box {
            max-width: 500px;
            margin: 20px auto;
        }
        .search-box input {
            width: 100%;
            padding: 12px 20px;
            border: 2px solid var(--border);
            border-radius: 25px;
            background: var(--card-bg);
            color: var(--text-primary);
            font-size: 1rem;
            outline: none;
            transition: border-color 0.3s;
        }
        .search-box input:focus {
            border-color: var(--primary);
        }
        .search-box input::placeholder {
            color: var(--text-muted);
        }
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 25px;
        }
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 25px;
            transition: all 0.3s ease;
        }
        .paper-card:hover {
            border-color: var(--primary);
            transform: translateY(-3px);
            box-shadow: 0 10px 30px rgba(236, 72, 153, 0.15);
        }
        .paper-card.hidden {
            display: none;
        }
        .paper-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-bottom: 10px;
            line-height: 1.4;
        }
        .paper-title a {
            color: inherit;
            text-decoration: none;
            transition: color 0.3s;
        }
        .paper-title a:hover {
            color: var(--primary);
        }
        .paper-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 12px;
        }
        .paper-date {
            background: var(--primary);
            color: white;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
            font-weight: 500;
        }
        .paper-category {
            background: #334155;
            color: #e2e8f0;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
        }
        .paper-authors {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 12px;
            line-height: 1.5;
        }
        .paper-abstract {
            color: #94a3b8;
            font-size: 0.9rem;
            line-height: 1.7;
            margin-bottom: 15px;
        }
        .paper-abstract.expanded {
            max-height: none;
        }
        .expand-btn {
            background: none;
            border: none;
            color: var(--primary);
            cursor: pointer;
            font-size: 0.85rem;
            padding: 0;
            margin-bottom: 15px;
        }
        .expand-btn:hover {
            text-decoration: underline;
        }
        .paper-actions {
            display: flex;
            gap: 10px;
        }
        .pdf-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            transition: all 0.3s;
        }
        .pdf-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 15px rgba(236, 72, 153, 0.3);
        }
        .arxiv-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: var(--surface);
            color: var(--text-primary);
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            border: 1px solid var(--border);
            transition: all 0.3s;
        }
        .arxiv-btn:hover {
            border-color: var(--primary);
            color: var(--primary);
        }
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: var(--text-muted);
        }
        .no-results h3 {
            font-size: 1.5rem;
            margin-bottom: 10px;
            color: var(--text-secondary);
        }
        .keywords-info {
            background: #1e293b;
            padding: 20px;
            border-radius: 12px;
            margin-bottom: 30px;
            text-align: center;
            border: 1px solid #334155;
        }
        .keywords-info > span {
            color: #e2e8f0;
            font-size: 0.95rem;
            font-weight: 500;
        }
        .keyword-tag {
            display: inline-block;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: #ffffff;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin: 4px;
            text-shadow: 0 1px 2px rgba(0,0,0,0.2);
        }
        .keyword-editor {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #334155;
        }
        .keyword-input-group {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .keyword-input {
            padding: 10px 16px;
            border: 2px solid #334155;
            border-radius: 25px;
            background: #0f172a;
            color: #f8fafc;
            font-size: 0.9rem;
            width: 250px;
            outline: none;
            transition: border-color 0.3s;
        }
        .keyword-input:focus {
            border-color: #6366f1;
        }
        .keyword-input::placeholder {
            color: #64748b;
        }
        .fetch-btn {
            padding: 10px 24px;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: white;
            border: none;
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .fetch-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 20px rgba(99, 102, 241, 0.4);
        }
        .fetch-btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }
        .editor-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-top: 8px;
        }
        @media (max-width: 768px) {
            .papers-grid {
                grid-template-columns: 1fr;
            }
            .papers-header h1 {
                font-size: 1.8rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="index.html" class="nav-logo">
                <span class="logo-icon">ğŸ’»</span>
                <span class="logo-text">Coding Blog</span>
            </a>
            <ul class="nav-menu">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li><a href="index.html#projects" class="nav-link">Projects</a></li>
                <li><a href="papers.html" class="nav-link active">Papers</a></li>
                <li><a href="index.html#about" class="nav-link">About</a></li>
            </ul>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <div class="papers-container">
        <div class="papers-header">
            <h1>ğŸ“š arXiv Paper Feed</h1>
            <p class="update-info">Last updated: 2026-02-24 01:44:01 UTC</p>
            <p class="auto-update-info">â° Auto-updates daily at midnight UTC via GitHub Actions</p>
            <div class="keywords-info">
                <span>Current keywords: </span>
                <div id="currentKeywords">
                    <span class="keyword-tag">large language model</span><span class="keyword-tag">machine learning</span><span class="keyword-tag">biostatistics</span><span class="keyword-tag">deep learning</span>
                </div>
                <div class="keyword-editor">
                    <p class="editor-hint">ğŸ”„ Try different keywords (fetches live from arXiv):</p>
                    <div class="keyword-input-group">
                        <input type="text" id="customKeywords" class="keyword-input" 
                               placeholder="e.g., reinforcement learning, NLP" 
                               value="large language model, machine learning, biostatistics, deep learning">
                        <button onclick="fetchCustomPapers()" class="fetch-btn" id="fetchBtn">
                            Fetch Papers
                        </button>
                    </div>
                    <p class="editor-hint">Separate multiple keywords with commas</p>
                </div>
            </div>
            <div class="search-box">
                <input type="text" id="searchInput" placeholder="ğŸ” Filter papers by title, author, or abstract..." oninput="filterPapers()">
            </div>
        </div>

        <div class="papers-grid" id="papersGrid">

            <article class="paper-card" data-search="assigning confidence: k-partition ensembles aggelos semoglou john pavlopoulos clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. this assignment-level instability can undermine both accuracy and robustness. ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. we introduce cake (confidence in assignments via k-partition ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. these are combined into a single, interpretable score in [0,1]. our theoretical analysis shows that cake remains effective under noise and separates stable from unstable points. experiments on synthetic and real-world datasets indicate that cake effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18435v1" target="_blank" rel="noopener">Assigning Confidence: K-partition Ensembles</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Aggelos Semoglou, John Pavlopoulos</p>
                <p class="paper-abstract" id="abstract-0">Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective...</p>
                <button class="expand-btn" onclick="toggleAbstract(0, 'Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.', 'Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective...')" id="expand-btn-0">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18435v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18435v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="viraasat: traversing novel paths for indian cultural reasoning harshul raj surana arijit maji aryan vats akash ghosh sriparna saha amit sheth large language models (llms) have made significant progress in reasoning tasks across various domains such as mathematics and coding. however, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving indian culture. existing cultural benchmarks are (i) manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. to address this, we introduce viraasat, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop question-answering dataset for indian culture. viraasat leverages a knowledge graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of indian culture (history, festivals, etc). viraasat spans all 28 states and 8 union territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. we evaluate current state-of-the-art (sota) llms on viraasat and identify key limitations in reasoning wherein fine-tuning on chain-of-thought(cot) traces fails to ground and synthesize low-probability facts. to bridge this gap, we propose a novel framework named symbolic chain-of-manipulation (scom). adapting the chain-of-manipulation paradigm, we train the model to simulate atomic knowledge graph manipulations internally. scom teaches the model to reliably traverse the topological structure of the graph. experiments on supervised fine-tuning (sft) demonstrate that scom outperforms standard cot baselines by up to 20%. we release the viraasat dataset along with our findings, laying a strong foundation towards building culturally aware reasoning models.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18429v1" target="_blank" rel="noopener">VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">cs.IR</span>
                </div>
                <p class="paper-authors">Harshul Raj Surana, Arijit Maji, Aryan Vats, Akash Ghosh, Sriparna Saha <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-1">Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich...</p>
                <button class="expand-btn" onclick="toggleAbstract(1, 'Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.', 'Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich...')" id="expand-btn-1">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18429v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18429v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="the geometry of noise: why diffusion models don&#x27;t need noise conditioning mojtaba sahraee-ardakan mauricio delbracio peyman milanfar autonomous (noise-agnostic) generative models, such as equilibrium matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. while recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? we resolve this paradox by formalizing marginal energy, $e_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$, where $p(\mathbf{u}) = \int p(\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. we prove that generation using autonomous models is not merely blind denoising, but a specific form of riemannian gradient flow on this marginal energy. through a novel relative energy decomposition, we demonstrate that while the raw marginal energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. we also establish the structural stability conditions for sampling with autonomous models. we identify a ``jensen gap&#x27;&#x27; in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18428v1" target="_blank" rel="noopener">The Geometry of Noise: Why Diffusion Models Don&#x27;t Need Noise Conditioning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.CV</span><span class="paper-category">eess.IV</span>
                </div>
                <p class="paper-authors">Mojtaba Sahraee-Ardakan, Mauricio Delbracio, Peyman Milanfar</p>
                <p class="paper-abstract" id="abstract-2">Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates...</p>
                <button class="expand-btn" onclick="toggleAbstract(2, 'Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$, where $p(\mathbf{u}) = \int p(\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap&#x27;&#x27; in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.', 'Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates...')" id="expand-btn-2">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18428v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18428v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="spatio-spectroscopic representation learning using unsupervised convolutional long-short term memory networks kameswara bharadwaj mantha lucy fortson ramanakumar sankar claudia scarlata chris lintott sandor kruk mike walmsley hugh dickinson karen masters brooke simmons rebecca smethurst integral field spectroscopy (ifs) surveys offer a unique new landscape in which to learn in both spatial and spectroscopic dimensions and could help uncover previously unknown insights into galaxy evolution. in this work, we demonstrate a new unsupervised deep learning framework using convolutional long-short term memory network autoencoders to encode generalized feature representations across both spatial and spectroscopic dimensions spanning $19$ optical emission lines (3800a $&lt; Î»&lt;$ 8000a) among a sample of $\sim 9000$ galaxies from the manga ifs survey. as a demonstrative exercise, we assess our model on a sample of $290$ active galactic nuclei (agn) and highlight scientifically interesting characteristics of some highly anomalous agn.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18426v1" target="_blank" rel="noopener">Spatio-Spectroscopic Representation Learning using Unsupervised Convolutional Long-Short Term Memory Networks</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">astro-ph.GA</span><span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Kameswara Bharadwaj Mantha, Lucy Fortson, Ramanakumar Sankar, Claudia Scarlata, Chris Lintott <em>(+6 more)</em></p>
                <p class="paper-abstract" id="abstract-3">Integral Field Spectroscopy (IFS) surveys offer a unique new landscape in which to learn in both spatial and spectroscopic dimensions and could help uncover previously unknown insights into galaxy...</p>
                <button class="expand-btn" onclick="toggleAbstract(3, 'Integral Field Spectroscopy (IFS) surveys offer a unique new landscape in which to learn in both spatial and spectroscopic dimensions and could help uncover previously unknown insights into galaxy evolution. In this work, we demonstrate a new unsupervised deep learning framework using Convolutional Long-Short Term Memory Network Autoencoders to encode generalized feature representations across both spatial and spectroscopic dimensions spanning $19$ optical emission lines (3800A $&lt; Î»&lt;$ 8000A) among a sample of $\sim 9000$ galaxies from the MaNGA IFS survey. As a demonstrative exercise, we assess our model on a sample of $290$ Active Galactic Nuclei (AGN) and highlight scientifically interesting characteristics of some highly anomalous AGN.', 'Integral Field Spectroscopy (IFS) surveys offer a unique new landscape in which to learn in both spatial and spectroscopic dimensions and could help uncover previously unknown insights into galaxy...')" id="expand-btn-3">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18426v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18426v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="spq: an ensemble technique for large language model compression jiamin yao eren gultepe this study presents an ensemble technique, spq (svd-pruning-quantization), for large language model (llm) compression that combines variance-retained singular value decomposition (svd), activation-based pruning, and post-training linear quantization. each component targets a different source of inefficiency: i) pruning removes redundant neurons in mlp layers, ii) svd reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. at matched compression ratios, spq outperforms individual methods (svd-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. applied to llama-2-7b, spq achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., wikitext-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as c4, truthfulqa, and gsm8k. compared to strong baselines like gptq and sparsegpt, spq offers competitive perplexity and accuracy while using less memory (6.86 gb vs. 7.16 gb for gptq). moreover, spq improves inference throughput over gptq, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. the effectiveness of spq&#x27;s robust compression through layer-aware and complementary compression techniques may provide practical deployment of llms in memory-constrained environments. code is available at: https://github.com/jiaminyao/spq_llm_compression/">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18420v1" target="_blank" rel="noopener">SPQ: An Ensemble Technique for Large Language Model Compression</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Jiamin Yao, Eren Gultepe</p>
                <p class="paper-abstract" id="abstract-4">This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD),...</p>
                <button class="expand-btn" onclick="toggleAbstract(4, 'This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ&#x27;s robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/', 'This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD),...')" id="expand-btn-4">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18420v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18420v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="benchmarking graph neural networks in solving hard constraint satisfaction problems geri skenderi lorenzo buffoni francesco d&#x27;amico david machado raffaele marino matteo negri federico ricci-tersenghi carlo lucibello maria chiara angelini graph neural networks (gnns) are increasingly applied to hard optimization problems, often claiming superiority over classical heuristics. however, such claims risk being unsolid due to a lack of standard benchmarks on truly hard instances. from a statistical physics perspective, we propose new hard benchmarks based on random problems. we provide these benchmarks, along with performance results from both classical heuristics and gnns. our fair comparison shows that classical algorithms still outperform gnns. we discuss the challenges for neural networks in this domain. future claims of superiority can be made more robust using our benchmarks, available at https://github.com/artlabbocconi/randcspbench.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18419v1" target="_blank" rel="noopener">Benchmarking Graph Neural Networks in Solving Hard Constraint Satisfaction Problems</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cond-mat.dis-nn</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Geri Skenderi, Lorenzo Buffoni, Francesco D'Amico, David Machado, Raffaele Marino <em>(+4 more)</em></p>
                <p class="paper-abstract" id="abstract-5">Graph neural networks (GNNs) are increasingly applied to hard optimization problems, often claiming superiority over classical heuristics. However, such claims risk being unsolid due to a lack of...</p>
                <button class="expand-btn" onclick="toggleAbstract(5, 'Graph neural networks (GNNs) are increasingly applied to hard optimization problems, often claiming superiority over classical heuristics. However, such claims risk being unsolid due to a lack of standard benchmarks on truly hard instances. From a statistical physics perspective, we propose new hard benchmarks based on random problems. We provide these benchmarks, along with performance results from both classical heuristics and GNNs. Our fair comparison shows that classical algorithms still outperform GNNs. We discuss the challenges for neural networks in this domain. Future claims of superiority can be made more robust using our benchmarks, available at https://github.com/ArtLabBocconi/RandCSPBench.', 'Graph neural networks (GNNs) are increasingly applied to hard optimization problems, often claiming superiority over classical heuristics. However, such claims risk being unsolid due to a lack of...')" id="expand-btn-5">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18419v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18419v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="subgroups of $u(d)$ induce natural rnn and transformer architectures joshua nunley this paper presents a direct framework for sequence models with hidden states on closed subgroups of u(d). we use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. we then specialize to o(d) and evaluate orthogonal-state rnn and transformer models on tiny shakespeare and penn treebank under parameter-matched settings. we also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current o(d) experiments.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18417v1" target="_blank" rel="noopener">Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Joshua Nunley</p>
                <p class="paper-abstract" id="abstract-6">This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a...</p>
                <button class="expand-btn" onclick="toggleAbstract(6, 'This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.', 'This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a...')" id="expand-btn-6">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18417v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18417v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="ai-wrapped: participatory, privacy-preserving measurement of longitudinal llm use in-the-wild cathy mengying fang sheer karny chayapatr archiwaranguprok yasith samaradivakara pat pataranutaporn pattie maes alignment research on large language models (llms) increasingly depends on understanding how these systems are used in everyday contexts. yet naturalistic interaction data is difficult to access due to privacy constraints and platform control. we present ai-wrapped, a prototype workflow for collecting naturalistic llm usage data while providing participants with an immediate ``wrapped&#x27;&#x27;-style report on their usage statistics, top topics, and safety-relevant behavioral patterns. we report findings from an initial deployment with 82 u.s.-based adults across 48,495 conversations from their 2025 histories. participants used llms for both instrumental and reflective purposes, including creative work, professional tasks, and emotional or existential themes. some usage patterns were consistent with potential over-reliance or perfectionistic refinement, while heavier users showed comparatively more reflective exchanges than primarily transactional ones. methodologically, even with zero data retention and pii removal, participants may remain hesitant to share chat data due to perceived privacy and judgment risks, underscoring the importance of trust, agency, and transparent design when building measurement infrastructure for alignment research.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18415v1" target="_blank" rel="noopener">AI-Wrapped: Participatory, Privacy-Preserving Measurement of Longitudinal LLM Use In-the-Wild</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.HC</span>
                </div>
                <p class="paper-authors">Cathy Mengying Fang, Sheer Karny, Chayapatr Archiwaranguprok, Yasith Samaradivakara, Pat Pataranutaporn <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-7">Alignment research on large language models (LLMs) increasingly depends on understanding how these systems are used in everyday contexts. yet naturalistic interaction data is difficult to access due...</p>
                <button class="expand-btn" onclick="toggleAbstract(7, 'Alignment research on large language models (LLMs) increasingly depends on understanding how these systems are used in everyday contexts. yet naturalistic interaction data is difficult to access due to privacy constraints and platform control. We present AI-Wrapped, a prototype workflow for collecting naturalistic LLM usage data while providing participants with an immediate ``wrapped&#x27;&#x27;-style report on their usage statistics, top topics, and safety-relevant behavioral patterns. We report findings from an initial deployment with 82 U.S.-based adults across 48,495 conversations from their 2025 histories. Participants used LLMs for both instrumental and reflective purposes, including creative work, professional tasks, and emotional or existential themes. Some usage patterns were consistent with potential over-reliance or perfectionistic refinement, while heavier users showed comparatively more reflective exchanges than primarily transactional ones. Methodologically, even with zero data retention and PII removal, participants may remain hesitant to share chat data due to perceived privacy and judgment risks, underscoring the importance of trust, agency, and transparent design when building measurement infrastructure for alignment research.', 'Alignment research on large language models (LLMs) increasingly depends on understanding how these systems are used in everyday contexts. yet naturalistic interaction data is difficult to access due...')" id="expand-btn-7">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18415v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18415v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="unifying approach to uniform expressivity of graph neural networks huan luo jonni virtema the expressive power of graph neural networks (gnns) is often analysed via correspondence to the weisfeiler-leman (wl) algorithm and fragments of first-order logic. standard gnns are limited to performing aggregation over immediate neighbourhoods or over global read-outs. to increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). in this paper, we formalize this architectural trend by introducing template gnns (t-gnns), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. we propose a corresponding logic, graded template modal logic (gml(t)), and generalized notions of template-based bisimulation and wl algorithm. we establish an equivalence between the expressive power of t-gnns and gml(t), and provide a unifying approach for analysing gnn expressivity: we show how standard ac-gnns and its recent variants can be interpreted as instantiations of t-gnns.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18409v1" target="_blank" rel="noopener">Unifying approach to uniform expressivity of graph neural networks</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.LO</span>
                </div>
                <p class="paper-authors">Huan Luo, Jonni Virtema</p>
                <p class="paper-abstract" id="abstract-8">The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to...</p>
                <button class="expand-btn" onclick="toggleAbstract(8, 'The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.', 'The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to...')" id="expand-btn-8">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18409v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18409v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="latent equivariant operators for robust object recognition: promise and challenges minh dinh stÃ©phane deny despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. an alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. here, using simple datasets of rotated and translated noisy mnist, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. while conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18406v1" target="_blank" rel="noopener">Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.CV</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Minh Dinh, StÃ©phane Deny</p>
                <p class="paper-abstract" id="abstract-9">Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example...</p>
                <button class="expand-btn" onclick="toggleAbstract(9, 'Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.', 'Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example...')" id="expand-btn-9">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18406v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18406v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="scientific knowledge-guided machine learning for vessel power prediction: a comparative study orfeas bourchas george papalambrou accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. conventional machine learning approaches, such as support vector machines, variants of artificial neural networks (anns), and tree-based methods like random forests, extra tree regressors, and xgboost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. this study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. the baseline component, derived from calm-water power curves of the form $p = cv^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. by constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. in this study, an xgboost, a simple neural network, and a physics-informed neural network (pinn) coupled with the baseline component were compared to identical models without the baseline component. validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. the proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18403v1" target="_blank" rel="noopener">Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Orfeas Bourchas, George Papalambrou</p>
                <p class="paper-abstract" id="abstract-10">Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as...</p>
                <button class="expand-btn" onclick="toggleAbstract(10, 'Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.', 'Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as...')" id="expand-btn-10">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18403v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18403v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="leakage and second-order dynamics improve hippocampal rnn replay josue casco-rodriguez nanda h. krishna richard g. baraniuk biological neural networks (like the hippocampus) can internally generate &quot;replay&quot; resembling stimulus-driven activity. recent computational models of replay use noisy recurrent neural networks (rnns) trained to path-integrate. replay in these networks has been described as langevin sampling, but new modifiers of noisy rnn replay have surpassed this description. we re-examine noisy rnn replay as sampling to understand or improve it in three ways: (1) under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in rnns for replay. (2) we confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-markov sampling that also slows replay. (3) we propose the first model of temporally compressed replay in noisy path-integrating rnns through hidden state momentum, connect it to underdamped langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. we verify our findings via path-integration of 2d triangular and t-maze paths and of high-dimensional paths of synthetic rat place cell activity.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18401v1" target="_blank" rel="noopener">Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span><span class="paper-category">q-bio.NC</span>
                </div>
                <p class="paper-authors">Josue Casco-Rodriguez, Nanda H. Krishna, Richard G. Baraniuk</p>
                <p class="paper-abstract" id="abstract-11">Biological neural networks (like the hippocampus) can internally generate &quot;replay&quot; resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks...</p>
                <button class="expand-btn" onclick="toggleAbstract(11, 'Biological neural networks (like the hippocampus) can internally generate &quot;replay&quot; resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.', 'Biological neural networks (like the hippocampus) can internally generate &quot;replay&quot; resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks...')" id="expand-btn-11">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18401v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18401v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="prism-fcp: byzantine-resilient federated conformal prediction via partial sharing ehsan lari reza arablouei stefan werner we propose prism-fcp (partial sharing and robust calibration with statistical margins for federated conformal prediction), a byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against byzantine attacks during both model training and conformal calibration. existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. in contrast, prism-fcp mitigates attacks end-to-end. during training, clients partially share updates by transmitting only $m$ of $d$ parameters per round. this attenuates the expected energy of an adversary&#x27;s perturbation in the aggregated update by a factor of $m/d$, yielding lower mean-square error (mse) and tighter prediction intervals. during calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected byzantine contributions before estimating the conformal quantile. extensive experiments on both synthetic data and the uci superconductivity dataset demonstrate that prism-fcp maintains nominal coverage guarantees under byzantine attacks while avoiding the interval inflation observed in standard fcp with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18396v1" target="_blank" rel="noopener">PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">eess.SP</span><span class="paper-category">math.PR</span>
                </div>
                <p class="paper-authors">Ehsan Lari, Reza Arablouei, Stefan Werner</p>
                <p class="paper-abstract" id="abstract-12">We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes...</p>
                <button class="expand-btn" onclick="toggleAbstract(12, 'We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary&#x27;s perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.', 'We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes...')" id="expand-btn-12">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18396v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18396v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="learning to tune pure pursuit in autonomous racing: joint lookahead and steering-gain control with ppo mohamed elgouhary amr s. el-wakeel pure pursuit (pp) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. we propose a reinforcement-learning (rl) approach that jointly chooses the lookahead ld and a steering gain g online using proximal policy optimization (ppo). the policy observes compact state features (speed and curvature taps) and outputs (ld, g) at each control step. trained in f1tenth gym and deployed in a ros 2 stack, the policy drives pp directly (with light smoothing) and requires no per-map retuning. across simulation and real-car tests, the proposed rl-pp controller that jointly selects (ld, g) consistently outperforms fixed-lookahead pp, velocity-scheduled adaptive pp, and an rl lookahead-only variant, and it also exceeds a kinematic mpc raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18386v1" target="_blank" rel="noopener">Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.RO</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Mohamed Elgouhary, Amr S. El-Wakeel</p>
                <p class="paper-abstract" id="abstract-13">Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead...</p>
                <button class="expand-btn" onclick="toggleAbstract(13, 'Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.', 'Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead...')" id="expand-btn-13">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18386v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18386v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="fedzmg: efficient client-side optimization in federated learning fotios zantalis evangelos zervas grigorios koulouras federated learning (fl) enables distributed model training on edge devices while preserving data privacy. however, clients tend to have non-independent and identically distributed (non-iid) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. while adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained iot environments. this paper introduces federated zero mean gradients (fedzmg), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. advancing the idea of gradient centralization, fedzmg projects local gradients onto a zero-mean hyperplane, effectively neutralizing the &quot;intensity&quot; or &quot;bias&quot; shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. a theoretical analysis is provided, proving that fedzmg reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard fedavg. extensive empirical evaluations on emnist, cifar100, and shakespeare datasets demonstrate that fedzmg achieves better convergence speed and final validation accuracy compared to the baseline fedavg and the adaptive optimizer fedadam, particularly in highly non-iid settings.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18384v1" target="_blank" rel="noopener">FedZMG: Efficient Client-Side Optimization in Federated Learning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Fotios Zantalis, Evangelos Zervas, Grigorios Koulouras</p>
                <p class="paper-abstract" id="abstract-14">Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data,...</p>
                <button class="expand-btn" onclick="toggleAbstract(14, 'Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the &quot;intensity&quot; or &quot;bias&quot; shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.', 'Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data,...')" id="expand-btn-14">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18384v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18384v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="theory and interpretability of quantum extreme learning machines: a pauli-transfer matrix approach markus gross hans-martin rieser quantum reservoir computers (qrcs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to train. here, we consider n-qubit quantum extreme learning machines (qelms) with continuous-time reservoir dynamics. qelms are memoryless qrcs capable of various ml tasks, including image classification and time series forecasting. we apply the pauli transfer matrix (ptm) formalism to theoretically analyze the influence of encoding, reservoir dynamics, and measurement operations, including temporal multiplexing, on the qelm performance. this formalism makes explicit that the encoding determines the complete set of (nonlinear) features available to the qelm, while the quantum channels linearly transform these features before they are probed by the chosen measurement operators. optimizing a qelm can therefore be cast as a decoding problem in which one shapes the channel-induced transformations such that task-relevant features become available to the regressor. the ptm formalism allows one to identify the classical representation of a qelm and thereby guide its design towards a given training objective. as a specific application, we focus on learning nonlinear dynamical systems and show that a qelm trained on such trajectories learns a surrogate-approximation to the underlying flow map.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18377v1" target="_blank" rel="noopener">Theory and interpretability of Quantum Extreme Learning Machines: a Pauli-transfer matrix approach</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">quant-ph</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Markus Gross, Hans-Martin Rieser</p>
                <p class="paper-abstract" id="abstract-15">Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to...</p>
                <button class="expand-btn" onclick="toggleAbstract(15, 'Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to train. Here, we consider n-qubit quantum extreme learning machines (QELMs) with continuous-time reservoir dynamics. QELMs are memoryless QRCs capable of various ML tasks, including image classification and time series forecasting. We apply the Pauli transfer matrix (PTM) formalism to theoretically analyze the influence of encoding, reservoir dynamics, and measurement operations, including temporal multiplexing, on the QELM performance. This formalism makes explicit that the encoding determines the complete set of (nonlinear) features available to the QELM, while the quantum channels linearly transform these features before they are probed by the chosen measurement operators. Optimizing a QELM can therefore be cast as a decoding problem in which one shapes the channel-induced transformations such that task-relevant features become available to the regressor. The PTM formalism allows one to identify the classical representation of a QELM and thereby guide its design towards a given training objective. As a specific application, we focus on learning nonlinear dynamical systems and show that a QELM trained on such trajectories learns a surrogate-approximation to the underlying flow map.', 'Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to...')" id="expand-btn-15">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18377v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18377v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="&quot;how do i ...?&quot;: procedural questions predominate student-llm chatbot conversations alexandra neagu marcus messer peter johnson rhodri nelson providing scaffolding through educational chatbots built on large language models (llm) has potential risks and benefits that remain an open area of research. when students navigate impasses, they ask for help by formulating impasse-driven questions. within interactions with llm chatbots, such questions shape the user prompts and drive the pedagogical effectiveness of the chatbot&#x27;s response. this paper focuses on such student questions from two datasets of distinct learning contexts: formative self-study, and summative assessed coursework. we analysed 6,113 messages from both learning contexts, using 11 different llms and three human raters to classify student questions using four existing schemas. on the feasibility of using llms as raters, results showed moderate-to-good inter-rater reliability, with higher consistency than human raters. the data showed that &#x27;procedural&#x27; questions predominated in both learning contexts, but more so when students prepare for summative assessment. these results provide a basis on which to use llms for classification of student questions. however, we identify clear limitations in both the ability to classify with schemas and the value of doing so: schemas are limited and thus struggle to accommodate the semantic richness of composite prompts, offering only partial understanding the wider risks and benefits of chatbot integration. in the future, we recommend an analysis approach that captures the nuanced, multi-turn nature of conversation, for example, by applying methods from conversation analysis in discursive psychology.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18372v1" target="_blank" rel="noopener">&quot;How Do I ...?&quot;: Procedural Questions Predominate Student-LLM Chatbot Conversations</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.HC</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Alexandra Neagu, Marcus Messer, Peter Johnson, Rhodri Nelson</p>
                <p class="paper-abstract" id="abstract-16">Providing scaffolding through educational chatbots built on Large Language Models (LLM) has potential risks and benefits that remain an open area of research. When students navigate impasses, they...</p>
                <button class="expand-btn" onclick="toggleAbstract(16, 'Providing scaffolding through educational chatbots built on Large Language Models (LLM) has potential risks and benefits that remain an open area of research. When students navigate impasses, they ask for help by formulating impasse-driven questions. Within interactions with LLM chatbots, such questions shape the user prompts and drive the pedagogical effectiveness of the chatbot&#x27;s response. This paper focuses on such student questions from two datasets of distinct learning contexts: formative self-study, and summative assessed coursework. We analysed 6,113 messages from both learning contexts, using 11 different LLMs and three human raters to classify student questions using four existing schemas. On the feasibility of using LLMs as raters, results showed moderate-to-good inter-rater reliability, with higher consistency than human raters. The data showed that &#x27;procedural&#x27; questions predominated in both learning contexts, but more so when students prepare for summative assessment. These results provide a basis on which to use LLMs for classification of student questions. However, we identify clear limitations in both the ability to classify with schemas and the value of doing so: schemas are limited and thus struggle to accommodate the semantic richness of composite prompts, offering only partial understanding the wider risks and benefits of chatbot integration. In the future, we recommend an analysis approach that captures the nuanced, multi-turn nature of conversation, for example, by applying methods from conversation analysis in discursive psychology.', 'Providing scaffolding through educational chatbots built on Large Language Models (LLM) has potential risks and benefits that remain an open area of research. When students navigate impasses, they...')" id="expand-btn-16">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18372v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18372v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="quantum maximum likelihood prediction via hilbert space embeddings sreejith sreekumar nir weinberger recent works have proposed various explanations for the ability of modern large language models (llms) to perform in-context prediction. we propose an alternative conceptual viewpoint from an information-geometric and statistical perspective. motivated by bach[2023], we model training as learning an embedding of probability distributions into the space of quantum density operators, and in-context learning as maximum-likelihood prediction over a specified class of quantum models. we provide an interpretation of this predictor in terms of quantum reverse information projection and quantum pythagorean theorem when the class of quantum models is sufficiently expressive. we further derive non-asymptotic performance guarantees in terms of convergence rates and concentration inequalities, both in trace norm and quantum relative entropy. our approach provides a unified framework to handle both classical and quantum llms.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18364v1" target="_blank" rel="noopener">Quantum Maximum Likelihood Prediction via Hilbert Space Embeddings</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.IT</span><span class="paper-category">cs.LG</span><span class="paper-category">quant-ph</span>
                </div>
                <p class="paper-authors">Sreejith Sreekumar, Nir Weinberger</p>
                <p class="paper-abstract" id="abstract-17">Recent works have proposed various explanations for the ability of modern large language models (LLMs) to perform in-context prediction. We propose an alternative conceptual viewpoint from an...</p>
                <button class="expand-btn" onclick="toggleAbstract(17, 'Recent works have proposed various explanations for the ability of modern large language models (LLMs) to perform in-context prediction. We propose an alternative conceptual viewpoint from an information-geometric and statistical perspective. Motivated by Bach[2023], we model training as learning an embedding of probability distributions into the space of quantum density operators, and in-context learning as maximum-likelihood prediction over a specified class of quantum models. We provide an interpretation of this predictor in terms of quantum reverse information projection and quantum Pythagorean theorem when the class of quantum models is sufficiently expressive. We further derive non-asymptotic performance guarantees in terms of convergence rates and concentration inequalities, both in trace norm and quantum relative entropy. Our approach provides a unified framework to handle both classical and quantum LLMs.', 'Recent works have proposed various explanations for the ability of modern large language models (LLMs) to perform in-context prediction. We propose an alternative conceptual viewpoint from an...')" id="expand-btn-17">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18364v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18364v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="qualitative coding analysis through open-source large language models: a user study and design recommendations tung t. ngo dai nguyen van anh-minh nguyen phuong-anh do anh nguyen-quoc qualitative data analysis is labor-intensive, yet the privacy risks associated with commercial large language models (llms) often preclude their use in sensitive research. to address this, we introduce chatqda, an on-device framework powered by open-source llms designed for privacy-preserving open coding. our mixed-methods user study reveals that while participants rated the system highly for usability and perceived efficiency, they exhibited &quot;conditional trust&quot;, valuing the tool for surface-level extraction while questioning its interpretive nuance and consistency. furthermore, despite the technical security of local deployment, participants reported epistemic uncertainty regarding data protection, suggesting that invisible security measures are insufficient to foster trust. we conclude with design recommendations for local-first analysis tools that prioritize verifiable privacy and methodological rigor.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18352v1" target="_blank" rel="noopener">Qualitative Coding Analysis through Open-Source Large Language Models: A User Study and Design Recommendations</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">cs.HC</span><span class="paper-category">cs.CR</span><span class="paper-category">cs.SE</span>
                </div>
                <p class="paper-authors">Tung T. Ngo, Dai Nguyen Van, Anh-Minh Nguyen, Phuong-Anh Do, Anh Nguyen-Quoc</p>
                <p class="paper-abstract" id="abstract-18">Qualitative data analysis is labor-intensive, yet the privacy risks associated with commercial Large Language Models (LLMs) often preclude their use in sensitive research. To address this, we...</p>
                <button class="expand-btn" onclick="toggleAbstract(18, 'Qualitative data analysis is labor-intensive, yet the privacy risks associated with commercial Large Language Models (LLMs) often preclude their use in sensitive research. To address this, we introduce ChatQDA, an on-device framework powered by open-source LLMs designed for privacy-preserving open coding. Our mixed-methods user study reveals that while participants rated the system highly for usability and perceived efficiency, they exhibited &quot;conditional trust&quot;, valuing the tool for surface-level extraction while questioning its interpretive nuance and consistency. Furthermore, despite the technical security of local deployment, participants reported epistemic uncertainty regarding data protection, suggesting that invisible security measures are insufficient to foster trust. We conclude with design recommendations for local-first analysis tools that prioritize verifiable privacy and methodological rigor.', 'Qualitative data analysis is labor-intensive, yet the privacy risks associated with commercial Large Language Models (LLMs) often preclude their use in sensitive research. To address this, we...')" id="expand-btn-18">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18352v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18352v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="quantum-enhanced satellite image classification qi zhang anton simen carlos flores-garrigÃ³s gabriel alvarado barrios paolo a. erdman enrique solano aaron c. kemp vincent beltrani vedangi pathak hamed mohammadbagherpoor we demonstrate the application of a quantum feature extraction method to enhance multi-class image classification for space applications. by harnessing the dynamics of many-body spin hamiltonians, the method generates expressive quantum features that, when combined with classical processing, lead to quantum-enhanced classification accuracy. using a strong and well-established resnet50 baseline, we achieved a maximum classical accuracy of 83%, which can be improved to 84% with a transfer learning approach. in contrast, applying our quantum-classical method the performance is increased to 87% accuracy, demonstrating a clear and reproducible improvement over robust classical approaches. implemented on several of ibm&#x27;s quantum processors, our hybrid quantum-classical approach delivers consistent gains of 2-3% in absolute accuracy. these results highlight the practical potential of current and near-term quantum processors in high-stakes, data-driven domains such as satellite imaging and remote sensing, while suggesting broader applicability in real-world machine learning tasks.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.18350v1" target="_blank" rel="noopener">Quantum-enhanced satellite image classification</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-20</span>
                    <span class="paper-category">quant-ph</span><span class="paper-category">cs.CV</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Qi Zhang, Anton Simen, Carlos Flores-GarrigÃ³s, Gabriel Alvarado Barrios, Paolo A. Erdman <em>(+5 more)</em></p>
                <p class="paper-abstract" id="abstract-19">We demonstrate the application of a quantum feature extraction method to enhance multi-class image classification for space applications. By harnessing the dynamics of many-body spin Hamiltonians,...</p>
                <button class="expand-btn" onclick="toggleAbstract(19, 'We demonstrate the application of a quantum feature extraction method to enhance multi-class image classification for space applications. By harnessing the dynamics of many-body spin Hamiltonians, the method generates expressive quantum features that, when combined with classical processing, lead to quantum-enhanced classification accuracy. Using a strong and well-established ResNet50 baseline, we achieved a maximum classical accuracy of 83%, which can be improved to 84% with a transfer learning approach. In contrast, applying our quantum-classical method the performance is increased to 87% accuracy, demonstrating a clear and reproducible improvement over robust classical approaches. Implemented on several of IBM&#x27;s quantum processors, our hybrid quantum-classical approach delivers consistent gains of 2-3% in absolute accuracy. These results highlight the practical potential of current and near-term quantum processors in high-stakes, data-driven domains such as satellite imaging and remote sensing, while suggesting broader applicability in real-world machine learning tasks.', 'We demonstrate the application of a quantum feature extraction method to enhance multi-class image classification for space applications. By harnessing the dynamics of many-body spin Hamiltonians,...')" id="expand-btn-19">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.18350v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.18350v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

        </div>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2026 Coding Blog | BST 236 Computing I | Harvard University</p>
            <p class="footer-links">
                <a href="https://github.com">GitHub</a>
            </p>
        </div>
    </footer>

    <script src="js/main.js"></script>
    <script>
        function filterPapers() {
            const query = document.getElementById('searchInput').value.toLowerCase();
            const cards = document.querySelectorAll('.paper-card');
            
            cards.forEach(card => {
                const searchText = card.getAttribute('data-search');
                if (searchText.includes(query)) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });
        }
        
        function toggleAbstract(index, fullText, shortText) {
            const abstractEl = document.getElementById('abstract-' + index);
            const btnEl = document.getElementById('expand-btn-' + index);
            
            if (abstractEl.classList.contains('expanded')) {
                abstractEl.textContent = shortText;
                abstractEl.classList.remove('expanded');
                btnEl.textContent = 'Show more â–¼';
            } else {
                abstractEl.textContent = fullText;
                abstractEl.classList.add('expanded');
                btnEl.textContent = 'Show less â–²';
            }
        }
        
        // Client-side arXiv fetching for custom keywords
        // CORS proxies to try in order
        const corsProxies = [
            url => `https://corsproxy.io/?${encodeURIComponent(url)}`,
            url => `https://api.codetabs.com/v1/proxy?quest=${encodeURIComponent(url)}`,
            url => `https://api.allorigins.win/raw?url=${encodeURIComponent(url)}`
        ];
        
        async function tryFetchWithProxies(url, proxies) {
            for (let i = 0; i < proxies.length; i++) {
                const proxyUrl = proxies[i](url);
                try {
                    const response = await fetch(proxyUrl, { timeout: 10000 });
                    if (response.ok) {
                        return await response.text();
                    }
                } catch (e) {
                    console.log(`Proxy ${i + 1} failed:`, e.message);
                }
            }
            throw new Error('All CORS proxies failed. Please try again later or edit scripts/config.json directly.');
        }
        
        async function fetchCustomPapers() {
            const input = document.getElementById('customKeywords').value.trim();
            const btn = document.getElementById('fetchBtn');
            const grid = document.getElementById('papersGrid');
            const keywordsDisplay = document.getElementById('currentKeywords');
            
            if (!input) {
                alert('Please enter at least one keyword');
                return;
            }
            
            const keywords = input.split(',').map(k => k.trim()).filter(k => k);
            
            btn.disabled = true;
            btn.textContent = 'Fetching...';
            
            // Update displayed keywords
            keywordsDisplay.innerHTML = keywords.map(kw => 
                `<span class="keyword-tag">${escapeHtml(kw)}</span>`
            ).join('');
            
            // Build arXiv query
            const searchQuery = keywords.map(kw => `all:"${kw}"`).join(' OR ');
            const url = `https://export.arxiv.org/api/query?search_query=${encodeURIComponent(searchQuery)}&start=0&max_results=20&sortBy=submittedDate&sortOrder=descending`;
            
            try {
                const xmlText = await tryFetchWithProxies(url, corsProxies);
                
                // Parse XML
                const parser = new DOMParser();
                const xmlDoc = parser.parseFromString(xmlText, 'text/xml');
                const entries = xmlDoc.querySelectorAll('entry');
                
                if (entries.length === 0) {
                    grid.innerHTML = `
                        <div class="no-results" style="grid-column: 1 / -1;">
                            <h3>No papers found</h3>
                            <p>Try different keywords.</p>
                        </div>
                    `;
                } else {
                    let html = '';
                    entries.forEach((entry, i) => {
                        const title = entry.querySelector('title')?.textContent?.replace(/\s+/g, ' ').trim() || 'Untitled';
                        const abstract = entry.querySelector('summary')?.textContent?.replace(/\s+/g, ' ').trim() || 'No abstract';
                        const published = entry.querySelector('published')?.textContent?.substring(0, 10) || 'Unknown';
                        const id = entry.querySelector('id')?.textContent || '';
                        const pdfUrl = id.replace('/abs/', '/pdf/') + '.pdf';
                        
                        const authors = [];
                        entry.querySelectorAll('author name').forEach(n => authors.push(n.textContent));
                        const authorsStr = authors.length > 5 
                            ? authors.slice(0, 5).join(', ') + ` <em>(+${authors.length - 5} more)</em>`
                            : authors.join(', ');
                        
                        const categories = [];
                        entry.querySelectorAll('category').forEach(c => {
                            const term = c.getAttribute('term');
                            if (term && categories.length < 3) categories.push(term);
                        });
                        
                        const abstractShort = abstract.length > 200 
                            ? abstract.substring(0, 200).replace(/\s+\S*$/, '') + '...'
                            : abstract;
                        
                        html += `
                            <article class="paper-card" data-search="${escapeHtml(title.toLowerCase())} ${escapeHtml(authors.join(' ').toLowerCase())} ${escapeHtml(abstract.toLowerCase())}">
                                <h2 class="paper-title">
                                    <a href="${escapeHtml(id)}" target="_blank" rel="noopener">${escapeHtml(title)}</a>
                                </h2>
                                <div class="paper-meta">
                                    <span class="paper-date">ğŸ“… ${escapeHtml(published)}</span>
                                    ${categories.map(c => `<span class="paper-category">${escapeHtml(c)}</span>`).join('')}
                                </div>
                                <p class="paper-authors">${authorsStr}</p>
                                <p class="paper-abstract" id="abstract-dyn-${i}">${escapeHtml(abstractShort)}</p>
                                <button class="expand-btn" onclick="toggleAbstract('dyn-${i}', '${escapeHtml(abstract).replace(/'/g, "\\'")}', '${escapeHtml(abstractShort).replace(/'/g, "\\'")}')">Show more â–¼</button>
                                <div class="paper-actions">
                                    <a href="${escapeHtml(pdfUrl)}" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                                    <a href="${escapeHtml(id)}" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                                </div>
                            </article>
                        `;
                    });
                    grid.innerHTML = html;
                }
                
                // Update timestamp
                const now = new Date().toISOString().replace('T', ' ').substring(0, 19) + ' (live fetch)';
                document.querySelector('.update-info').textContent = 'Last updated: ' + now;
                
            } catch (error) {
                console.error('Fetch error:', error);
                grid.innerHTML = `
                    <div class="no-results" style="grid-column: 1 / -1;">
                        <h3>Error fetching papers</h3>
                        <p>${escapeHtml(error.message)}</p>
                        <p style="margin-top: 15px; font-size: 0.9rem;">
                            <strong>Alternative:</strong> Edit <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">scripts/config.json</code> 
                            and run <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">python scripts/fetch_arxiv.py</code> locally, 
                            or push to GitHub to trigger the auto-update.
                        </p>
                    </div>
                `;
            }
            
            btn.disabled = false;
            btn.textContent = 'Fetch Papers';
        }
        
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }
        
        // Allow Enter key to trigger fetch
        document.getElementById('customKeywords').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') fetchCustomPapers();
        });
    </script>
</body>
</html>
