<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Feed | Coding Blog</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <style>
        .papers-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 100px 20px 40px;
        }
        .papers-header {
            text-align: center;
            margin-bottom: 40px;
        }
        .papers-header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .update-info {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 8px;
        }
        .auto-update-info {
            color: #22c55e;
            font-size: 0.85rem;
            margin-bottom: 15px;
        }
        .config-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-bottom: 10px;
        }
        .config-hint code {
            background: #334155;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: var(--font-code);
            color: #f472b6;
        }
        .search-box {
            max-width: 500px;
            margin: 20px auto;
        }
        .search-box input {
            width: 100%;
            padding: 12px 20px;
            border: 2px solid var(--border);
            border-radius: 25px;
            background: var(--card-bg);
            color: var(--text-primary);
            font-size: 1rem;
            outline: none;
            transition: border-color 0.3s;
        }
        .search-box input:focus {
            border-color: var(--primary);
        }
        .search-box input::placeholder {
            color: var(--text-muted);
        }
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 25px;
        }
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 25px;
            transition: all 0.3s ease;
        }
        .paper-card:hover {
            border-color: var(--primary);
            transform: translateY(-3px);
            box-shadow: 0 10px 30px rgba(236, 72, 153, 0.15);
        }
        .paper-card.hidden {
            display: none;
        }
        .paper-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-bottom: 10px;
            line-height: 1.4;
        }
        .paper-title a {
            color: inherit;
            text-decoration: none;
            transition: color 0.3s;
        }
        .paper-title a:hover {
            color: var(--primary);
        }
        .paper-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 12px;
        }
        .paper-date {
            background: var(--primary);
            color: white;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
            font-weight: 500;
        }
        .paper-category {
            background: #334155;
            color: #e2e8f0;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
        }
        .paper-authors {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 12px;
            line-height: 1.5;
        }
        .paper-abstract {
            color: #94a3b8;
            font-size: 0.9rem;
            line-height: 1.7;
            margin-bottom: 15px;
        }
        .paper-abstract.expanded {
            max-height: none;
        }
        .expand-btn {
            background: none;
            border: none;
            color: var(--primary);
            cursor: pointer;
            font-size: 0.85rem;
            padding: 0;
            margin-bottom: 15px;
        }
        .expand-btn:hover {
            text-decoration: underline;
        }
        .paper-actions {
            display: flex;
            gap: 10px;
        }
        .pdf-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            transition: all 0.3s;
        }
        .pdf-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 15px rgba(236, 72, 153, 0.3);
        }
        .arxiv-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: var(--surface);
            color: var(--text-primary);
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            border: 1px solid var(--border);
            transition: all 0.3s;
        }
        .arxiv-btn:hover {
            border-color: var(--primary);
            color: var(--primary);
        }
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: var(--text-muted);
        }
        .no-results h3 {
            font-size: 1.5rem;
            margin-bottom: 10px;
            color: var(--text-secondary);
        }
        .keywords-info {
            background: #1e293b;
            padding: 20px;
            border-radius: 12px;
            margin-bottom: 30px;
            text-align: center;
            border: 1px solid #334155;
        }
        .keywords-info > span {
            color: #e2e8f0;
            font-size: 0.95rem;
            font-weight: 500;
        }
        .keyword-tag {
            display: inline-block;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: #ffffff;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin: 4px;
            text-shadow: 0 1px 2px rgba(0,0,0,0.2);
        }
        .keyword-editor {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #334155;
        }
        .keyword-input-group {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .keyword-input {
            padding: 10px 16px;
            border: 2px solid #334155;
            border-radius: 25px;
            background: #0f172a;
            color: #f8fafc;
            font-size: 0.9rem;
            width: 250px;
            outline: none;
            transition: border-color 0.3s;
        }
        .keyword-input:focus {
            border-color: #6366f1;
        }
        .keyword-input::placeholder {
            color: #64748b;
        }
        .fetch-btn {
            padding: 10px 24px;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: white;
            border: none;
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .fetch-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 20px rgba(99, 102, 241, 0.4);
        }
        .fetch-btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }
        .editor-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-top: 8px;
        }
        @media (max-width: 768px) {
            .papers-grid {
                grid-template-columns: 1fr;
            }
            .papers-header h1 {
                font-size: 1.8rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="index.html" class="nav-logo">
                <span class="logo-icon">ğŸ’»</span>
                <span class="logo-text">Coding Blog</span>
            </a>
            <ul class="nav-menu">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li><a href="index.html#projects" class="nav-link">Projects</a></li>
                <li><a href="papers.html" class="nav-link active">Papers</a></li>
                <li><a href="index.html#about" class="nav-link">About</a></li>
            </ul>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <div class="papers-container">
        <div class="papers-header">
            <h1>ğŸ“š arXiv Paper Feed</h1>
            <p class="update-info">Last updated: 2026-02-27 01:29:07 UTC</p>
            <p class="auto-update-info">â° Auto-updates daily at midnight UTC via GitHub Actions</p>
            <div class="keywords-info">
                <span>Current keywords: </span>
                <div id="currentKeywords">
                    <span class="keyword-tag">large language model</span><span class="keyword-tag">machine learning</span><span class="keyword-tag">biostatistics</span><span class="keyword-tag">deep learning</span>
                </div>
                <div class="keyword-editor">
                    <p class="editor-hint">ğŸ”„ Try different keywords (fetches live from arXiv):</p>
                    <div class="keyword-input-group">
                        <input type="text" id="customKeywords" class="keyword-input" 
                               placeholder="e.g., reinforcement learning, NLP" 
                               value="large language model, machine learning, biostatistics, deep learning">
                        <button onclick="fetchCustomPapers()" class="fetch-btn" id="fetchBtn">
                            Fetch Papers
                        </button>
                    </div>
                    <p class="editor-hint">Separate multiple keywords with commas</p>
                </div>
            </div>
            <div class="search-box">
                <input type="text" id="searchInput" placeholder="ğŸ” Filter papers by title, author, or abstract..." oninput="filterPapers()">
            </div>
        </div>

        <div class="papers-grid" id="papersGrid">

            <article class="paper-card" data-search="recovered in translation: efficient pipeline for automated translation of benchmarks and datasets hanna yukhymenko anton alexandrov martin vechev the reliability of multilingual large language model (llm) evaluation is currently compromised by the inconsistent quality of translated benchmarks. existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. in this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. we demonstrate that adapting test-time compute scaling strategies, specifically universal self-improvement (usi) and our proposed multi-round ranking method, t-rank, allows for significantly higher quality outputs compared to traditional pipelines. our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. we apply this approach to translate popular benchmarks and datasets into eight eastern and southern european languages (ukrainian, bulgarian, slovak, romanian, lithuanian, estonian, turkish, greek). evaluations using both reference-based metrics and llm-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. we release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual ai development.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22207v1" target="_blank" rel="noopener">Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Hanna Yukhymenko, Anton Alexandrov, Martin Vechev</p>
                <p class="paper-abstract" id="abstract-0">The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic...</p>
                <button class="expand-btn" onclick="toggleAbstract(0, 'The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.', 'The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic...')" id="expand-btn-0">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22207v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22207v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="sumtablets: a transliteration dataset of sumerian tablets cole simmons richard diehl martinez dan jurafsky sumerian transliteration is a conventional system for representing a scholar&#x27;s interpretation of a tablet in the latin script. thanks to visionary digital assyriology projects such as etcsl, cdli, and oracc, a large number of sumerian transliterations have been published online, and these data are well-structured for a variety of search and analysis tasks. however, the absence of a comprehensive, accessible dataset pairing transliterations with a digital representation of the tablet&#x27;s cuneiform glyphs has prevented the application of modern natural language processing (nlp) methods to the task of sumerian transliteration. to address this gap, we present sumtablets, a dataset pairing unicode representations of 91,606 sumerian cuneiform tablets (totaling 6,970,407 glyphs) with the associated transliterations published by oracc. we construct sumtablets by first preprocessing and standardizing the oracc transliterations before mapping each reading back to the unicode representation of the source glyph. further, we retain parallel structural information (e.g., surfaces, newlines, broken segments) through the use of special tokens. we release sumtablets as a hugging face dataset (cc by 4.0) and open source data preparation code via github. additionally, we leverage sumtablets to implement and evaluate two transliteration baselines: (1) weighted sampling from a glyph&#x27;s possible readings, and (2) fine-tuning an autoregressive language model. our fine-tuned language model achieves an average transliteration character-level f-score (chrf) of 97.55, demonstrating the immediate potential of transformer-based transliteration models in allowing experts to rapidly verify generated transliterations rather than manually transliterating tablets one-by-one.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22200v1" target="_blank" rel="noopener">SumTablets: A Transliteration Dataset of Sumerian Tablets</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Cole Simmons, Richard Diehl Martinez, Dan Jurafsky</p>
                <p class="paper-abstract" id="abstract-1">Sumerian transliteration is a conventional system for representing a scholar&#x27;s interpretation of a tablet in the Latin script. Thanks to visionary digital Assyriology projects such as ETCSL, CDLI,...</p>
                <button class="expand-btn" onclick="toggleAbstract(1, 'Sumerian transliteration is a conventional system for representing a scholar&#x27;s interpretation of a tablet in the Latin script. Thanks to visionary digital Assyriology projects such as ETCSL, CDLI, and Oracc, a large number of Sumerian transliterations have been published online, and these data are well-structured for a variety of search and analysis tasks. However, the absence of a comprehensive, accessible dataset pairing transliterations with a digital representation of the tablet&#x27;s cuneiform glyphs has prevented the application of modern Natural Language Processing (NLP) methods to the task of Sumerian transliteration. To address this gap, we present SumTablets, a dataset pairing Unicode representations of 91,606 Sumerian cuneiform tablets (totaling 6,970,407 glyphs) with the associated transliterations published by Oracc. We construct SumTablets by first preprocessing and standardizing the Oracc transliterations before mapping each reading back to the Unicode representation of the source glyph. Further, we retain parallel structural information (e.g., surfaces, newlines, broken segments) through the use of special tokens. We release SumTablets as a Hugging Face Dataset (CC BY 4.0) and open source data preparation code via GitHub. Additionally, we leverage SumTablets to implement and evaluate two transliteration baselines: (1) weighted sampling from a glyph&#x27;s possible readings, and (2) fine-tuning an autoregressive language model. Our fine-tuned language model achieves an average transliteration character-level F-score (chrF) of 97.55, demonstrating the immediate potential of transformer-based transliteration models in allowing experts to rapidly verify generated transliterations rather than manually transliterating tablets one-by-one.', 'Sumerian transliteration is a conventional system for representing a scholar&#x27;s interpretation of a tablet in the Latin script. Thanks to visionary digital Assyriology projects such as ETCSL, CDLI,...')" id="expand-btn-1">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22200v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22200v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="off-the-shelf image-to-image models are all you need to defeat image protection schemes xavier pleimling sifat muhammad abdullah gunjan balde peng gao mainack mondal murtuza jadliwala bimal viswanath advances in generative ai (genai) have led to the development of various protection strategies to prevent the unauthorized use of images. these methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. we show that off-the-shelf image-to-image genai models can be repurposed as generic ``denoisers&quot; using a simple text prompt, effectively removing a wide range of protective perturbations. across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image&#x27;s utility for the adversary. our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. we stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf genai models. code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22197v1" target="_blank" rel="noopener">Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.CV</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Xavier Pleimling, Sifat Muhammad Abdullah, Gunjan Balde, Peng Gao, Mainack Mondal <em>(+2 more)</em></p>
                <p class="paper-abstract" id="abstract-2">Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective...</p>
                <button class="expand-btn" onclick="toggleAbstract(2, 'Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers&quot; using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image&#x27;s utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser', 'Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective...')" id="expand-btn-2">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22197v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22197v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="gui-libra: training native gui agents to reason and act with action-aware supervision and partially verifiable rl rui yang qianhui wu zhaoyang wang hanyang chen ke yang hao cheng huaxiu yao baoling peng huan zhang jianfeng gao tong zhang open-source native gui agents still lag behind closed-source systems on long-horizon navigation tasks. this gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of gui agents. we identify two fundamental issues in these pipelines: (i) standard sft with cot reasoning often hurts grounding, and (ii) step-wise rlvr-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. this makes offline step-wise metrics weak predictors of online task success. in this work, we present gui-libra, a tailored training recipe that addresses these challenges. first, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81k gui reasoning dataset. second, to reconcile reasoning with grounding, we propose action-aware sft that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. third, to stabilize rl under partial verifiability, we identify the overlooked importance of kl regularization in rlvr and show that a kl trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. across diverse web and mobile benchmarks, gui-libra consistently improves both step-wise accuracy and end-to-end task completion. our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. we release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable gui agents.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22190v1" target="_blank" rel="noopener">GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Rui Yang, Qianhui Wu, Zhaoyang Wang, Hanyang Chen, Ke Yang <em>(+6 more)</em></p>
                <p class="paper-abstract" id="abstract-3">Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data,...</p>
                <button class="expand-btn" onclick="toggleAbstract(3, 'Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.', 'Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data,...')" id="expand-btn-3">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22190v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22190v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="surrogate models for rock-fluid interaction: a grid-size-invariant approach nathalie c. pinheiro donghu guo hannah p. menke aniket c. joshi claire e. heaney ahmed h. elsheikh christopher c. pain modelling rock-fluid interaction requires solving a set of partial differential equations (pdes) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. this restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. as a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. four of these are reduced-order models (rom) based on one neural network for compression and another for prediction. the other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. in addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of unet and unet++ architectures, and demonstrate that unet++ outperforms unet for surrogate models. furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the roms analysed. the application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22188v1" target="_blank" rel="noopener">Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span><span class="paper-category">physics.flu-dyn</span>
                </div>
                <p class="paper-authors">Nathalie C. Pinheiro, Donghu Guo, Hannah P. Menke, Aniket C. Joshi, Claire E. Heaney <em>(+2 more)</em></p>
                <p class="paper-abstract" id="abstract-4">Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces....</p>
                <button class="expand-btn" onclick="toggleAbstract(4, 'Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction.', 'Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces....')" id="expand-btn-4">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22188v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22188v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="learning and naming subgroups with exceptional survival characteristics mhd jawad al rahwanji sascha xu nils philipp walter jilles vreeken in many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. in medicine, for example, it allows determining which patients benefit from treatment, and in predictive maintenance, which components are more likely to fail. existing methods for discovering subgroups with exceptional survival characteristics require restrictive assumptions about the survival model (e.g. proportional hazards), pre-discretized features, and, as they compare average statistics, tend to overlook individual deviations. in this paper, we propose sysurv, a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves, automatically learns conditions and how to combine these into inherently interpretable rules, so as to select subgroups with exceptional survival characteristics. empirical evaluation on a wide range of datasets and settings, including a case study on cancer data, shows that sysurv reveals insightful and actionable survival subgroups.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22179v1" target="_blank" rel="noopener">Learning and Naming Subgroups with Exceptional Survival Characteristics</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Mhd Jawad Al Rahwanji, Sascha Xu, Nils Philipp Walter, Jilles Vreeken</p>
                <p class="paper-abstract" id="abstract-5">In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit...</p>
                <button class="expand-btn" onclick="toggleAbstract(5, 'In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit from treatment, and in predictive maintenance, which components are more likely to fail. Existing methods for discovering subgroups with exceptional survival characteristics require restrictive assumptions about the survival model (e.g. proportional hazards), pre-discretized features, and, as they compare average statistics, tend to overlook individual deviations. In this paper, we propose Sysurv, a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves, automatically learns conditions and how to combine these into inherently interpretable rules, so as to select subgroups with exceptional survival characteristics. Empirical evaluation on a wide range of datasets and settings, including a case study on cancer data, shows that Sysurv reveals insightful and actionable survival subgroups.', 'In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit...')" id="expand-btn-5">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22179v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22179v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="a taxonomy of human--mllm interaction in early-stage sketch-based design ideation weiayn shi kenny tsu wei choo as multimodal large language models (mllms) are increasingly integrated into early-stage design tools, it is important to understand how designers collaborate with ai during ideation. in a user study with 12 participants, we analysed sketch-based design interactions with an mllm-powered system using automatically recorded interaction logs and post-task interviews. based on how creative responsibility was allocated between humans and the ai, we predefined four interaction modes: human-only, human-lead, ai-lead, and co-evolution, and analysed how these modes manifested during sketch-based design ideation. our results show that designers rarely rely on a single mode; instead, human-led and ai-led roles are frequently interwoven and shift across ideation instances. these findings provide an empirical basis for future work to investigate why designers shift roles with ai and how interactive systems can better support such dynamic collaboration.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22171v1" target="_blank" rel="noopener">A Taxonomy of Human--MLLM Interaction in Early-Stage Sketch-Based Design Ideation</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.HC</span>
                </div>
                <p class="paper-authors">Weiayn Shi, Kenny Tsu Wei Choo</p>
                <p class="paper-abstract" id="abstract-6">As multimodal large language models (MLLMs) are increasingly integrated into early-stage design tools, it is important to understand how designers collaborate with AI during ideation. In a user study...</p>
                <button class="expand-btn" onclick="toggleAbstract(6, 'As multimodal large language models (MLLMs) are increasingly integrated into early-stage design tools, it is important to understand how designers collaborate with AI during ideation. In a user study with 12 participants, we analysed sketch-based design interactions with an MLLM-powered system using automatically recorded interaction logs and post-task interviews. Based on how creative responsibility was allocated between humans and the AI, we predefined four interaction modes: Human-Only, Human-Lead, AI-Lead, and Co-Evolution, and analysed how these modes manifested during sketch-based design ideation. Our results show that designers rarely rely on a single mode; instead, human-led and AI-led roles are frequently interwoven and shift across ideation instances. These findings provide an empirical basis for future work to investigate why designers shift roles with AI and how interactive systems can better support such dynamic collaboration.', 'As multimodal large language models (MLLMs) are increasingly integrated into early-stage design tools, it is important to understand how designers collaborate with AI during ideation. In a user study...')" id="expand-btn-6">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22171v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22171v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="llmtailor: a layer-wise tailoring tool for efficient checkpointing of large language models minqiu sun xin huang luanzheng guo nathan r. tallent kento sato dong dai checkpointing is essential for fault tolerance in training large language models (llms). however, existing methods, regardless of their i/o strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. recent studies reveal that updates across llm layers are highly non-uniform. across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. this suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. to address this gap, we propose \texttt{llmtailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. our evaluation indicates that llmtailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for llama3.1-8b) and checkpoint time (e.g., 2.8 times faster for qwen2.5-7b) while maintaining model quality.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22158v1" target="_blank" rel="noopener">LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.DC</span>
                </div>
                <p class="paper-authors">Minqiu Sun, Xin Huang, Luanzheng Guo, Nathan R. Tallent, Kento Sato <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-7">Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and...</p>
                <button class="expand-btn" onclick="toggleAbstract(7, 'Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality.', 'Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and...')" id="expand-btn-7">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22158v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22158v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="dynamic personality adaptation in large language models via state machines leon pielage ole hÃ¤tscher mitja back bernhard marschall benjamin risse the inability of large language models (llms) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. we propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or llms used. these scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.we evaluate this framework by operationalizing the interpersonal circumplex (ipc) in a medical education setting. results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale llms. this work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22157v1" target="_blank" rel="noopener">Dynamic Personality Adaptation in Large Language Models via State Machines</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">cs.HC</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Leon Pielage, Ole HÃ¤tscher, Mitja Back, Bernhard Marschall, Benjamin Risse</p>
                <p class="paper-abstract" id="abstract-8">The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose...</p>
                <button class="expand-btn" onclick="toggleAbstract(8, 'The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.', 'The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose...')" id="expand-btn-8">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22157v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22157v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="provable last-iterate convergence for multi-objective safe llm alignment via optimistic primal-dual yining li peizhong ju ness shroff reinforcement learning from human feedback (rlhf) plays a significant role in aligning large language models (llms) with human preferences. while rlhf with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. in this work, we propose a universal primal-dual framework for safe rlhf that unifies a broad class of existing alignment algorithms, including safe-rlhf, one-shot, and multi-shot based methods. building on this framework, we introduce an optimistic primal-dual (opd) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. we establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained rl and practical rlhf.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22146v1" target="_blank" rel="noopener">Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Yining Li, Peizhong Ju, Ness Shroff</p>
                <p class="paper-abstract" id="abstract-9">Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be...</p>
                <button class="expand-btn" onclick="toggleAbstract(9, 'Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.', 'Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be...')" id="expand-btn-9">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22146v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22146v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="when ai writes, whose voice remains? quantifying cultural marker erasure across world english varieties in large language models satyam kumar navneet joydeep chandra yong zhang large language models (llms) are increasingly used to ``professionalize&#x27;&#x27; workplace communication, often at the cost of linguistic identity. we introduce &quot;cultural ghosting&quot;, the systematic erasure of linguistic markers unique to non-native english varieties during text processing. through analysis of 22,350 llm outputs generated from 1,490 culturally marked texts (indian, singaporean,&amp; nigerian english) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: identity erasure rate (ier) &amp; semantic preservation score (sps). across all prompts, we find an overall ier of 10.26%, with model-level variation from 3.5% to 20.5% (5.9x range). crucially, we identify a semantic preservation paradox: models maintain high semantic similarity (mean sps = 0.748) while systematically erasing cultural markers. pragmatic markers (politeness conventions) are 1.9x more vulnerable than lexical markers (71.5% vs. 37.1% erasure). our experiments demonstrate that explicit cultural-preservation prompts reduce erasure by 29% without sacrificing semantic quality.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22145v1" target="_blank" rel="noopener">When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.HC</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Satyam Kumar Navneet, Joydeep Chandra, Yong Zhang</p>
                <p class="paper-abstract" id="abstract-10">Large Language Models (LLMs) are increasingly used to ``professionalize&#x27;&#x27; workplace communication, often at the cost of linguistic identity. We introduce &quot;Cultural Ghosting&quot;, the systematic erasure...</p>
                <button class="expand-btn" onclick="toggleAbstract(10, 'Large Language Models (LLMs) are increasingly used to ``professionalize&#x27;&#x27; workplace communication, often at the cost of linguistic identity. We introduce &quot;Cultural Ghosting&quot;, the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,&amp; Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: Identity Erasure Rate (IER) &amp; Semantic Preservation Score (SPS). Across all prompts, we find an overall IER of 10.26%, with model-level variation from 3.5% to 20.5% (5.9x range). Crucially, we identify a Semantic Preservation Paradox: models maintain high semantic similarity (mean SPS = 0.748) while systematically erasing cultural markers. Pragmatic markers (politeness conventions) are 1.9x more vulnerable than lexical markers (71.5% vs. 37.1% erasure). Our experiments demonstrate that explicit cultural-preservation prompts reduce erasure by 29% without sacrificing semantic quality.', 'Large Language Models (LLMs) are increasingly used to ``professionalize&#x27;&#x27; workplace communication, often at the cost of linguistic identity. We introduce &quot;Cultural Ghosting&quot;, the systematic erasure...')" id="expand-btn-10">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22145v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22145v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="weavetime: stream from earlier frames into emergent memory in videollms yulin zhang cheng shi sibei yang recent advances in multimodal large language models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. we diagnose a core limitation of current video-llms, namely time-agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. we present weavetime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. we introduce a lightweight temporal reconstruction objective-our streaming order perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. at inference, a past-current dynamic focus cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. plugged into exsiting video-llm without architectural changes, weavetime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. these results establish weavetime as a practical path toward time aware stream video-llms under strict online, time causal constraints. code and weights will be made publicly available. project page: https://zhangyl4.github.io/publications/weavetime/">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22142v1" target="_blank" rel="noopener">WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Yulin Zhang, Cheng Shi, Sibei Yang</p>
                <p class="paper-abstract" id="abstract-11">Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for...</p>
                <button class="expand-btn" onclick="toggleAbstract(11, 'Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/', 'Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for...')" id="expand-btn-11">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22142v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22142v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="sigmaquant: hardware-aware heterogeneous quantization method for edge dnn inference qunyou liu pengbo yu marina zapater david atienza deep neural networks (dnns) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. while uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. in contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. filling these gaps, this work introduces \textbf{\textit{sigmaquant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22136v1" target="_blank" rel="noopener">SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AR</span>
                </div>
                <p class="paper-authors">Qunyou Liu, Pengbo Yu, Marina Zapater, David Atienza</p>
                <p class="paper-abstract" id="abstract-12">Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory,...</p>
                <button class="expand-btn" onclick="toggleAbstract(12, 'Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. In contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. Nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. Filling these gaps, this work introduces \textbf{\textit{SigmaQuant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search.', 'Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory,...')" id="expand-btn-12">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22136v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22136v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="giving meaning to movements: challenges and opportunities in expanding communication by pairing unaided aac with speech generated messages imran kabir sharon ann redmon lynn r elko kevin williams mitchell a case dawn j sowers krista wilkinson syed masum billah augmentative and alternative communication (aac) technologies are categorized into two forms: aided aac, which uses external devices like speech-generating systems to produce standardized output, and unaided aac, which relies on body-based gestures for natural expression but requires shared understanding. we investigate how to combine these approaches to harness the speed and naturalness of unaided aac while maintaining the intelligibility of aided aac, a largely unexplored area for individuals with communication and motor impairments. through 18 months of participatory design with aac users, we identified key challenges and opportunities and developed allyaac, a wearable system with a wrist-worn imu paired with a smartphone app. we evaluated allyaac in a field study with 14 participants and produced a dataset containing over 600,000 multimodal data points featuring atypical gestures--the first of its kind. our findings reveal challenges in recognizing personalized, idiosyncratic gestures and demonstrate how to address them using transformer-based large machine learning (ml) models with different pretraining strategies. in sum, we contribute design principles and a reference implementation for adaptive, personalized systems combining aided and unaided aac.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22131v1" target="_blank" rel="noopener">Giving Meaning to Movements: Challenges and Opportunities in Expanding Communication by Pairing Unaided AAC with Speech Generated Messages</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.HC</span>
                </div>
                <p class="paper-authors">Imran Kabir, Sharon Ann Redmon, Lynn R Elko, Kevin Williams, Mitchell A Case <em>(+3 more)</em></p>
                <p class="paper-abstract" id="abstract-13">Augmentative and Alternative Communication (AAC) technologies are categorized into two forms: aided AAC, which uses external devices like speech-generating systems to produce standardized output, and...</p>
                <button class="expand-btn" onclick="toggleAbstract(13, 'Augmentative and Alternative Communication (AAC) technologies are categorized into two forms: aided AAC, which uses external devices like speech-generating systems to produce standardized output, and unaided AAC, which relies on body-based gestures for natural expression but requires shared understanding. We investigate how to combine these approaches to harness the speed and naturalness of unaided AAC while maintaining the intelligibility of aided AAC, a largely unexplored area for individuals with communication and motor impairments. Through 18 months of participatory design with AAC users, we identified key challenges and opportunities and developed AllyAAC, a wearable system with a wrist-worn IMU paired with a smartphone app. We evaluated AllyAAC in a field study with 14 participants and produced a dataset containing over 600,000 multimodal data points featuring atypical gestures--the first of its kind. Our findings reveal challenges in recognizing personalized, idiosyncratic gestures and demonstrate how to address them using Transformer-based large machine learning (ML) models with different pretraining strategies. In sum, we contribute design principles and a reference implementation for adaptive, personalized systems combining aided and unaided AAC.', 'Augmentative and Alternative Communication (AAC) technologies are categorized into two forms: aided AAC, which uses external devices like speech-generating systems to produce standardized output, and...')" id="expand-btn-13">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22131v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22131v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="sample complexity bounds for robust mean estimation with mean-shift contamination ilias diakonikolas giannis iakovidis daniel m. kane sihan liu we study the basic task of mean estimation in the presence of mean-shift contamination. in the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the clean samples by samples drawn from arbitrarily shifted versions of the base distribution. prior work characterized the sample complexity of this task for the special cases of the gaussian and laplace distributions. specifically, it was shown that consistent estimation is possible in these cases, a property that is provably impossible in huber&#x27;s contamination model. an open question posed in earlier work was to determine the sample complexity of mean estimation in the mean-shift contamination model for general base distributions. in this work, we study and essentially resolve this open question. specifically, we show that, under mild spectral conditions on the characteristic function of the (potentially multivariate) base distribution, there exists a sample-efficient algorithm that estimates the target mean to any desired accuracy. we complement our upper bound with a qualitatively matching sample complexity lower bound. our techniques make critical use of fourier analysis, and in particular introduce the notion of a fourier witness as an essential ingredient of our upper and lower bounds.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22130v1" target="_blank" rel="noopener">Sample Complexity Bounds for Robust Mean Estimation with Mean-Shift Contamination</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.DS</span>
                </div>
                <p class="paper-authors">Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, Sihan Liu</p>
                <p class="paper-abstract" id="abstract-14">We study the basic task of mean estimation in the presence of mean-shift contamination. In the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the...</p>
                <button class="expand-btn" onclick="toggleAbstract(14, 'We study the basic task of mean estimation in the presence of mean-shift contamination. In the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the clean samples by samples drawn from arbitrarily shifted versions of the base distribution. Prior work characterized the sample complexity of this task for the special cases of the Gaussian and Laplace distributions. Specifically, it was shown that consistent estimation is possible in these cases, a property that is provably impossible in Huber&#x27;s contamination model. An open question posed in earlier work was to determine the sample complexity of mean estimation in the mean-shift contamination model for general base distributions. In this work, we study and essentially resolve this open question. Specifically, we show that, under mild spectral conditions on the characteristic function of the (potentially multivariate) base distribution, there exists a sample-efficient algorithm that estimates the target mean to any desired accuracy. We complement our upper bound with a qualitatively matching sample complexity lower bound. Our techniques make critical use of Fourier analysis, and in particular introduce the notion of a Fourier witness as an essential ingredient of our upper and lower bounds.', 'We study the basic task of mean estimation in the presence of mean-shift contamination. In the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the...')" id="expand-btn-14">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22130v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22130v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="swe-protÃ©gÃ©: learning to selectively collaborate with an expert unlocks small language models as software engineering agents patrick tser jern kon archana pradeep ang chen alexander p. ellis warren hunt zijian wang john yang samuel thompson small language models (slms) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as swe-bench, where they suffer from pervasive action looping and low resolution rates. we introduce swe-protÃ©gÃ©, a post-training framework that reframes software repair as an expert-protÃ©gÃ© collaboration problem. in swe-protÃ©gÃ©, an slm remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. we lightly post-train qwen2.5-coder-7b-instruct to achieve 42.4% pass@1 on swe-bench verified, a +25.4% improvement over the prior slm state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22124v1" target="_blank" rel="noopener">SWE-ProtÃ©gÃ©: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.SE</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Patrick Tser Jern Kon, Archana Pradeep, Ang Chen, Alexander P. Ellis, Warren Hunt <em>(+3 more)</em></p>
                <p class="paper-abstract" id="abstract-15">Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench,...</p>
                <button class="expand-btn" onclick="toggleAbstract(15, 'Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-ProtÃ©gÃ©, a post-training framework that reframes software repair as an expert-protÃ©gÃ© collaboration problem. In SWE-ProtÃ©gÃ©, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).', 'Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench,...')" id="expand-btn-15">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22124v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22124v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="probing the geometry of diffusion models with the string method elio moreau florentin coeurdoux grÃ©goire ferre eric vanden-eijnden understanding the geometry of learned distributions is fundamental to improving and interpreting diffusion models, yet systematic tools for exploring their landscape remain limited. standard latent-space interpolations fail to respect the structure of the learned distribution, often traversing low-density regions. we introduce a framework based on the string method that computes continuous paths between samples by evolving curves under the learned score function. operating on pretrained models without retraining, our approach interpolates between three regimes: pure generative transport, which yields continuous sample paths; gradient-dominated dynamics, which recover minimum energy paths (meps); and finite-temperature string dynamics, which compute principal curves -- self-consistent paths that balance energy and entropy. we demonstrate that the choice of regime matters in practice. for image diffusion models, meps contain high-likelihood but unrealistic &#x27;&#x27;cartoon&#x27;&#x27; images, confirming prior observations that likelihood maxima appear unrealistic; principal curves instead yield realistic morphing sequences despite lower likelihood. for protein structure prediction, our method computes transition pathways between metastable conformers directly from models trained on static structures, yielding paths with physically plausible intermediates. together, these results establish the string method as a principled tool for probing the modal structure of diffusion models -- identifying modes, characterizing barriers, and mapping connectivity in complex learned distributions.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22122v1" target="_blank" rel="noopener">Probing the Geometry of Diffusion Models with the String Method</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">stat.ML</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Elio Moreau, Florentin Coeurdoux, GrÃ©goire Ferre, Eric Vanden-Eijnden</p>
                <p class="paper-abstract" id="abstract-16">Understanding the geometry of learned distributions is fundamental to improving and interpreting diffusion models, yet systematic tools for exploring their landscape remain limited. Standard...</p>
                <button class="expand-btn" onclick="toggleAbstract(16, 'Understanding the geometry of learned distributions is fundamental to improving and interpreting diffusion models, yet systematic tools for exploring their landscape remain limited. Standard latent-space interpolations fail to respect the structure of the learned distribution, often traversing low-density regions. We introduce a framework based on the string method that computes continuous paths between samples by evolving curves under the learned score function. Operating on pretrained models without retraining, our approach interpolates between three regimes: pure generative transport, which yields continuous sample paths; gradient-dominated dynamics, which recover minimum energy paths (MEPs); and finite-temperature string dynamics, which compute principal curves -- self-consistent paths that balance energy and entropy. We demonstrate that the choice of regime matters in practice. For image diffusion models, MEPs contain high-likelihood but unrealistic &#x27;&#x27;cartoon&#x27;&#x27; images, confirming prior observations that likelihood maxima appear unrealistic; principal curves instead yield realistic morphing sequences despite lower likelihood. For protein structure prediction, our method computes transition pathways between metastable conformers directly from models trained on static structures, yielding paths with physically plausible intermediates. Together, these results establish the string method as a principled tool for probing the modal structure of diffusion models -- identifying modes, characterizing barriers, and mapping connectivity in complex learned distributions.', 'Understanding the geometry of learned distributions is fundamental to improving and interpreting diffusion models, yet systematic tools for exploring their landscape remain limited. Standard...')" id="expand-btn-16">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22122v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22122v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="slice and explain: logic-based explanations for neural networks through domain slicing luiz fernando paulino queiroz carlos henrique leitÃ£o cavalcante thiago alves rocha neural networks (nns) are pervasive across various domains but often lack interpretability. to address the growing need for explanations, logic-based approaches have been proposed to explain predictions made by nns, offering correctness guarantees. however, scalability remains a concern in these methods. this paper proposes an approach leveraging domain slicing to facilitate explanation generation for nns. by reducing the complexity of logical constraints through slicing, we decrease explanation time by up to 40\% less time, as indicated through comparative experiments. our findings highlight the efficacy of domain slicing in enhancing explanation efficiency for nns.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22115v1" target="_blank" rel="noopener">Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.LO</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Luiz Fernando Paulino Queiroz, Carlos Henrique LeitÃ£o Cavalcante, Thiago Alves Rocha</p>
                <p class="paper-abstract" id="abstract-17">Neural networks (NNs) are pervasive across various domains but often lack interpretability. To address the growing need for explanations, logic-based approaches have been proposed to explain...</p>
                <button class="expand-btn" onclick="toggleAbstract(17, 'Neural networks (NNs) are pervasive across various domains but often lack interpretability. To address the growing need for explanations, logic-based approaches have been proposed to explain predictions made by NNs, offering correctness guarantees. However, scalability remains a concern in these methods. This paper proposes an approach leveraging domain slicing to facilitate explanation generation for NNs. By reducing the complexity of logical constraints through slicing, we decrease explanation time by up to 40\% less time, as indicated through comparative experiments. Our findings highlight the efficacy of domain slicing in enhancing explanation efficiency for NNs.', 'Neural networks (NNs) are pervasive across various domains but often lack interpretability. To address the growing need for explanations, logic-based approaches have been proposed to explain...')" id="expand-btn-17">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22115v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22115v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="don&#x27;t stop me now: rethinking validation criteria for model parameter selection andrea apicella francesco isgrÃ² andrea pollastro roberto prevete despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. in this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural classifiers, with attention to early stopping. using fully connected networks on standard benchmarks under $k$-fold evaluation, we compare: (i) early stopping with patience and (ii) post-hoc selection over all epochs (i.e. no early stopping). models are trained with cross-entropy, c-loss, or polyloss; the model parameter selection on the validation set is made using accuracy or one of the three loss functions, each considered independently. three main findings emerge. (1) early stopping based on validation accuracy performs worst, consistently selecting checkpoints with lower test accuracy than both loss-based early stopping and post-hoc selection. (2) loss-based validation criteria yield comparable and more stable test accuracy. (3) across datasets and folds, any single validation rule often underperforms the test-optimal checkpoint. overall, the selected model typically achieves test-set performance statistically lower than the best performance across all epochs, regardless of the validation criterion. our results suggest avoiding validation accuracy (in particular with early stopping) for parameter selection, favoring loss-based validation criteria.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22107v1" target="_blank" rel="noopener">Don&#x27;t stop me now: Rethinking Validation Criteria for Model Parameter Selection</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Andrea Apicella, Francesco IsgrÃ², Andrea Pollastro, Roberto Prevete</p>
                <p class="paper-abstract" id="abstract-18">Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and...</p>
                <button class="expand-btn" onclick="toggleAbstract(18, 'Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural classifiers, with attention to early stopping. Using fully connected networks on standard benchmarks under $k$-fold evaluation, we compare: (i) early stopping with patience and (ii) post-hoc selection over all epochs (i.e. no early stopping). Models are trained with cross-entropy, C-Loss, or PolyLoss; the model parameter selection on the validation set is made using accuracy or one of the three loss functions, each considered independently. Three main findings emerge. (1) Early stopping based on validation accuracy performs worst, consistently selecting checkpoints with lower test accuracy than both loss-based early stopping and post-hoc selection. (2) Loss-based validation criteria yield comparable and more stable test accuracy. (3) Across datasets and folds, any single validation rule often underperforms the test-optimal checkpoint. Overall, the selected model typically achieves test-set performance statistically lower than the best performance across all epochs, regardless of the validation criterion. Our results suggest avoiding validation accuracy (in particular with early stopping) for parameter selection, favoring loss-based validation criteria.', 'Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and...')" id="expand-btn-18">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22107v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22107v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="pasta: a modular program analysis tool framework for accelerators mao lin hyeran jeon keren zhou the increasing complexity and diversity of hardware accelerators in modern computing systems demand flexible, low-overhead program analysis tools. we present pasta, a low-overhead and modular program analysis tool framework for accelerators. pasta abstracts over low-level profiling apis and diverse deep learning frameworks, offering users a unified interface to capture and analyze runtime events at multiple levels. its extensible design enables researchers and practitioners to rapidly prototype custom tools with minimal overhead. we demonstrate the utility of pasta by developing several analysis tools, including a deep learning workload characterization tool and a uvm optimization tool. through extensive evaluation on mainstream deep learning workloads tested on nvidia and amd gpus under both single- and multi-gpu scenarios, we demonstrate pasta&#x27;s broad applicability. on nvidia gpus, we further show that pasta provides detailed performance insights with significantly lower overhead, up to 1.3*10^4 faster than conventional analysis tools, thanks to its gpu-accelerated backend. pasta strikes a practical balance between usability, extensibility, and efficiency, making it well-suited for modern accelerator-based computing environments.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.22103v1" target="_blank" rel="noopener">PASTA: A Modular Program Analysis Tool Framework for Accelerators</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-25</span>
                    <span class="paper-category">cs.DC</span><span class="paper-category">cs.PF</span>
                </div>
                <p class="paper-authors">Mao Lin, Hyeran Jeon, Keren Zhou</p>
                <p class="paper-abstract" id="abstract-19">The increasing complexity and diversity of hardware accelerators in modern computing systems demand flexible, low-overhead program analysis tools. We present PASTA, a low-overhead and modular Program...</p>
                <button class="expand-btn" onclick="toggleAbstract(19, 'The increasing complexity and diversity of hardware accelerators in modern computing systems demand flexible, low-overhead program analysis tools. We present PASTA, a low-overhead and modular Program AnalysiS Tool Framework for Accelerators. PASTA abstracts over low-level profiling APIs and diverse deep learning frameworks, offering users a unified interface to capture and analyze runtime events at multiple levels. Its extensible design enables researchers and practitioners to rapidly prototype custom tools with minimal overhead. We demonstrate the utility of PASTA by developing several analysis tools, including a deep learning workload characterization tool and a UVM optimization tool. Through extensive evaluation on mainstream deep learning workloads tested on NVIDIA and AMD GPUs under both single- and multi-GPU scenarios, we demonstrate PASTA&#x27;s broad applicability. On NVIDIA GPUs, we further show that PASTA provides detailed performance insights with significantly lower overhead, up to 1.3*10^4 faster than conventional analysis tools, thanks to its GPU-accelerated backend. PASTA strikes a practical balance between usability, extensibility, and efficiency, making it well-suited for modern accelerator-based computing environments.', 'The increasing complexity and diversity of hardware accelerators in modern computing systems demand flexible, low-overhead program analysis tools. We present PASTA, a low-overhead and modular Program...')" id="expand-btn-19">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.22103v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.22103v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

        </div>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2026 Coding Blog | BST 236 Computing I | Harvard University</p>
            <p class="footer-links">
                <a href="https://github.com">GitHub</a>
            </p>
        </div>
    </footer>

    <script src="js/main.js"></script>
    <script>
        function filterPapers() {
            const query = document.getElementById('searchInput').value.toLowerCase();
            const cards = document.querySelectorAll('.paper-card');
            
            cards.forEach(card => {
                const searchText = card.getAttribute('data-search');
                if (searchText.includes(query)) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });
        }
        
        function toggleAbstract(index, fullText, shortText) {
            const abstractEl = document.getElementById('abstract-' + index);
            const btnEl = document.getElementById('expand-btn-' + index);
            
            if (abstractEl.classList.contains('expanded')) {
                abstractEl.textContent = shortText;
                abstractEl.classList.remove('expanded');
                btnEl.textContent = 'Show more â–¼';
            } else {
                abstractEl.textContent = fullText;
                abstractEl.classList.add('expanded');
                btnEl.textContent = 'Show less â–²';
            }
        }
        
        // Client-side arXiv fetching for custom keywords
        // CORS proxies to try in order
        const corsProxies = [
            url => `https://corsproxy.io/?${encodeURIComponent(url)}`,
            url => `https://api.codetabs.com/v1/proxy?quest=${encodeURIComponent(url)}`,
            url => `https://api.allorigins.win/raw?url=${encodeURIComponent(url)}`
        ];
        
        async function tryFetchWithProxies(url, proxies) {
            for (let i = 0; i < proxies.length; i++) {
                const proxyUrl = proxies[i](url);
                try {
                    const response = await fetch(proxyUrl, { timeout: 10000 });
                    if (response.ok) {
                        return await response.text();
                    }
                } catch (e) {
                    console.log(`Proxy ${i + 1} failed:`, e.message);
                }
            }
            throw new Error('All CORS proxies failed. Please try again later or edit scripts/config.json directly.');
        }
        
        async function fetchCustomPapers() {
            const input = document.getElementById('customKeywords').value.trim();
            const btn = document.getElementById('fetchBtn');
            const grid = document.getElementById('papersGrid');
            const keywordsDisplay = document.getElementById('currentKeywords');
            
            if (!input) {
                alert('Please enter at least one keyword');
                return;
            }
            
            const keywords = input.split(',').map(k => k.trim()).filter(k => k);
            
            btn.disabled = true;
            btn.textContent = 'Fetching...';
            
            // Update displayed keywords
            keywordsDisplay.innerHTML = keywords.map(kw => 
                `<span class="keyword-tag">${escapeHtml(kw)}</span>`
            ).join('');
            
            // Build arXiv query
            const searchQuery = keywords.map(kw => `all:"${kw}"`).join(' OR ');
            const url = `https://export.arxiv.org/api/query?search_query=${encodeURIComponent(searchQuery)}&start=0&max_results=20&sortBy=submittedDate&sortOrder=descending`;
            
            try {
                const xmlText = await tryFetchWithProxies(url, corsProxies);
                
                // Parse XML
                const parser = new DOMParser();
                const xmlDoc = parser.parseFromString(xmlText, 'text/xml');
                const entries = xmlDoc.querySelectorAll('entry');
                
                if (entries.length === 0) {
                    grid.innerHTML = `
                        <div class="no-results" style="grid-column: 1 / -1;">
                            <h3>No papers found</h3>
                            <p>Try different keywords.</p>
                        </div>
                    `;
                } else {
                    let html = '';
                    entries.forEach((entry, i) => {
                        const title = entry.querySelector('title')?.textContent?.replace(/\s+/g, ' ').trim() || 'Untitled';
                        const abstract = entry.querySelector('summary')?.textContent?.replace(/\s+/g, ' ').trim() || 'No abstract';
                        const published = entry.querySelector('published')?.textContent?.substring(0, 10) || 'Unknown';
                        const id = entry.querySelector('id')?.textContent || '';
                        const pdfUrl = id.replace('/abs/', '/pdf/') + '.pdf';
                        
                        const authors = [];
                        entry.querySelectorAll('author name').forEach(n => authors.push(n.textContent));
                        const authorsStr = authors.length > 5 
                            ? authors.slice(0, 5).join(', ') + ` <em>(+${authors.length - 5} more)</em>`
                            : authors.join(', ');
                        
                        const categories = [];
                        entry.querySelectorAll('category').forEach(c => {
                            const term = c.getAttribute('term');
                            if (term && categories.length < 3) categories.push(term);
                        });
                        
                        const abstractShort = abstract.length > 200 
                            ? abstract.substring(0, 200).replace(/\s+\S*$/, '') + '...'
                            : abstract;
                        
                        html += `
                            <article class="paper-card" data-search="${escapeHtml(title.toLowerCase())} ${escapeHtml(authors.join(' ').toLowerCase())} ${escapeHtml(abstract.toLowerCase())}">
                                <h2 class="paper-title">
                                    <a href="${escapeHtml(id)}" target="_blank" rel="noopener">${escapeHtml(title)}</a>
                                </h2>
                                <div class="paper-meta">
                                    <span class="paper-date">ğŸ“… ${escapeHtml(published)}</span>
                                    ${categories.map(c => `<span class="paper-category">${escapeHtml(c)}</span>`).join('')}
                                </div>
                                <p class="paper-authors">${authorsStr}</p>
                                <p class="paper-abstract" id="abstract-dyn-${i}">${escapeHtml(abstractShort)}</p>
                                <button class="expand-btn" onclick="toggleAbstract('dyn-${i}', '${escapeHtml(abstract).replace(/'/g, "\\'")}', '${escapeHtml(abstractShort).replace(/'/g, "\\'")}')">Show more â–¼</button>
                                <div class="paper-actions">
                                    <a href="${escapeHtml(pdfUrl)}" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                                    <a href="${escapeHtml(id)}" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                                </div>
                            </article>
                        `;
                    });
                    grid.innerHTML = html;
                }
                
                // Update timestamp
                const now = new Date().toISOString().replace('T', ' ').substring(0, 19) + ' (live fetch)';
                document.querySelector('.update-info').textContent = 'Last updated: ' + now;
                
            } catch (error) {
                console.error('Fetch error:', error);
                grid.innerHTML = `
                    <div class="no-results" style="grid-column: 1 / -1;">
                        <h3>Error fetching papers</h3>
                        <p>${escapeHtml(error.message)}</p>
                        <p style="margin-top: 15px; font-size: 0.9rem;">
                            <strong>Alternative:</strong> Edit <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">scripts/config.json</code> 
                            and run <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">python scripts/fetch_arxiv.py</code> locally, 
                            or push to GitHub to trigger the auto-update.
                        </p>
                    </div>
                `;
            }
            
            btn.disabled = false;
            btn.textContent = 'Fetch Papers';
        }
        
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }
        
        // Allow Enter key to trigger fetch
        document.getElementById('customKeywords').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') fetchCustomPapers();
        });
    </script>
</body>
</html>
