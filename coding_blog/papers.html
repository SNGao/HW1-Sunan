<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Feed | Coding Blog</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <style>
        .papers-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 100px 20px 40px;
        }
        .papers-header {
            text-align: center;
            margin-bottom: 40px;
        }
        .papers-header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .update-info {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 8px;
        }
        .auto-update-info {
            color: #22c55e;
            font-size: 0.85rem;
            margin-bottom: 15px;
        }
        .config-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-bottom: 10px;
        }
        .config-hint code {
            background: #334155;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: var(--font-code);
            color: #f472b6;
        }
        .search-box {
            max-width: 500px;
            margin: 20px auto;
        }
        .search-box input {
            width: 100%;
            padding: 12px 20px;
            border: 2px solid var(--border);
            border-radius: 25px;
            background: var(--card-bg);
            color: var(--text-primary);
            font-size: 1rem;
            outline: none;
            transition: border-color 0.3s;
        }
        .search-box input:focus {
            border-color: var(--primary);
        }
        .search-box input::placeholder {
            color: var(--text-muted);
        }
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 25px;
        }
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 25px;
            transition: all 0.3s ease;
        }
        .paper-card:hover {
            border-color: var(--primary);
            transform: translateY(-3px);
            box-shadow: 0 10px 30px rgba(236, 72, 153, 0.15);
        }
        .paper-card.hidden {
            display: none;
        }
        .paper-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-bottom: 10px;
            line-height: 1.4;
        }
        .paper-title a {
            color: inherit;
            text-decoration: none;
            transition: color 0.3s;
        }
        .paper-title a:hover {
            color: var(--primary);
        }
        .paper-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 12px;
        }
        .paper-date {
            background: var(--primary);
            color: white;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
            font-weight: 500;
        }
        .paper-category {
            background: #334155;
            color: #e2e8f0;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
        }
        .paper-authors {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 12px;
            line-height: 1.5;
        }
        .paper-abstract {
            color: #94a3b8;
            font-size: 0.9rem;
            line-height: 1.7;
            margin-bottom: 15px;
        }
        .paper-abstract.expanded {
            max-height: none;
        }
        .expand-btn {
            background: none;
            border: none;
            color: var(--primary);
            cursor: pointer;
            font-size: 0.85rem;
            padding: 0;
            margin-bottom: 15px;
        }
        .expand-btn:hover {
            text-decoration: underline;
        }
        .paper-actions {
            display: flex;
            gap: 10px;
        }
        .pdf-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            transition: all 0.3s;
        }
        .pdf-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 15px rgba(236, 72, 153, 0.3);
        }
        .arxiv-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: var(--surface);
            color: var(--text-primary);
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            border: 1px solid var(--border);
            transition: all 0.3s;
        }
        .arxiv-btn:hover {
            border-color: var(--primary);
            color: var(--primary);
        }
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: var(--text-muted);
        }
        .no-results h3 {
            font-size: 1.5rem;
            margin-bottom: 10px;
            color: var(--text-secondary);
        }
        .keywords-info {
            background: #1e293b;
            padding: 20px;
            border-radius: 12px;
            margin-bottom: 30px;
            text-align: center;
            border: 1px solid #334155;
        }
        .keywords-info > span {
            color: #e2e8f0;
            font-size: 0.95rem;
            font-weight: 500;
        }
        .keyword-tag {
            display: inline-block;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: #ffffff;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin: 4px;
            text-shadow: 0 1px 2px rgba(0,0,0,0.2);
        }
        .keyword-editor {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #334155;
        }
        .keyword-input-group {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .keyword-input {
            padding: 10px 16px;
            border: 2px solid #334155;
            border-radius: 25px;
            background: #0f172a;
            color: #f8fafc;
            font-size: 0.9rem;
            width: 250px;
            outline: none;
            transition: border-color 0.3s;
        }
        .keyword-input:focus {
            border-color: #6366f1;
        }
        .keyword-input::placeholder {
            color: #64748b;
        }
        .fetch-btn {
            padding: 10px 24px;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: white;
            border: none;
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .fetch-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 20px rgba(99, 102, 241, 0.4);
        }
        .fetch-btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }
        .editor-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-top: 8px;
        }
        @media (max-width: 768px) {
            .papers-grid {
                grid-template-columns: 1fr;
            }
            .papers-header h1 {
                font-size: 1.8rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="index.html" class="nav-logo">
                <span class="logo-icon">üíª</span>
                <span class="logo-text">Coding Blog</span>
            </a>
            <ul class="nav-menu">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li><a href="index.html#projects" class="nav-link">Projects</a></li>
                <li><a href="papers.html" class="nav-link active">Papers</a></li>
                <li><a href="index.html#about" class="nav-link">About</a></li>
            </ul>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <div class="papers-container">
        <div class="papers-header">
            <h1>üìö arXiv Paper Feed</h1>
            <p class="update-info">Last updated: 2026-02-25 01:46:50 UTC</p>
            <p class="auto-update-info">‚è∞ Auto-updates daily at midnight UTC via GitHub Actions</p>
            <div class="keywords-info">
                <span>Current keywords: </span>
                <div id="currentKeywords">
                    <span class="keyword-tag">large language model</span><span class="keyword-tag">machine learning</span><span class="keyword-tag">biostatistics</span><span class="keyword-tag">deep learning</span>
                </div>
                <div class="keyword-editor">
                    <p class="editor-hint">üîÑ Try different keywords (fetches live from arXiv):</p>
                    <div class="keyword-input-group">
                        <input type="text" id="customKeywords" class="keyword-input" 
                               placeholder="e.g., reinforcement learning, NLP" 
                               value="large language model, machine learning, biostatistics, deep learning">
                        <button onclick="fetchCustomPapers()" class="fetch-btn" id="fetchBtn">
                            Fetch Papers
                        </button>
                    </div>
                    <p class="editor-hint">Separate multiple keywords with commas</p>
                </div>
            </div>
            <div class="search-box">
                <input type="text" id="searchInput" placeholder="üîç Filter papers by title, author, or abstract..." oninput="filterPapers()">
            </div>
        </div>

        <div class="papers-grid" id="papersGrid">

            <article class="paper-card" data-search="a very big video reasoning suite maijunxian wang ruisi wang juyi lin ran ji thadd√§us wiedemer qingying gao dezhi luo yaoyao qian lianyu huang zelong hong jiahui ge qianli ma hang he yifan zhou lingzi guo lantao mei jiachen li hanwen xing tianqi zhao fengyuan yu weihang xiao yizheng jiao jianheng hou danyang zhang pengcheng xu boyang zhong zehong zhao gaoyun fang john kitaoka yile xu hua xu kenton blacutt tin nguyen siyuan song haoran sun shaoyue wen linyang he runming wang yanzhi wang mengyue yang ziqiao ma rapha√´l milli√®re freda shi nuno vasconcelos daniel khashabi alan yuille yilun du ziming liu bo li dahua lin ziwei liu vikash kumar yijiang li lei yang zhongang cai hokin deng rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. however, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. to address this gap, we introduce the very big video reasoning (vbvr) dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. we further present vbvr-bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. leveraging the vbvr suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. together, vbvr lays a foundation for the next stage of research in generalizable video reasoning. the data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20159v1" target="_blank" rel="noopener">A Very Big Video Reasoning Suite</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.CV</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Maijunxian Wang, Ruisi Wang, Juyi Lin, Ran Ji, Thadd√§us Wiedemer <em>(+51 more)</em></p>
                <p class="paper-abstract" id="abstract-0">Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual...</p>
                <button class="expand-btn" onclick="toggleAbstract(0, 'Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .', 'Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual...')" id="expand-btn-0">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20159v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20159v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="skill-inject: measuring agent vulnerability to skill file attacks david schmotz luca beurer-kellner sahar abdelnabi maksym andriushchenko llm agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. skills allow users to extend llm applications with specialized third-party code, knowledge, and instructions. although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. we identify skill-based prompt injection as a significant threat and introduce skillinject, a benchmark evaluating the susceptibility of widely-used llm agents to injections through skill files. skillinject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. we evaluate frontier llms on skillinject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. our results show that today&#x27;s agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. they furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. our benchmark is available at https://www.skill-inject.com/.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20156v1" target="_blank" rel="noopener">Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.CR</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi, Maksym Andriushchenko</p>
                <p class="paper-abstract" id="abstract-1">LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code,...</p>
                <button class="expand-btn" onclick="toggleAbstract(1, 'LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today&#x27;s agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.', 'LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code,...')" id="expand-btn-1">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20156v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20156v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="jucal: jointly calibrating aleatoric and epistemic uncertainty in classification tasks jakob heiss s√∂ren lambrecht jakob weissteiner hanna wutte ≈æan ≈æuriƒç josef teichmann bin yu we study post-calibration uncertainty for trained ensembles of classifiers. specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. however, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. to address this shortcoming, we present a simple but powerful calibration algorithm joint uncertainty calibration (jucal) that jointly calibrates aleatoric and epistemic uncertainty. jucal jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (nll) on the validation/calibration dataset. jucal can be applied to any trained ensemble of classifiers (e.g., transformers, cnns, or tree-based methods), with minimal computational overhead, without requiring access to the models&#x27; internal parameters. we experimentally evaluate jucal on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. our experiments show that jucal significantly outperforms sota calibration methods across all considered classification tasks, reducing nll and predictive set size by up to 15% and 20%, respectively. interestingly, even applying jucal to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of nll and predictive set size, resulting in up to 10 times smaller inference costs. thus, we propose jucal as a new go-to method for calibrating ensembles in classification.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20153v1" target="_blank" rel="noopener">JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">stat.ML</span><span class="paper-category">cs.LG</span><span class="paper-category">stat.ME</span>
                </div>
                <p class="paper-authors">Jakob Heiss, S√∂ren Lambrecht, Jakob Weissteiner, Hanna Wutte, ≈Ωan ≈Ωuriƒç <em>(+2 more)</em></p>
                <p class="paper-abstract" id="abstract-2">We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely...</p>
                <button class="expand-btn" onclick="toggleAbstract(2, 'We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models&#x27; internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.', 'We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely...')" id="expand-btn-2">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20153v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20153v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="behavior learning (bl): learning hierarchical optimization structures from data zhenyao ma yue liang dongxu li inspired by behavioral science, we propose behavior learning (bl), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. it unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. bl parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. each block represents and can be written in symbolic form as a utility maximization problem (ump), a foundational paradigm in behavioral science and a universal framework of optimization. bl supports architectures ranging from a single ump to hierarchical compositions, the latter modeling hierarchical optimization structures. its smooth and monotone variant (ibl) guarantees identifiability. theoretically, we establish the universal approximation property of bl, and analyze the m-estimation properties of ibl. empirically, bl demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. code: https://github.com/moonyliang/behavior-learning ; install via pip install blnetwork.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20152v1" target="_blank" rel="noopener">Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span><span class="paper-category">stat.ML</span>
                </div>
                <p class="paper-authors">Zhenyao Ma, Yue Liang, Dongxu Li</p>
                <p class="paper-abstract" id="abstract-3">Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data,...</p>
                <button class="expand-btn" onclick="toggleAbstract(3, 'Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.', 'Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data,...')" id="expand-btn-3">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20152v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20152v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="conformal risk control for non-monotonic losses anastasios n. angelopoulos conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. the original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. the guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. we give applications of this technique to selective image classification, fdr and iou control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20151v1" target="_blank" rel="noopener">Conformal Risk Control for Non-Monotonic Losses</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">stat.ME</span><span class="paper-category">cs.LG</span><span class="paper-category">math.ST</span>
                </div>
                <p class="paper-authors">Anastasios N. Angelopoulos</p>
                <p class="paper-abstract" id="abstract-4">Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a...</p>
                <button class="expand-btn" onclick="toggleAbstract(4, 'Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.', 'Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a...')" id="expand-btn-4">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20151v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20151v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="packflow: generative molecular crystal structure prediction via reinforcement learning alignment akshay subramanian elton pan juno nam maurice weiler shuhui qu cheol woo park tommi s. jaakkola elsa olivetti rafael gomez-bombarelli organic molecular crystals underpin technologies ranging from pharmaceuticals to organic electronics, yet predicting solid-state packing of molecules remains challenging because candidate generation is combinatorial and stability is only resolved after costly energy evaluations. here we introduce packflow, a flow matching framework for molecular crystal structure prediction (csp) that generates heavy-atom crystal proposals by jointly sampling cartesian coordinates and unit-cell lattice parameters given a molecular graph. this lattice-aware generation interfaces directly with downstream relaxation and lattice-energy ranking, positioning packflow as a scalable proposal engine within standard csp pipelines. to explicitly steer generation toward physically favourable regions, we propose physics alignment, a reinforcement learning post-training stage that uses machine-learned interatomic potential energies and forces as stability proxies. physics alignment improves physical validity without altering inference-time sampling. we validate packflow&#x27;s performance against heuristic baselines through two distinct evaluations. first, on a broad unseen set of molecular systems, we demonstrate superior candidate generation capability, with proposals exhibiting greater structural similarity to experimental polymorphs. second, we assess the full end-to-end workflow on two unseen csp blind-test case studies, including relaxation and lattice-energy analysis. in both settings, packflow outperforms heuristics-based methods by concentrating probability mass in low-energy basins, yielding candidates that relax into lower-energy minima and offering a practical route to amortize the relax-and-rank bottleneck.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20140v1" target="_blank" rel="noopener">PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">physics.chem-ph</span>
                </div>
                <p class="paper-authors">Akshay Subramanian, Elton Pan, Juno Nam, Maurice Weiler, Shuhui Qu <em>(+4 more)</em></p>
                <p class="paper-abstract" id="abstract-5">Organic molecular crystals underpin technologies ranging from pharmaceuticals to organic electronics, yet predicting solid-state packing of molecules remains challenging because candidate generation...</p>
                <button class="expand-btn" onclick="toggleAbstract(5, 'Organic molecular crystals underpin technologies ranging from pharmaceuticals to organic electronics, yet predicting solid-state packing of molecules remains challenging because candidate generation is combinatorial and stability is only resolved after costly energy evaluations. Here we introduce PackFlow, a flow matching framework for molecular crystal structure prediction (CSP) that generates heavy-atom crystal proposals by jointly sampling Cartesian coordinates and unit-cell lattice parameters given a molecular graph. This lattice-aware generation interfaces directly with downstream relaxation and lattice-energy ranking, positioning PackFlow as a scalable proposal engine within standard CSP pipelines. To explicitly steer generation toward physically favourable regions, we propose physics alignment, a reinforcement learning post-training stage that uses machine-learned interatomic potential energies and forces as stability proxies. Physics alignment improves physical validity without altering inference-time sampling. We validate PackFlow&#x27;s performance against heuristic baselines through two distinct evaluations. First, on a broad unseen set of molecular systems, we demonstrate superior candidate generation capability, with proposals exhibiting greater structural similarity to experimental polymorphs. Second, we assess the full end-to-end workflow on two unseen CSP blind-test case studies, including relaxation and lattice-energy analysis. In both settings, PackFlow outperforms heuristics-based methods by concentrating probability mass in low-energy basins, yielding candidates that relax into lower-energy minima and offering a practical route to amortize the relax-and-rank bottleneck.', 'Organic molecular crystals underpin technologies ranging from pharmaceuticals to organic electronics, yet predicting solid-state packing of molecules remains challenging because candidate generation...')" id="expand-btn-5">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20140v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20140v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="do large language models understand data visualization rules? martin sinnona valentin bonas emmanuel iarussi viviana siless data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. while prior work has shown that large language models (llms) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. constraint-based systems such as draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of llms as flexible rule validators. in this paper, we present the first systematic evaluation of llms against visualization rules using hard-verification ground truth derived from answer set programming (asp). we translated a subset of draco&#x27;s constraints into natural-language statements and generated a controlled dataset of 2,000 vega-lite specifications annotated with explicit rule violations. llms were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. results show that frontier models achieve high adherence (gemma 3 4b / 27b: 100%, gpt-oss 20b: 98%) and reliably detect common violations (f1 up to 0.82),yet performance drops for subtler perceptual rules (f1 &lt; 0.15 for some categories) and for outputs generated from technical asp formulations.translating constraints into natural language improved performance by up to 150% for smaller models. these findings demonstrate the potential of llms as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20137v1" target="_blank" rel="noopener">Do Large Language Models Understand Data Visualization Rules?</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Martin Sinnona, Valentin Bonas, Emmanuel Iarussi, Viviana Siless</p>
                <p class="paper-abstract" id="abstract-6">Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate...</p>
                <button class="expand-btn" onclick="toggleAbstract(6, 'Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco&#x27;s constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 &lt; 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.', 'Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate...')" id="expand-btn-6">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20137v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20137v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="knight: knowledge graph-driven multiple-choice question generation with adaptive hardness calibration mohammad amanlou erfan shafiee moghaddam yasaman amou jafari mahdi noori farhan farsi behnam bahrak with the rise of large language models (llms), they have become instrumental in applications such as retrieval-augmented generation (rag). yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. we introduce knight, an llm-based, knowledge-graph-driven framework for generating multiple-choice question (mcq) datasets from external sources. knight constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. this knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. we instantiate knight on wikipedia/wikidata while keeping the framework domain- and ontology-agnostic. as a case study, knight produces six mcq datasets in history, biology, and mathematics. we evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). results show that knight enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with mmlu-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20135v1" target="_blank" rel="noopener">KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.IR</span>
                </div>
                <p class="paper-authors">Mohammad Amanlou, Erfan Shafiee Moghaddam, Yasaman Amou Jafari, Mahdi Noori, Farhan Farsi <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-7">With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the...</p>
                <button class="expand-btn" onclick="toggleAbstract(7, 'With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.', 'With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the...')" id="expand-btn-7">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20135v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20135v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="adaevolve: adaptive llm driven zeroth-order optimization mert cemri shubham agrawal akshat gupta shu liu audrey cheng qiuyang mang ashwin naren lutfi eren erdogan koushik sen matei zaharia alex dimakis ion stoica the paradigm of automated program generation is shifting from one-shot generation to inference-time search, where large language models (llms) function as semantic mutation operators within evolutionary loops. while effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. this rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. we introduce adaevolve, a framework that reformulates llm-driven evolution as a hierarchical adaptive optimization problem. adaevolve uses an &quot;accumulated improvement signal&quot; to unify decisions across three levels: local adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; global adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and meta-guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. we demonstrate that adaevolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20133v1" target="_blank" rel="noopener">AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.NE</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Mert Cemri, Shubham Agrawal, Akshat Gupta, Shu Liu, Audrey Cheng <em>(+7 more)</em></p>
                <p class="paper-abstract" id="abstract-8">The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within...</p>
                <button class="expand-btn" onclick="toggleAbstract(8, 'The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an &quot;accumulated improvement signal&quot; to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.', 'The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within...')" id="expand-btn-8">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20133v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20133v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="lad: learning advantage distribution for reasoning wendi li sharon li current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. this paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. to address this issue, we introduce learning advantage distributions (lad), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. by establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical lad objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. this yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. lad incurs no extra training cost compared to grpo and scales naturally to llm post-training. in a controlled bandit setting, lad faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. experiments on math and code reasoning tasks across several llm backbones show that lad reliably improves both accuracy and generative diversity.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20132v1" target="_blank" rel="noopener">LAD: Learning Advantage Distribution for Reasoning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Wendi Li, Sharon Li</p>
                <p class="paper-abstract" id="abstract-9">Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting...</p>
                <button class="expand-btn" onclick="toggleAbstract(9, 'Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.', 'Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting...')" id="expand-btn-9">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20132v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20132v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="to reason or not to: selective chain-of-thought in medical question answering zaifu zhan min zeng shuang zhou yiran song xiaoyi chen yu hou yifan wu yang ruan rui zhang objective: to improve the efficiency of medical question answering (medqa) with large language models (llms) by avoiding unnecessary reasoning while maintaining accuracy. methods: we propose selective chain-of-thought (selective cot), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. two open-source llms (llama-3.1-8b and qwen-2.5-7b) were evaluated on four biomedical qa benchmarks-headqa, medqa-usmle, medmcqa, and pubmedqa. metrics included accuracy, total generated tokens, and inference time. results: selective cot reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\leq$4\%). in some model-task pairs, it achieved both higher accuracy and greater efficiency than standard cot. compared with fixed-length cot, selective cot reached similar or superior accuracy at substantially lower computational cost. discussion: selective cot dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability. conclusion: selective cot provides a simple, model-agnostic, and cost-effective approach for medical qa, aligning reasoning effort with question complexity to enhance real-world deployability of llm-based clinical systems.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20130v1" target="_blank" rel="noopener">To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Zaifu Zhan, Min Zeng, Shuang Zhou, Yiran Song, Xiaoyi Chen <em>(+4 more)</em></p>
                <p class="paper-abstract" id="abstract-10">Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy. Methods: We propose...</p>
                <button class="expand-btn" onclick="toggleAbstract(10, 'Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy. Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time. Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\leq$4\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost. Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability. Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.', 'Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy. Methods: We propose...')" id="expand-btn-10">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20130v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20130v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="adaptation to intrinsic dependence in diffusion language models yunxiao zhao changxiao cai diffusion language models (dlms) have recently emerged as a promising alternative to autoregressive (ar) approaches, enabling parallel token generation beyond a rigid left-to-right order. despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. in this work, we introduce a distribution-agnostic unmasking schedule for dlms that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. in contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. we show that, for two specific parameter choices, the sampling convergence guarantees -- measured by kullback-leibler (kl) divergence -- scale as $\widetilde o(\mathsf{tc}/k)$ and $\widetilde o(\mathsf{dtc}/k)$ respectively. here, $k$ is the number of iterations, and $\mathsf{tc}$ and $\mathsf{dtc}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. importantly, our guarantees hold in the practically relevant parallel-sampling regime $k&lt;l$ where $l$ is the token sequence length. these results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. overall, our findings unveil the adaptivity of dlms to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20126v1" target="_blank" rel="noopener">Adaptation to Intrinsic Dependence in Diffusion Language Models</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.IT</span><span class="paper-category">math.ST</span>
                </div>
                <p class="paper-authors">Yunxiao Zhao, Changxiao Cai</p>
                <p class="paper-abstract" id="abstract-11">Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite...</p>
                <button class="expand-btn" onclick="toggleAbstract(11, 'Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\widetilde O(\mathsf{TC}/K)$ and $\widetilde O(\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\mathsf{TC}$ and $\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K&lt;L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.', 'Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite...')" id="expand-btn-11">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20126v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20126v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="nanoknow: how to know what your language model knows lingwei gu nour jedidi jimmy lin how do large language models (llms) know what they know? answering this question has been difficult because pre-training data is often a &quot;black box&quot; -- unknown or inaccessible. the recent release of nanochat -- a family of small llms with fully open pre-training data -- addresses this as it provides a transparent view into where a model&#x27;s parametric knowledge comes from. towards the goal of understanding how knowledge is encoded by llms, we release nanoknow, a benchmark dataset that partitions questions from natural questions and squad into splits based on whether their answers are present in nanochat&#x27;s pre-training corpus. using these splits, we can now properly disentangle the sources of knowledge that llms rely on when producing an output. to demonstrate nanoknow&#x27;s utility, we conduct experiments using eight nanochat checkpoints. our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. we release all nanoknow artifacts at https://github.com/castorini/nanoknow.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20122v1" target="_blank" rel="noopener">NanoKnow: How to Know What Your Language Model Knows</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.IR</span>
                </div>
                <p class="paper-authors">Lingwei Gu, Nour Jedidi, Jimmy Lin</p>
                <p class="paper-abstract" id="abstract-12">How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a &quot;black box&quot; -- unknown or inaccessible. The recent release of...</p>
                <button class="expand-btn" onclick="toggleAbstract(12, 'How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a &quot;black box&quot; -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model&#x27;s parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat&#x27;s pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow&#x27;s utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.', 'How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a &quot;black box&quot; -- unknown or inaccessible. The recent release of...')" id="expand-btn-12">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20122v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20122v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="resyn: autonomously scaling synthetic environments for reasoning models andre he nathaniel weir kaj bostrom allen nie darion cassel sam bayless huzefa rangwala reinforcement learning with verifiable rewards (rlvr) has emerged as a promising approach for training reasoning language models (rlms) by leveraging supervision from verifiers. although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. in this work, we scale rlvr by introducing resyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. a qwen2.5-7b-instruct model trained with rl on resyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging bbeh benchmark. ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in rlms">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20117v1" target="_blank" rel="noopener">ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.AI</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Andre He, Nathaniel Weir, Kaj Bostrom, Allen Nie, Darion Cassel <em>(+2 more)</em></p>
                <p class="paper-abstract" id="abstract-13">Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier...</p>
                <button class="expand-btn" onclick="toggleAbstract(13, 'Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs', 'Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier...')" id="expand-btn-13">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20117v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20117v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="reliable abstention under adversarial injections: tight lower bounds and new upper bounds ezra edelman surbhi goel we study online learning in the adversarial injection model introduced by [goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\ from an unknown distribution $\mathcal{d}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. crucially, labels are always consistent with a fixed target concept (the clean-label setting). the learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\ rounds. perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $o(d^2 \log t)$ combined error for vc dimension $d$, while distribution-agnostic algorithms achieve only $\tilde{o}(\sqrt{t})$ for restricted classes, leaving open whether this gap is fundamental. we resolve this question by proving a matching $œâ(\sqrt{t})$ lower bound for vc dimension $1$, establishing a sharp separation between the two information regimes. on the algorithmic side, we introduce a potential-based framework driven by \emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. we instantiate this framework using two combinatorial dimensions: (1) \emph{inference dimension}, yielding combined error $\tilde{o}(t^{1-1/k})$ for classes of inference dimension $k$, and (2) \emph{certificate dimension}, a new relaxation we introduce. as an application, we show that halfspaces in $\mathbb{r}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\tilde{o}(t^{2/3})$ for this class. this is notable since [blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20111v1" target="_blank" rel="noopener">Reliable Abstention under Adversarial Injections: Tight Lower Bounds and New Upper Bounds</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Ezra Edelman, Surbhi Goel</p>
                <p class="paper-abstract" id="abstract-14">We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\ from an unknown distribution...</p>
                <button class="expand-btn" onclick="toggleAbstract(14, 'We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\ from an unknown distribution $\mathcal{D}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. Crucially, labels are always consistent with a fixed target concept (the clean-label setting). The learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\ rounds. Perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $O(d^2 \log T)$ combined error for VC dimension $d$, while distribution-agnostic algorithms achieve only $\tilde{O}(\sqrt{T})$ for restricted classes, leaving open whether this gap is fundamental. We resolve this question by proving a matching $Œ©(\sqrt{T})$ lower bound for VC dimension $1$, establishing a sharp separation between the two information regimes. On the algorithmic side, we introduce a potential-based framework driven by \emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. We instantiate this framework using two combinatorial dimensions: (1) \emph{inference dimension}, yielding combined error $\tilde{O}(T^{1-1/k})$ for classes of inference dimension $k$, and (2) \emph{certificate dimension}, a new relaxation we introduce. As an application, we show that halfspaces in $\mathbb{R}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\tilde{O}(T^{2/3})$ for this class. This is notable since [Blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention.', 'We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\ from an unknown distribution...')" id="expand-btn-14">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20111v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20111v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="align when they want, complement when they need! human-centered ensembles for adaptive human-ai collaboration hasan amin ming yin rajiv khanna in human-ai decision making, designing ai that complements human expertise has been a natural strategy to enhance human-ai collaboration, yet it often comes at the cost of decreased ai performance in areas of human strengths. this can inadvertently erode human trust and cause them to ignore ai advice precisely when it is most needed. conversely, an aligned ai fosters trust yet risks reinforcing suboptimal human behavior and lowering human-ai team performance. in this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single ai model to assist human decision making. to overcome this, we introduce a novel human-centered adaptive ai ensemble that strategically toggles between two specialist ai models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal rational routing shortcut mechanism. comprehensive theoretical analyses elucidate why the adaptive ai ensemble is effective and when it yields maximum benefits. moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive ai ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single ai models that are trained to either optimize for their independent performance or even the human-ai team performance.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20104v1" target="_blank" rel="noopener">Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.AI</span><span class="paper-category">cs.HC</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Hasan Amin, Ming Yin, Rajiv Khanna</p>
                <p class="paper-abstract" id="abstract-15">In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in...</p>
                <button class="expand-btn" onclick="toggleAbstract(15, 'In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.', 'In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in...')" id="expand-btn-15">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20104v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20104v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="barriersteer: llm safety via learning barrier steering thanh q. tran arun verma kiwan wong bryan kian hsiang low daniela rus wei xiao despite the state-of-the-art performance of large language models (llms) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. we introduce barriersteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model&#x27;s latent representation space. barriersteer employs a steering mechanism based on control barrier functions (cbfs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. by enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying llm parameters, barriersteer preserves the model&#x27;s original capabilities and performance. we provide theoretical results establishing that applying cbfs in latent space offers a principled and computationally efficient approach to enforcing safety. our experiments across multiple models and datasets show that barriersteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20102v1" target="_blank" rel="noopener">BarrierSteer: LLM Safety via Learning Barrier Steering</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Thanh Q. Tran, Arun Verma, Kiwan Wong, Bryan Kian Hsiang Low, Daniela Rus <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-16">Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to...</p>
                <button class="expand-btn" onclick="toggleAbstract(16, 'Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model&#x27;s latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model&#x27;s original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.', 'Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to...')" id="expand-btn-16">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20102v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20102v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="causalflip: a benchmark for llm causal judgment beyond semantic matching yuzhe wang yaochen zhu jundong li as large language models (llms) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. however, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of llms, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. to bridge this critical gap, we propose a new causal reasoning benchmark, causalflip, designed to encourage the development of new llm paradigm or training algorithms that ground llm reasoning in causality rather than semantic correlation. causalflip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. to further probe models&#x27; reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. we evaluate llms under multiple training paradigms, including answer-only training, explicit chain-of-thought (cot) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. our results show that explicit cot can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base llms.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20094v1" target="_blank" rel="noopener">CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Yuzhe Wang, Yaochen Zhu, Jundong Li</p>
                <p class="paper-abstract" id="abstract-17">As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious...</p>
                <button class="expand-btn" onclick="toggleAbstract(17, 'As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models&#x27; reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.', 'As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious...')" id="expand-btn-17">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20094v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20094v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="how retrieved context shapes internal representations in rag samuel yeh sharon li retrieval-augmented generation (rag) enhances large language models (llms) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. in realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. while prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in rag. in this work, we study rag through the lens of latent representations. we systematically analyze how different types of retrieved documents affect the hidden states of llms, and how these internal representation shifts relate to downstream generation behavior. across four question-answering datasets and three llms, we analyze internal representations under controlled single- and multi-document settings. our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on llms output behaviors and insights for rag system design.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20091v1" target="_blank" rel="noopener">How Retrieved Context Shapes Internal Representations in RAG</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Samuel Yeh, Sharon Li</p>
                <p class="paper-abstract" id="abstract-18">Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In...</p>
                <button class="expand-btn" onclick="toggleAbstract(18, 'Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.', 'Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In...')" id="expand-btn-18">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20091v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20091v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="do large language models understand data visualization principles? martin sinnona valentin bonas viviana siless emmanuel iarussi data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. while prior work has shown that large language models (llms) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (vlms) can reason about and enforce visualization principles directly. constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. this motivates leveraging llms and vlms as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. in this paper, we present the first systematic evaluation of both llms and vlms on their ability to reason about visualization principles, using hard verification ground truth derived from answer set programming (asp). we compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 vega-lite specifications annotated with explicit principle violations, complemented by over 300 real-world vega-lite charts. we evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. they also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.20084v1" target="_blank" rel="noopener">Do Large Language Models Understand Data Visualization Principles?</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">üìÖ 2026-02-23</span>
                    <span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Martin Sinnona, Valentin Bonas, Viviana Siless, Emmanuel Iarussi</p>
                <p class="paper-abstract" id="abstract-19">Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate...</p>
                <button class="expand-btn" onclick="toggleAbstract(19, 'Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.', 'Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate...')" id="expand-btn-19">Show more ‚ñº</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.20084v1" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.20084v1" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                </div>
            </article>

        </div>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2026 Coding Blog | BST 236 Computing I | Harvard University</p>
            <p class="footer-links">
                <a href="https://github.com">GitHub</a>
            </p>
        </div>
    </footer>

    <script src="js/main.js"></script>
    <script>
        function filterPapers() {
            const query = document.getElementById('searchInput').value.toLowerCase();
            const cards = document.querySelectorAll('.paper-card');
            
            cards.forEach(card => {
                const searchText = card.getAttribute('data-search');
                if (searchText.includes(query)) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });
        }
        
        function toggleAbstract(index, fullText, shortText) {
            const abstractEl = document.getElementById('abstract-' + index);
            const btnEl = document.getElementById('expand-btn-' + index);
            
            if (abstractEl.classList.contains('expanded')) {
                abstractEl.textContent = shortText;
                abstractEl.classList.remove('expanded');
                btnEl.textContent = 'Show more ‚ñº';
            } else {
                abstractEl.textContent = fullText;
                abstractEl.classList.add('expanded');
                btnEl.textContent = 'Show less ‚ñ≤';
            }
        }
        
        // Client-side arXiv fetching for custom keywords
        // CORS proxies to try in order
        const corsProxies = [
            url => `https://corsproxy.io/?${encodeURIComponent(url)}`,
            url => `https://api.codetabs.com/v1/proxy?quest=${encodeURIComponent(url)}`,
            url => `https://api.allorigins.win/raw?url=${encodeURIComponent(url)}`
        ];
        
        async function tryFetchWithProxies(url, proxies) {
            for (let i = 0; i < proxies.length; i++) {
                const proxyUrl = proxies[i](url);
                try {
                    const response = await fetch(proxyUrl, { timeout: 10000 });
                    if (response.ok) {
                        return await response.text();
                    }
                } catch (e) {
                    console.log(`Proxy ${i + 1} failed:`, e.message);
                }
            }
            throw new Error('All CORS proxies failed. Please try again later or edit scripts/config.json directly.');
        }
        
        async function fetchCustomPapers() {
            const input = document.getElementById('customKeywords').value.trim();
            const btn = document.getElementById('fetchBtn');
            const grid = document.getElementById('papersGrid');
            const keywordsDisplay = document.getElementById('currentKeywords');
            
            if (!input) {
                alert('Please enter at least one keyword');
                return;
            }
            
            const keywords = input.split(',').map(k => k.trim()).filter(k => k);
            
            btn.disabled = true;
            btn.textContent = 'Fetching...';
            
            // Update displayed keywords
            keywordsDisplay.innerHTML = keywords.map(kw => 
                `<span class="keyword-tag">${escapeHtml(kw)}</span>`
            ).join('');
            
            // Build arXiv query
            const searchQuery = keywords.map(kw => `all:"${kw}"`).join(' OR ');
            const url = `https://export.arxiv.org/api/query?search_query=${encodeURIComponent(searchQuery)}&start=0&max_results=20&sortBy=submittedDate&sortOrder=descending`;
            
            try {
                const xmlText = await tryFetchWithProxies(url, corsProxies);
                
                // Parse XML
                const parser = new DOMParser();
                const xmlDoc = parser.parseFromString(xmlText, 'text/xml');
                const entries = xmlDoc.querySelectorAll('entry');
                
                if (entries.length === 0) {
                    grid.innerHTML = `
                        <div class="no-results" style="grid-column: 1 / -1;">
                            <h3>No papers found</h3>
                            <p>Try different keywords.</p>
                        </div>
                    `;
                } else {
                    let html = '';
                    entries.forEach((entry, i) => {
                        const title = entry.querySelector('title')?.textContent?.replace(/\s+/g, ' ').trim() || 'Untitled';
                        const abstract = entry.querySelector('summary')?.textContent?.replace(/\s+/g, ' ').trim() || 'No abstract';
                        const published = entry.querySelector('published')?.textContent?.substring(0, 10) || 'Unknown';
                        const id = entry.querySelector('id')?.textContent || '';
                        const pdfUrl = id.replace('/abs/', '/pdf/') + '.pdf';
                        
                        const authors = [];
                        entry.querySelectorAll('author name').forEach(n => authors.push(n.textContent));
                        const authorsStr = authors.length > 5 
                            ? authors.slice(0, 5).join(', ') + ` <em>(+${authors.length - 5} more)</em>`
                            : authors.join(', ');
                        
                        const categories = [];
                        entry.querySelectorAll('category').forEach(c => {
                            const term = c.getAttribute('term');
                            if (term && categories.length < 3) categories.push(term);
                        });
                        
                        const abstractShort = abstract.length > 200 
                            ? abstract.substring(0, 200).replace(/\s+\S*$/, '') + '...'
                            : abstract;
                        
                        html += `
                            <article class="paper-card" data-search="${escapeHtml(title.toLowerCase())} ${escapeHtml(authors.join(' ').toLowerCase())} ${escapeHtml(abstract.toLowerCase())}">
                                <h2 class="paper-title">
                                    <a href="${escapeHtml(id)}" target="_blank" rel="noopener">${escapeHtml(title)}</a>
                                </h2>
                                <div class="paper-meta">
                                    <span class="paper-date">üìÖ ${escapeHtml(published)}</span>
                                    ${categories.map(c => `<span class="paper-category">${escapeHtml(c)}</span>`).join('')}
                                </div>
                                <p class="paper-authors">${authorsStr}</p>
                                <p class="paper-abstract" id="abstract-dyn-${i}">${escapeHtml(abstractShort)}</p>
                                <button class="expand-btn" onclick="toggleAbstract('dyn-${i}', '${escapeHtml(abstract).replace(/'/g, "\\'")}', '${escapeHtml(abstractShort).replace(/'/g, "\\'")}')">Show more ‚ñº</button>
                                <div class="paper-actions">
                                    <a href="${escapeHtml(pdfUrl)}" class="pdf-btn" target="_blank" rel="noopener">üìÑ View PDF</a>
                                    <a href="${escapeHtml(id)}" class="arxiv-btn" target="_blank" rel="noopener">üîó arXiv</a>
                                </div>
                            </article>
                        `;
                    });
                    grid.innerHTML = html;
                }
                
                // Update timestamp
                const now = new Date().toISOString().replace('T', ' ').substring(0, 19) + ' (live fetch)';
                document.querySelector('.update-info').textContent = 'Last updated: ' + now;
                
            } catch (error) {
                console.error('Fetch error:', error);
                grid.innerHTML = `
                    <div class="no-results" style="grid-column: 1 / -1;">
                        <h3>Error fetching papers</h3>
                        <p>${escapeHtml(error.message)}</p>
                        <p style="margin-top: 15px; font-size: 0.9rem;">
                            <strong>Alternative:</strong> Edit <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">scripts/config.json</code> 
                            and run <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">python scripts/fetch_arxiv.py</code> locally, 
                            or push to GitHub to trigger the auto-update.
                        </p>
                    </div>
                `;
            }
            
            btn.disabled = false;
            btn.textContent = 'Fetch Papers';
        }
        
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }
        
        // Allow Enter key to trigger fetch
        document.getElementById('customKeywords').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') fetchCustomPapers();
        });
    </script>
</body>
</html>
