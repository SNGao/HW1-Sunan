<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Feed | Coding Blog</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <style>
        .papers-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 100px 20px 40px;
        }
        .papers-header {
            text-align: center;
            margin-bottom: 40px;
        }
        .papers-header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .update-info {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 8px;
        }
        .auto-update-info {
            color: #22c55e;
            font-size: 0.85rem;
            margin-bottom: 15px;
        }
        .config-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-bottom: 10px;
        }
        .config-hint code {
            background: #334155;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: var(--font-code);
            color: #f472b6;
        }
        .search-box {
            max-width: 500px;
            margin: 20px auto;
        }
        .search-box input {
            width: 100%;
            padding: 12px 20px;
            border: 2px solid var(--border);
            border-radius: 25px;
            background: var(--card-bg);
            color: var(--text-primary);
            font-size: 1rem;
            outline: none;
            transition: border-color 0.3s;
        }
        .search-box input:focus {
            border-color: var(--primary);
        }
        .search-box input::placeholder {
            color: var(--text-muted);
        }
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 25px;
        }
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 25px;
            transition: all 0.3s ease;
        }
        .paper-card:hover {
            border-color: var(--primary);
            transform: translateY(-3px);
            box-shadow: 0 10px 30px rgba(236, 72, 153, 0.15);
        }
        .paper-card.hidden {
            display: none;
        }
        .paper-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-bottom: 10px;
            line-height: 1.4;
        }
        .paper-title a {
            color: inherit;
            text-decoration: none;
            transition: color 0.3s;
        }
        .paper-title a:hover {
            color: var(--primary);
        }
        .paper-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 12px;
        }
        .paper-date {
            background: var(--primary);
            color: white;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
            font-weight: 500;
        }
        .paper-category {
            background: #334155;
            color: #e2e8f0;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
        }
        .paper-authors {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 12px;
            line-height: 1.5;
        }
        .paper-abstract {
            color: #94a3b8;
            font-size: 0.9rem;
            line-height: 1.7;
            margin-bottom: 15px;
        }
        .paper-abstract.expanded {
            max-height: none;
        }
        .expand-btn {
            background: none;
            border: none;
            color: var(--primary);
            cursor: pointer;
            font-size: 0.85rem;
            padding: 0;
            margin-bottom: 15px;
        }
        .expand-btn:hover {
            text-decoration: underline;
        }
        .paper-actions {
            display: flex;
            gap: 10px;
        }
        .pdf-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            transition: all 0.3s;
        }
        .pdf-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 15px rgba(236, 72, 153, 0.3);
        }
        .arxiv-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: var(--surface);
            color: var(--text-primary);
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            border: 1px solid var(--border);
            transition: all 0.3s;
        }
        .arxiv-btn:hover {
            border-color: var(--primary);
            color: var(--primary);
        }
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: var(--text-muted);
        }
        .no-results h3 {
            font-size: 1.5rem;
            margin-bottom: 10px;
            color: var(--text-secondary);
        }
        .keywords-info {
            background: #1e293b;
            padding: 20px;
            border-radius: 12px;
            margin-bottom: 30px;
            text-align: center;
            border: 1px solid #334155;
        }
        .keywords-info > span {
            color: #e2e8f0;
            font-size: 0.95rem;
            font-weight: 500;
        }
        .keyword-tag {
            display: inline-block;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: #ffffff;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin: 4px;
            text-shadow: 0 1px 2px rgba(0,0,0,0.2);
        }
        .keyword-editor {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #334155;
        }
        .keyword-input-group {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .keyword-input {
            padding: 10px 16px;
            border: 2px solid #334155;
            border-radius: 25px;
            background: #0f172a;
            color: #f8fafc;
            font-size: 0.9rem;
            width: 250px;
            outline: none;
            transition: border-color 0.3s;
        }
        .keyword-input:focus {
            border-color: #6366f1;
        }
        .keyword-input::placeholder {
            color: #64748b;
        }
        .fetch-btn {
            padding: 10px 24px;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: white;
            border: none;
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .fetch-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 20px rgba(99, 102, 241, 0.4);
        }
        .fetch-btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }
        .editor-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-top: 8px;
        }
        @media (max-width: 768px) {
            .papers-grid {
                grid-template-columns: 1fr;
            }
            .papers-header h1 {
                font-size: 1.8rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="index.html" class="nav-logo">
                <span class="logo-icon">ğŸ’»</span>
                <span class="logo-text">Coding Blog</span>
            </a>
            <ul class="nav-menu">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li><a href="index.html#projects" class="nav-link">Projects</a></li>
                <li><a href="papers.html" class="nav-link active">Papers</a></li>
                <li><a href="index.html#about" class="nav-link">About</a></li>
            </ul>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <div class="papers-container">
        <div class="papers-header">
            <h1>ğŸ“š arXiv Paper Feed</h1>
            <p class="update-info">Last updated: 2026-03-01 01:51:32 UTC</p>
            <p class="auto-update-info">â° Auto-updates daily at midnight UTC via GitHub Actions</p>
            <div class="keywords-info">
                <span>Current keywords: </span>
                <div id="currentKeywords">
                    <span class="keyword-tag">large language model</span><span class="keyword-tag">machine learning</span><span class="keyword-tag">biostatistics</span><span class="keyword-tag">deep learning</span>
                </div>
                <div class="keyword-editor">
                    <p class="editor-hint">ğŸ”„ Try different keywords (fetches live from arXiv):</p>
                    <div class="keyword-input-group">
                        <input type="text" id="customKeywords" class="keyword-input" 
                               placeholder="e.g., reinforcement learning, NLP" 
                               value="large language model, machine learning, biostatistics, deep learning">
                        <button onclick="fetchCustomPapers()" class="fetch-btn" id="fetchBtn">
                            Fetch Papers
                        </button>
                    </div>
                    <p class="editor-hint">Separate multiple keywords with commas</p>
                </div>
            </div>
            <div class="search-box">
                <input type="text" id="searchInput" placeholder="ğŸ” Filter papers by title, author, or abstract..." oninput="filterPapers()">
            </div>
        </div>

        <div class="papers-grid" id="papersGrid">

            <article class="paper-card" data-search="medix-r1: open ended medical reinforcement learning sahal shaji mullappilly mohammed irfan kurpath omair mohamed mohamed zidan fahad khan salman khan rao anwer hisham cholakkal we introduce medix-r1, an open-ended reinforcement learning (rl) framework for medical multimodal large language models (mllms) that enables clinically grounded, free-form answers beyond multiple-choice formats. medix-r1 fine-tunes a baseline vision-language backbone with group based rl and a composite reward tailored for medical reasoning: an llm-based accuracy reward that judges semantic correctness with a strict yes/no decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. this multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or mcq-only rewards fall short. to measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a reference-based llm-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. despite using only $\sim51$k instruction examples, medix-r1 achieves excellent results across standard medical llm (text-only) and vlm (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. our results demonstrate that open-ended rl with comprehensive reward signals and llm-based evaluation is a practical path toward reliable medical reasoning in multimodal models. our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23363v1" target="_blank" rel="noopener">MediX-R1: Open Ended Medical Reinforcement Learning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Sahal Shaji Mullappilly, Mohammed Irfan Kurpath, Omair Mohamed, Mohamed Zidan, Fahad Khan <em>(+3 more)</em></p>
                <p class="paper-abstract" id="abstract-0">We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond...</p>
                <button class="expand-btn" onclick="toggleAbstract(0, 'We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com', 'We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond...')" id="expand-btn-0">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23363v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23363v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="model agreement via anchoring eric eaton surbhi goel marcel hussing michael kearns aaron roth sikata bela sengupta jessica sorrell numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. we adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. we would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies. we develop a simple general technique for proving bounds on independent model disagreement based on $\textit{anchoring}$ to the average of two models within the analysis. we then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). for clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23360v1" target="_blank" rel="noopener">Model Agreement via Anchoring</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Eric Eaton, Surbhi Goel, Marcel Hussing, Michael Kearns, Aaron Roth <em>(+2 more)</em></p>
                <p class="paper-abstract" id="abstract-1">Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model...</p>
                <button class="expand-btn" onclick="toggleAbstract(1, 'Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies. We develop a simple general technique for proving bounds on independent model disagreement based on $\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.', 'Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model...')" id="expand-btn-1">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23360v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23360v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="a dataset is worth 1 mb elad kimchi shoshani leeyam gabay yedid hoshen a dataset server must often distribute the same large payload to many clients, incurring massive communication costs. since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. while dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. in this paper, we propose pseudo-labels as data (plada), a method that completely eliminates pixel transmission. we assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., imagenet-1k, imagenet-21k) and communicate a new task by transmitting only the class labels for specific images. to address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. this selection process simultaneously maximizes training efficiency and minimizes transmission payload. experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 mb while retaining high classification accuracy, offering a promising solution for efficient dataset serving.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23358v1" target="_blank" rel="noopener">A Dataset is Worth 1 MB</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Elad Kimchi Shoshani, Leeyam Gabay, Yedid Hoshen</p>
                <p class="paper-abstract" id="abstract-2">A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks,...</p>
                <button class="expand-btn" onclick="toggleAbstract(2, 'A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.', 'A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks,...')" id="expand-btn-2">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23358v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23358v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="sotalign: semi-supervised alignment of unimodal vision and language models via optimal transport simon roschmann paul krzakala sonia mazelet quentin bouniot zeynep akata the platonic representation hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. in this work, we ask whether meaningful alignment can be achieved with substantially less supervision. we introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. to address this challenge, we propose sotalign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. unlike existing semi-supervised methods, sotalign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23353v1" target="_blank" rel="noopener">SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Simon Roschmann, Paul Krzakala, Sonia Mazelet, Quentin Bouniot, Zeynep Akata</p>
                <p class="paper-abstract" id="abstract-3">The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by...</p>
                <button class="expand-btn" onclick="toggleAbstract(3, 'The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.', 'The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by...')" id="expand-btn-3">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23353v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23353v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="flashoptim: optimizers for memory efficient training jose javier gonzalez ortiz abhay gupta chris renard davis blalock standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. these bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. with each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100gb of accelerator memory. we introduce flashoptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and api compatibility. our approach introduces two key techniques. first, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. together with 16-bit gradients, these techniques reduce adamw memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. they also cut model checkpoint sizes by more than half. experiments with flashoptim applied to sgd, adamw, and lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including llama-3.1-8b finetuning.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23349v1" target="_blank" rel="noopener">FlashOptim: Optimizers for Memory Efficient Training</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Jose Javier Gonzalez Ortiz, Abhay Gupta, Chris Renard, Davis Blalock</p>
                <p class="paper-abstract" id="abstract-4">Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and...</p>
                <button class="expand-btn" onclick="toggleAbstract(4, 'Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory. We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half. Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.', 'Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and...')" id="expand-btn-4">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23349v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23349v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="mean estimation from coarse data: characterizations and efficient algorithms alkis kalavasis anay mehrotra manolis zampetakis felix zhou ziyu zhu coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. this occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. we study gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. when the coarse samples, roughly speaking, have ``low&#x27;&#x27; information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). recent work by fotakis, kalavasis, kontonis, and tzamos [fkkt21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. moreover, they showed that without convexity, mean estimation becomes np-hard. however, two fundamental questions remained open: (1) when is the mean identifiable under convex partitions? (2) is computationally efficient estimation possible under identifiability and convex partitions? this work resolves both questions. [...]">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23341v1" target="_blank" rel="noopener">Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.DS</span><span class="paper-category">math.ST</span>
                </div>
                <p class="paper-authors">Alkis Kalavasis, Anay Mehrotra, Manolis Zampetakis, Felix Zhou, Ziyu Zhu</p>
                <p class="paper-abstract" id="abstract-5">Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding,...</p>
                <button class="expand-btn" onclick="toggleAbstract(5, 'Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low&#x27;&#x27; information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]', 'Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding,...')" id="expand-btn-5">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23341v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23341v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="differentiable zero-one loss via hypersimplex projections camilo gomez pengyang wang liansheng tang recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. in this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term soft-binary-argmax. after deriving its mathematical properties, we show how its jacobian can be efficiently computed and integrated into binary and multiclass learning systems. empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23336v1" target="_blank" rel="noopener">Differentiable Zero-One Loss via Hypersimplex Projections</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">stat.ML</span>
                </div>
                <p class="paper-authors">Camilo Gomez, Pengyang Wang, Liansheng Tang</p>
                <p class="paper-abstract" id="abstract-6">Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment...</p>
                <button class="expand-btn" onclick="toggleAbstract(6, 'Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.', 'Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment...')" id="expand-btn-6">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23336v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23336v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="utilizing llms for industrial process automation salim fares a growing number of publications address the best practices to use large language models (llms) for software engineering in recent years. however, most of this work focuses on widely-used general purpose programming languages like python due to their widespread usage training data. the utility of llms for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. this research aims to utilize and integrate llms in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23331v1" target="_blank" rel="noopener">Utilizing LLMs for Industrial Process Automation</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.SE</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Salim Fares</p>
                <p class="paper-abstract" id="abstract-7">A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general...</p>
                <button class="expand-btn" onclick="toggleAbstract(7, 'A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.', 'A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general...')" id="expand-btn-7">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23331v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23331v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="toward expert investment teams:a multi-agent llm system with fine-grained trading tasks kunihiro miyazaki takanobu kawahara stephen roberts stefan zohren the advancement of large language models (llms) has accelerated the development of autonomous financial trading systems. while mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. therefore, we propose a multi-agent llm trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. we evaluate the proposed framework using japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system&#x27;s output. this approach achieves superior performance. these findings contribute to the design of agent structure and task configuration when applying llm agents to trading systems in practical settings.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23330v1" target="_blank" rel="noopener">Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.AI</span><span class="paper-category">q-fin.TR</span>
                </div>
                <p class="paper-authors">Kunihiro Miyazaki, Takanobu Kawahara, Stephen Roberts, Stefan Zohren</p>
                <p class="paper-abstract" id="abstract-8">The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and...</p>
                <button class="expand-btn" onclick="toggleAbstract(8, 'The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system&#x27;s output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.', 'The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and...')" id="expand-btn-8">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23330v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23330v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="llm novice uplift on dual-use, in silico biology tasks chen bo calvin zhang christina q. knight nicholas kruus jason hausenloy pedro medeiros nathaniel li aiden kim yury orlovskiy coleman breen bryce cai jasper gÃ¶tting andrew bo liu samira nedungadi paula rodriguez yannis yiming he mohamed shaaban zifan wang seth donoughe julian michael large language models (llms) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. this uncertainty is central to understanding both scientific acceleration and dual-use risk. we conducted a multi-model, multi-benchmark human uplift study comparing novices with llm access versus internet-only access across eight biosecurity-relevant task sets. participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). we found that llm access provided substantial uplift: novices with llms were 4.16 times more accurate than controls (95% ci [2.63, 6.87]). on four benchmarks with available expert baselines (internet-only), novices with llms outperformed experts on three of them. perhaps surprisingly, standalone llms often exceeded llm-assisted novices, indicating that users were not eliciting the strongest available contributions from the llms. most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. overall, llms substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23329v1" target="_blank" rel="noopener">LLM Novice Uplift on Dual-Use, In Silico Biology Tasks</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.AI</span><span class="paper-category">cs.CL</span><span class="paper-category">cs.CR</span>
                </div>
                <p class="paper-authors">Chen Bo Calvin Zhang, Christina Q. Knight, Nicholas Kruus, Jason Hausenloy, Pedro Medeiros <em>(+14 more)</em></p>
                <p class="paper-abstract" id="abstract-9">Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only...</p>
                <button class="expand-btn" onclick="toggleAbstract(9, 'Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.', 'Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only...')" id="expand-btn-9">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23329v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23329v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="spin glass concepts in computer science, statistics, and learning andrea montanari spin glass theory studies the structure of sublevel sets and minima (or near-minima) of certain classes of random functions in high dimension. near-minima of random functions also play an important role in high-dimensional statistics and statistical learning, where minimizing the empirical risk (which is a random function of the model parameters) is the method of choice for learning a statistical model from noisy data. finally, near-minima of random functions are obviously central to average-case analysis of optimization algorithms. computer science, statistics, and machine learning naturally lead to questions that are traditionally not addressed within physics and mathematical physics. i will try to explain how ideas from spin glass theory have seeded recent developments in these fields. (this article was written on the occasion of the 2024 abel prize to michel talagrand.)">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23326v1" target="_blank" rel="noopener">Spin Glass Concepts in Computer Science, Statistics, and Learning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">math.PR</span><span class="paper-category">cond-mat.dis-nn</span>
                </div>
                <p class="paper-authors">Andrea Montanari</p>
                <p class="paper-abstract" id="abstract-10">Spin glass theory studies the structure of sublevel sets and minima (or near-minima) of certain classes of random functions in high dimension. Near-minima of random functions also play an important...</p>
                <button class="expand-btn" onclick="toggleAbstract(10, 'Spin glass theory studies the structure of sublevel sets and minima (or near-minima) of certain classes of random functions in high dimension. Near-minima of random functions also play an important role in high-dimensional statistics and statistical learning, where minimizing the empirical risk (which is a random function of the model parameters) is the method of choice for learning a statistical model from noisy data. Finally, near-minima of random functions are obviously central to average-case analysis of optimization algorithms. Computer science, statistics, and machine learning naturally lead to questions that are traditionally not addressed within physics and mathematical physics. I will try to explain how ideas from spin glass theory have seeded recent developments in these fields. (This article was written on the occasion of the 2024 Abel Prize to Michel Talagrand.)', 'Spin glass theory studies the structure of sublevel sets and minima (or near-minima) of certain classes of random functions in high dimension. Near-minima of random functions also play an important...')" id="expand-btn-10">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23326v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23326v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="deep ensemble graph neural networks for probabilistic cosmic-ray direction and energy reconstruction in autonomous radio arrays arsÃ¨ne ferriÃ¨re aurÃ©lien benoit-lÃ©vy olivier martineau-huynh matÃ­as tueros using advanced machine learning techniques, we developed a method for reconstructing precisely the arrival direction and energy of ultra-high-energy cosmic rays from the voltage traces they induced on ground-based radio detector arrays. in our approach, triggered antennas are represented as a graph structure, which serves as input for a graph neural network (gnn). by incorporating physical knowledge into both the gnn architecture and the input data, we improve the precision and reduce the required size of the training set with respect to a fully data-driven approach. this method achieves an angular resolution of 0.092Â° and an electromagnetic energy reconstruction resolution of 16.4% on simulated data with realistic noise conditions. we also employ uncertainty estimation methods to enhance the reliability of our predictions, quantifying the confidence of the gnn&#x27;s outputs and providing confidence intervals for both direction and energy reconstruction. finally, we investigate strategies to verify the model&#x27;s consistency and robustness under real life variations, with the goal of identifying scenarios in which predictions remain reliable despite domain shifts between simulation and reality.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23321v1" target="_blank" rel="noopener">Deep ensemble graph neural networks for probabilistic cosmic-ray direction and energy reconstruction in autonomous radio arrays</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">astro-ph.IM</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">ArsÃ¨ne FerriÃ¨re, AurÃ©lien Benoit-LÃ©vy, Olivier Martineau-Huynh, MatÃ­as Tueros</p>
                <p class="paper-abstract" id="abstract-11">Using advanced machine learning techniques, we developed a method for reconstructing precisely the arrival direction and energy of ultra-high-energy cosmic rays from the voltage traces they induced...</p>
                <button class="expand-btn" onclick="toggleAbstract(11, 'Using advanced machine learning techniques, we developed a method for reconstructing precisely the arrival direction and energy of ultra-high-energy cosmic rays from the voltage traces they induced on ground-based radio detector arrays. In our approach, triggered antennas are represented as a graph structure, which serves as input for a graph neural network (GNN). By incorporating physical knowledge into both the GNN architecture and the input data, we improve the precision and reduce the required size of the training set with respect to a fully data-driven approach. This method achieves an angular resolution of 0.092Â° and an electromagnetic energy reconstruction resolution of 16.4% on simulated data with realistic noise conditions. We also employ uncertainty estimation methods to enhance the reliability of our predictions, quantifying the confidence of the GNN&#x27;s outputs and providing confidence intervals for both direction and energy reconstruction. Finally, we investigate strategies to verify the model&#x27;s consistency and robustness under real life variations, with the goal of identifying scenarios in which predictions remain reliable despite domain shifts between simulation and reality.', 'Using advanced machine learning techniques, we developed a method for reconstructing precisely the arrival direction and energy of ultra-high-energy cosmic rays from the voltage traces they induced...')" id="expand-btn-11">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23321v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23321v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="parammem: augmenting language agents with parametric reflective memory tianjun yao yongqiang chen yujia zheng pan li zhiqiang shen kun zhang self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. we introduce parammem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. building on this module, we propose paramagent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. further analysis reveals that parammem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of parammem as an effective component for enhancing language agents.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23320v1" target="_blank" rel="noopener">ParamMem: Augmenting Language Agents with Parametric Reflective Memory</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.MA</span>
                </div>
                <p class="paper-authors">Tianjun Yao, Yongqiang Chen, Yujia Zheng, Pan Li, Zhiqiang Shen <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-12">Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this...</p>
                <button class="expand-btn" onclick="toggleAbstract(12, 'Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.', 'Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this...')" id="expand-btn-12">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23320v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23320v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="evaluating zero-shot and one-shot adaptation of small language models in leader-follower interaction rafael r. baptista andrÃ© de lima salgado ricardo v. godoy marcelo becker thiago boaventura gustavo j. g. lahr leader-follower interaction is an important paradigm in human-robot interaction (hri). yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. while large language models (llms) have shown promise for natural communication, their size and latency limit on-device deployment. small language models (slms) offer a potential alternative, but their effectiveness for role classification in hri has not been systematically evaluated. in this paper, we present a benchmark of slms for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. we investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. experiments with qwen2.5-0.5b reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. however, results also indicate a performance degradation in one-shot modes, where increased context length challenges the model&#x27;s architectural capacity. these findings demonstrate that fine-tuned slms provide an effective solution for direct role assignment, while highlighting critical trade-offs between dialogue complexity and classification reliability on the edge.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23312v1" target="_blank" rel="noopener">Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.HC</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Rafael R. Baptista, AndrÃ© de Lima Salgado, Ricardo V. Godoy, Marcelo Becker, Thiago Boaventura <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-13">Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots....</p>
                <button class="expand-btn" onclick="toggleAbstract(13, 'Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we present a benchmark of SLMs for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. We investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. Experiments with Qwen2.5-0.5B reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. However, results also indicate a performance degradation in one-shot modes, where increased context length challenges the model&#x27;s architectural capacity. These findings demonstrate that fine-tuned SLMs provide an effective solution for direct role assignment, while highlighting critical trade-offs between dialogue complexity and classification reliability on the edge.', 'Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots....')" id="expand-btn-13">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23312v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23312v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="thinkomni: lifting textual reasoning to omni-modal scenarios via guidance decoding yiran guan sifan tu dingkang liang linghao zhu jianzhong ju zhenbo luo jian luan yuliang liu xiang bai omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. while existing omni-modal large language models (ollm) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (lrm). however, enhancing the reasoning ability of ollms through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. to address these limitations, we propose thinkomni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. thinkomni introduces two key components: 1) lrm-as-a-guide, which leverages off-the-shelf lrms to guide the ollm decoding process; 2) stepwise contrastive scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. experiments on six multi-modal reasoning benchmarks demonstrate that thinkomni consistently delivers performance improvements, with main results achieving 70.2 on mathvista and 75.5 on mmau. overall, thinkomni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23306v1" target="_blank" rel="noopener">ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Yiran Guan, Sifan Tu, Dingkang Liang, Linghao Zhu, Jianzhong Ju <em>(+4 more)</em></p>
                <p class="paper-abstract" id="abstract-14">Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving...</p>
                <button class="expand-btn" onclick="toggleAbstract(14, 'Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.', 'Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving...')" id="expand-btn-14">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23306v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23306v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="a proper scoring rule for virtual staining samuel tonks steve hood ryan musso ceridwen hopely steve titus minh doan iain styles alexander krull generative virtual staining (vs) models for high-throughput screening (hts) can provide an estimated posterior distribution of possible biological feature values for each input and cell. however, when evaluating a vs model, the true posterior is unavailable. existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. we introduce information gain (ig) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. ig is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. we evaluate diffusion- and gan-based models on an extensive hts dataset using ig and other metrics and show that ig can reveal substantial performance differences other metrics cannot.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23305v1" target="_blank" rel="noopener">A Proper Scoring Rule for Virtual Staining</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Samuel Tonks, Steve Hood, Ryan Musso, Ceridwen Hopely, Steve Titus <em>(+3 more)</em></p>
                <p class="paper-abstract" id="abstract-15">Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However,...</p>
                <button class="expand-btn" onclick="toggleAbstract(15, 'Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. IG is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. We evaluate diffusion- and GAN-based models on an extensive HTS dataset using IG and other metrics and show that IG can reveal substantial performance differences other metrics cannot.', 'Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However,...')" id="expand-btn-15">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23305v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23305v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="inferential mechanics part 1: causal mechanistic theories of machine learning in chemical biology with implications ilya balabin thomas m. kaiser machine learning techniques are now routinely encountered in research laboratories across the globe. impressive progress has been made through ml and ai techniques with regards to large data set processing. this progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. however, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. this series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. this paper, part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. initial proof of these principles on a family of akt inhibitors is also provided. the second paper containing part 2 will provide a formal exploration of chemical similarity, and part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. this series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in nature without the need for the tools of reductionism: inferential mechanics.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23303v1" target="_blank" rel="noopener">Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Ilya Balabin, Thomas M. Kaiser</p>
                <p class="paper-abstract" id="abstract-16">Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set...</p>
                <button class="expand-btn" onclick="toggleAbstract(16, 'Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.', 'Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set...')" id="expand-btn-16">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23303v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23303v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="a mixture-of-experts model for multimodal emotion recognition in conversations soumya dutta smruthi balaji sriram ganapathy emotion recognition in conversations (erc) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. we propose mixture of speech-text experts for recognition of emotions (mister-e), a modular mixture-of-experts (moe) framework designed to decouple two core challenges in erc: modality-specific context modeling and multimodal information fusion. mister-e leverages large language models (llms) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. the system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. to further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a kl-divergence-based regulariza-tion across expert predictions. importantly, mister-e does not rely on speaker identity at any stage. experiments on three benchmark datasets-iemocap, meld, and mosi-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted f1-scores respectively, outperforming several baseline speech-text erc systems. we also provide various ablations to highlight the contributions made in the proposed approach.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23300v1" target="_blank" rel="noopener">A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">eess.AS</span>
                </div>
                <p class="paper-authors">Soumya Dutta, Smruthi Balaji, Sriram Ganapathy</p>
                <p class="paper-abstract" id="abstract-17">Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple...</p>
                <button class="expand-btn" onclick="toggleAbstract(17, 'Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.', 'Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple...')" id="expand-btn-17">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23300v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23300v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="conformalized neural networks for federated uncertainty quantification under dual heterogeneity quang-huy nguyen jiaqi wang wei-shinn ku federated learning (fl) faces challenges in uncertainty quantification (uq). without reliable uq, fl systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. existing federated uq approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. conformal prediction is a widely used distribution-free uq framework, yet its applications in heterogeneous fl settings remains underexplored. we provide fedwq-cp, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. fedwq-cp performs agent-server calibration in a single communication round. on each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. each agent then transmits only its quantile threshold and calibration sample size to the server. the server simply aggregates these thresholds through a weighted average to produce a global threshold. experimental results on seven public datasets for both classification and regression demonstrate that fedwq-cp empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23296v1" target="_blank" rel="noopener">Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Quang-Huy Nguyen, Jiaqi Wang, Wei-Shinn Ku</p>
                <p class="paper-abstract" id="abstract-18">Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local...</p>
                <button class="expand-btn" onclick="toggleAbstract(18, 'Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classification and regression demonstrate that FedWQ-CP empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.', 'Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local...')" id="expand-btn-18">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23296v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23296v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="manifoldgd: training-free hierarchical manifold guidance for diffusion-based dataset distillation ayush roy wei-yang alex lee rudrasis chakraborty vishnu suresh lokhande in recent times, large datasets hinder efficient model training while also containing redundant concepts. dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (ipc centroids), which often are rudimentary and suboptimal. we propose manifold-guided distillation (manifoldgd), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. our method employs ipcs computed via a hierarchical, divisive clustering of vae latent features, yielding a multi-scale coreset of ipcs that captures both coarse semantic modes and fine intra-class variability. using a local neighborhood of the extracted ipc centroids, we create the latent manifold for each diffusion denoising timestep. at each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. this formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of fid, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing manifoldgd as the first geometry-aware training-free data distillation framework.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.23295v1" target="_blank" rel="noopener">ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-26</span>
                    <span class="paper-category">cs.CV</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Ayush Roy, Wei-Yang Alex Lee, Rudrasis Chakraborty, Vishnu Suresh Lokhande</p>
                <p class="paper-abstract" id="abstract-19">In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of...</p>
                <button class="expand-btn" onclick="toggleAbstract(19, 'In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework.', 'In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of...')" id="expand-btn-19">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.23295v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.23295v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

        </div>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2026 Coding Blog | BST 236 Computing I | Harvard University</p>
            <p class="footer-links">
                <a href="https://github.com">GitHub</a>
            </p>
        </div>
    </footer>

    <script src="js/main.js"></script>
    <script>
        function filterPapers() {
            const query = document.getElementById('searchInput').value.toLowerCase();
            const cards = document.querySelectorAll('.paper-card');
            
            cards.forEach(card => {
                const searchText = card.getAttribute('data-search');
                if (searchText.includes(query)) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });
        }
        
        function toggleAbstract(index, fullText, shortText) {
            const abstractEl = document.getElementById('abstract-' + index);
            const btnEl = document.getElementById('expand-btn-' + index);
            
            if (abstractEl.classList.contains('expanded')) {
                abstractEl.textContent = shortText;
                abstractEl.classList.remove('expanded');
                btnEl.textContent = 'Show more â–¼';
            } else {
                abstractEl.textContent = fullText;
                abstractEl.classList.add('expanded');
                btnEl.textContent = 'Show less â–²';
            }
        }
        
        // Client-side arXiv fetching for custom keywords
        // CORS proxies to try in order
        const corsProxies = [
            url => `https://corsproxy.io/?${encodeURIComponent(url)}`,
            url => `https://api.codetabs.com/v1/proxy?quest=${encodeURIComponent(url)}`,
            url => `https://api.allorigins.win/raw?url=${encodeURIComponent(url)}`
        ];
        
        async function tryFetchWithProxies(url, proxies) {
            for (let i = 0; i < proxies.length; i++) {
                const proxyUrl = proxies[i](url);
                try {
                    const response = await fetch(proxyUrl, { timeout: 10000 });
                    if (response.ok) {
                        return await response.text();
                    }
                } catch (e) {
                    console.log(`Proxy ${i + 1} failed:`, e.message);
                }
            }
            throw new Error('All CORS proxies failed. Please try again later or edit scripts/config.json directly.');
        }
        
        async function fetchCustomPapers() {
            const input = document.getElementById('customKeywords').value.trim();
            const btn = document.getElementById('fetchBtn');
            const grid = document.getElementById('papersGrid');
            const keywordsDisplay = document.getElementById('currentKeywords');
            
            if (!input) {
                alert('Please enter at least one keyword');
                return;
            }
            
            const keywords = input.split(',').map(k => k.trim()).filter(k => k);
            
            btn.disabled = true;
            btn.textContent = 'Fetching...';
            
            // Update displayed keywords
            keywordsDisplay.innerHTML = keywords.map(kw => 
                `<span class="keyword-tag">${escapeHtml(kw)}</span>`
            ).join('');
            
            // Build arXiv query
            const searchQuery = keywords.map(kw => `all:"${kw}"`).join(' OR ');
            const url = `https://export.arxiv.org/api/query?search_query=${encodeURIComponent(searchQuery)}&start=0&max_results=20&sortBy=submittedDate&sortOrder=descending`;
            
            try {
                const xmlText = await tryFetchWithProxies(url, corsProxies);
                
                // Parse XML
                const parser = new DOMParser();
                const xmlDoc = parser.parseFromString(xmlText, 'text/xml');
                const entries = xmlDoc.querySelectorAll('entry');
                
                if (entries.length === 0) {
                    grid.innerHTML = `
                        <div class="no-results" style="grid-column: 1 / -1;">
                            <h3>No papers found</h3>
                            <p>Try different keywords.</p>
                        </div>
                    `;
                } else {
                    let html = '';
                    entries.forEach((entry, i) => {
                        const title = entry.querySelector('title')?.textContent?.replace(/\s+/g, ' ').trim() || 'Untitled';
                        const abstract = entry.querySelector('summary')?.textContent?.replace(/\s+/g, ' ').trim() || 'No abstract';
                        const published = entry.querySelector('published')?.textContent?.substring(0, 10) || 'Unknown';
                        const id = entry.querySelector('id')?.textContent || '';
                        const pdfUrl = id.replace('/abs/', '/pdf/') + '.pdf';
                        
                        const authors = [];
                        entry.querySelectorAll('author name').forEach(n => authors.push(n.textContent));
                        const authorsStr = authors.length > 5 
                            ? authors.slice(0, 5).join(', ') + ` <em>(+${authors.length - 5} more)</em>`
                            : authors.join(', ');
                        
                        const categories = [];
                        entry.querySelectorAll('category').forEach(c => {
                            const term = c.getAttribute('term');
                            if (term && categories.length < 3) categories.push(term);
                        });
                        
                        const abstractShort = abstract.length > 200 
                            ? abstract.substring(0, 200).replace(/\s+\S*$/, '') + '...'
                            : abstract;
                        
                        html += `
                            <article class="paper-card" data-search="${escapeHtml(title.toLowerCase())} ${escapeHtml(authors.join(' ').toLowerCase())} ${escapeHtml(abstract.toLowerCase())}">
                                <h2 class="paper-title">
                                    <a href="${escapeHtml(id)}" target="_blank" rel="noopener">${escapeHtml(title)}</a>
                                </h2>
                                <div class="paper-meta">
                                    <span class="paper-date">ğŸ“… ${escapeHtml(published)}</span>
                                    ${categories.map(c => `<span class="paper-category">${escapeHtml(c)}</span>`).join('')}
                                </div>
                                <p class="paper-authors">${authorsStr}</p>
                                <p class="paper-abstract" id="abstract-dyn-${i}">${escapeHtml(abstractShort)}</p>
                                <button class="expand-btn" onclick="toggleAbstract('dyn-${i}', '${escapeHtml(abstract).replace(/'/g, "\\'")}', '${escapeHtml(abstractShort).replace(/'/g, "\\'")}')">Show more â–¼</button>
                                <div class="paper-actions">
                                    <a href="${escapeHtml(pdfUrl)}" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                                    <a href="${escapeHtml(id)}" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                                </div>
                            </article>
                        `;
                    });
                    grid.innerHTML = html;
                }
                
                // Update timestamp
                const now = new Date().toISOString().replace('T', ' ').substring(0, 19) + ' (live fetch)';
                document.querySelector('.update-info').textContent = 'Last updated: ' + now;
                
            } catch (error) {
                console.error('Fetch error:', error);
                grid.innerHTML = `
                    <div class="no-results" style="grid-column: 1 / -1;">
                        <h3>Error fetching papers</h3>
                        <p>${escapeHtml(error.message)}</p>
                        <p style="margin-top: 15px; font-size: 0.9rem;">
                            <strong>Alternative:</strong> Edit <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">scripts/config.json</code> 
                            and run <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">python scripts/fetch_arxiv.py</code> locally, 
                            or push to GitHub to trigger the auto-update.
                        </p>
                    </div>
                `;
            }
            
            btn.disabled = false;
            btn.textContent = 'Fetch Papers';
        }
        
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }
        
        // Allow Enter key to trigger fetch
        document.getElementById('customKeywords').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') fetchCustomPapers();
        });
    </script>
</body>
</html>
