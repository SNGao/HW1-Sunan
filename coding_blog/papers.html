<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Feed | Coding Blog</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <style>
        .papers-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 100px 20px 40px;
        }
        .papers-header {
            text-align: center;
            margin-bottom: 40px;
        }
        .papers-header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .update-info {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 8px;
        }
        .auto-update-info {
            color: #22c55e;
            font-size: 0.85rem;
            margin-bottom: 15px;
        }
        .config-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-bottom: 10px;
        }
        .config-hint code {
            background: #334155;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: var(--font-code);
            color: #f472b6;
        }
        .search-box {
            max-width: 500px;
            margin: 20px auto;
        }
        .search-box input {
            width: 100%;
            padding: 12px 20px;
            border: 2px solid var(--border);
            border-radius: 25px;
            background: var(--card-bg);
            color: var(--text-primary);
            font-size: 1rem;
            outline: none;
            transition: border-color 0.3s;
        }
        .search-box input:focus {
            border-color: var(--primary);
        }
        .search-box input::placeholder {
            color: var(--text-muted);
        }
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 25px;
        }
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 25px;
            transition: all 0.3s ease;
        }
        .paper-card:hover {
            border-color: var(--primary);
            transform: translateY(-3px);
            box-shadow: 0 10px 30px rgba(236, 72, 153, 0.15);
        }
        .paper-card.hidden {
            display: none;
        }
        .paper-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-bottom: 10px;
            line-height: 1.4;
        }
        .paper-title a {
            color: inherit;
            text-decoration: none;
            transition: color 0.3s;
        }
        .paper-title a:hover {
            color: var(--primary);
        }
        .paper-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 12px;
        }
        .paper-date {
            background: var(--primary);
            color: white;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
            font-weight: 500;
        }
        .paper-category {
            background: #334155;
            color: #e2e8f0;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
        }
        .paper-authors {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 12px;
            line-height: 1.5;
        }
        .paper-abstract {
            color: #94a3b8;
            font-size: 0.9rem;
            line-height: 1.7;
            margin-bottom: 15px;
        }
        .paper-abstract.expanded {
            max-height: none;
        }
        .expand-btn {
            background: none;
            border: none;
            color: var(--primary);
            cursor: pointer;
            font-size: 0.85rem;
            padding: 0;
            margin-bottom: 15px;
        }
        .expand-btn:hover {
            text-decoration: underline;
        }
        .paper-actions {
            display: flex;
            gap: 10px;
        }
        .pdf-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            transition: all 0.3s;
        }
        .pdf-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 15px rgba(236, 72, 153, 0.3);
        }
        .arxiv-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: var(--surface);
            color: var(--text-primary);
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            border: 1px solid var(--border);
            transition: all 0.3s;
        }
        .arxiv-btn:hover {
            border-color: var(--primary);
            color: var(--primary);
        }
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: var(--text-muted);
        }
        .no-results h3 {
            font-size: 1.5rem;
            margin-bottom: 10px;
            color: var(--text-secondary);
        }
        .keywords-info {
            background: #1e293b;
            padding: 20px;
            border-radius: 12px;
            margin-bottom: 30px;
            text-align: center;
            border: 1px solid #334155;
        }
        .keywords-info > span {
            color: #e2e8f0;
            font-size: 0.95rem;
            font-weight: 500;
        }
        .keyword-tag {
            display: inline-block;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: #ffffff;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin: 4px;
            text-shadow: 0 1px 2px rgba(0,0,0,0.2);
        }
        .keyword-editor {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #334155;
        }
        .keyword-input-group {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .keyword-input {
            padding: 10px 16px;
            border: 2px solid #334155;
            border-radius: 25px;
            background: #0f172a;
            color: #f8fafc;
            font-size: 0.9rem;
            width: 250px;
            outline: none;
            transition: border-color 0.3s;
        }
        .keyword-input:focus {
            border-color: #6366f1;
        }
        .keyword-input::placeholder {
            color: #64748b;
        }
        .fetch-btn {
            padding: 10px 24px;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: white;
            border: none;
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .fetch-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 20px rgba(99, 102, 241, 0.4);
        }
        .fetch-btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }
        .editor-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-top: 8px;
        }
        @media (max-width: 768px) {
            .papers-grid {
                grid-template-columns: 1fr;
            }
            .papers-header h1 {
                font-size: 1.8rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="index.html" class="nav-logo">
                <span class="logo-icon">ğŸ’»</span>
                <span class="logo-text">Coding Blog</span>
            </a>
            <ul class="nav-menu">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li><a href="index.html#projects" class="nav-link">Projects</a></li>
                <li><a href="papers.html" class="nav-link active">Papers</a></li>
                <li><a href="index.html#about" class="nav-link">About</a></li>
            </ul>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <div class="papers-container">
        <div class="papers-header">
            <h1>ğŸ“š arXiv Paper Feed</h1>
            <p class="update-info">Last updated: 2026-02-20 01:42:53 UTC</p>
            <p class="auto-update-info">â° Auto-updates daily at midnight UTC via GitHub Actions</p>
            <div class="keywords-info">
                <span>Current keywords: </span>
                <div id="currentKeywords">
                    <span class="keyword-tag">large language model</span><span class="keyword-tag">machine learning</span><span class="keyword-tag">biostatistics</span><span class="keyword-tag">deep learning</span>
                </div>
                <div class="keyword-editor">
                    <p class="editor-hint">ğŸ”„ Try different keywords (fetches live from arXiv):</p>
                    <div class="keyword-input-group">
                        <input type="text" id="customKeywords" class="keyword-input" 
                               placeholder="e.g., reinforcement learning, NLP" 
                               value="large language model, machine learning, biostatistics, deep learning">
                        <button onclick="fetchCustomPapers()" class="fetch-btn" id="fetchBtn">
                            Fetch Papers
                        </button>
                    </div>
                    <p class="editor-hint">Separate multiple keywords with commas</p>
                </div>
            </div>
            <div class="search-box">
                <input type="text" id="searchInput" placeholder="ğŸ” Filter papers by title, author, or abstract..." oninput="filterPapers()">
            </div>
        </div>

        <div class="papers-grid" id="papersGrid">

            <article class="paper-card" data-search="knowledge-embedded latent projection for robust representation learning weijing tang ming yuan zongqi xia tianxi cai latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (ehrs), by capturing complex dependence structures through low-dimensional embeddings. however, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. in ehr applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in ehrs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel hilbert space. we develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. we establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. furthermore, we provide local convergence guarantees for our non-convex optimization procedure. extensive simulation studies and a real-world ehr application demonstrate the effectiveness of the proposed method.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16709v1" target="_blank" rel="noopener">Knowledge-Embedded Latent Projection for Robust Representation Learning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">math.ST</span><span class="paper-category">stat.ME</span>
                </div>
                <p class="paper-authors">Weijing Tang, Ming Yuan, Zongqi Xia, Tianxi Cai</p>
                <p class="paper-abstract" id="abstract-0">Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence...</p>
                <button class="expand-btn" onclick="toggleAbstract(0, 'Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.', 'Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence...')" id="expand-btn-0">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16709v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16709v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="learning humanoid end-effector control for open-vocabulary visual loco-manipulation runpei dong ziyan li xialin he saurabh gupta visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (ee) control and a generalizable understanding of the scene via visual inputs (e.g., rgb-d images). existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. this paper presents a new paradigm, hero, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. we achieve this by designing an accurate residual-aware ee tracking policy. this ee tracking policy combines classical robotics with machine learning. it uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. together, these innovations help us cut down the end-effector tracking error by 3.2x. we use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. we believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16705v1" target="_blank" rel="noopener">Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.RO</span><span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta</p>
                <p class="paper-abstract" id="abstract-1">Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g.,...</p>
                <button class="expand-btn" onclick="toggleAbstract(1, 'Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.', 'Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g.,...')" id="expand-btn-1">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16705v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16705v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="measuring mid-2025 llm-assistance on novice performance in biology shen zhou hong alex kleinman alyssa mathiowetz adam howes julian cohen suveer ganta alex letizia dora liao deepika pahari xavier roberts-gaal luca righetti joe torres large language models (llms) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. yet, whether this translates to improved human performance in the physical laboratory remains unclear. to address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (june-august 2025; n = 153) evaluating whether llms improve novice performance in tasks that collectively model a viral reverse genetics workflow. we observed no significant difference in the primary endpoint of workflow completion (5.2% llm vs. 6.6% internet; p = 0.759), nor in the success rate of individual tasks. however, the llm arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% llm vs. 55.3% internet; p = 0.059). post-hoc bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% cri 0.74-2.62) in success for a &quot;typical&quot; reverse genetics task under llm assistance. ordinal regression modelling suggests that participants in the llm arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). overall, mid-2025 llms did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. these results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of ai biosecurity assessments as model capabilities and user proficiency evolve.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16703v1" target="_blank" rel="noopener">Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.CY</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Shen Zhou Hong, Alex Kleinman, Alyssa Mathiowetz, Adam Howes, Julian Cohen <em>(+7 more)</em></p>
                <p class="paper-abstract" id="abstract-2">Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved...</p>
                <button class="expand-btn" onclick="toggleAbstract(2, 'Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a &quot;typical&quot; reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.', 'Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved...')" id="expand-btn-2">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16703v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16703v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="saliency-aware multi-route thinking: revisiting vision-language reasoning mingjia shi yinhan he yaochen zhu jundong li vision-language models (vlms) aim to reason by jointly leveraging visual and textual modalities. while allocating additional inference-time computation has proven effective for large language models (llms), achieving similar scaling in vlms remains challenging. a key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. to address these challenges, we propose \emph{saliency-aware principle} (sap) selection. sap operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. in addition, sap supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. sap is model-agnostic and data-free, requiring no additional training. empirical results show that sap achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than cot-style long sequential reasoning.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16702v1" target="_blank" rel="noopener">Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Mingjia Shi, Yinhan He, Yaochen Zhu, Jundong Li</p>
                <p class="paper-abstract" id="abstract-3">Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models...</p>
                <button class="expand-btn" onclick="toggleAbstract(3, 'Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.', 'Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models...')" id="expand-btn-3">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16702v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16702v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="causality is key for interpretability claims to generalise shruti joshi aaron mueller david klindt wieland brendel patrik reizinger dhanya sridhar interpretability research on large language models (llms) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. specifically, pearl&#x27;s causal hierarchy clarifies what an interpretability study can justify. observations establish associations between model behaviour and internal components. interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. however, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. we show how causal representation learning (crl) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16698v1" target="_blank" rel="noopener">Causality is Key for Interpretability Claims to Generalise</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Shruti Joshi, Aaron Mueller, David Klindt, Wieland Brendel, Patrik Reizinger <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-4">Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal...</p>
                <button class="expand-btn" onclick="toggleAbstract(4, 'Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl&#x27;s causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.', 'Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal...')" id="expand-btn-4">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16698v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16698v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="protecting the undeleted in machine unlearning aloni cohen refael kohen kobbi nissim uri stemmer machine unlearning aims to remove specific data points from a trained model, often striving to emulate &quot;perfect retraining&quot;, i.e., producing the model that would have been obtained had the deleted data never been included. we demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. we present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $Ï‰(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. we survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. to address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. we show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16697v1" target="_blank" rel="noopener">Protecting the Undeleted in Machine Unlearning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.DS</span>
                </div>
                <p class="paper-authors">Aloni Cohen, Refael Kohen, Kobbi Nissim, Uri Stemmer</p>
                <p class="paper-abstract" id="abstract-5">Machine unlearning aims to remove specific data points from a trained model, often striving to emulate &quot;perfect retraining&quot;, i.e., producing the model that would have been obtained had the deleted...</p>
                <button class="expand-btn" onclick="toggleAbstract(5, 'Machine unlearning aims to remove specific data points from a trained model, often striving to emulate &quot;perfect retraining&quot;, i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $Ï‰(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.', 'Machine unlearning aims to remove specific data points from a trained model, often striving to emulate &quot;perfect retraining&quot;, i.e., producing the model that would have been obtained had the deleted...')" id="expand-btn-5">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16697v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16697v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="parameter-free representations outperform single-cell foundation models on downstream benchmarks huan souza pankaj mehta single-cell rna sequencing (scrna-seq) data exhibit strong and reproducible statistical structure. this has motivated the development of large-scale foundation models, such as transcriptformer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. these embeddings have been used to obtain state-of-the-art (sota) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain sota or near sota performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16696v1" target="_blank" rel="noopener">Parameter-free representations outperform single-cell foundation models on downstream benchmarks</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">q-bio.GN</span><span class="paper-category">cs.LG</span><span class="paper-category">q-bio.QM</span>
                </div>
                <p class="paper-authors">Huan Souza, Pankaj Mehta</p>
                <p class="paper-abstract" id="abstract-6">Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that...</p>
                <button class="expand-btn" onclick="toggleAbstract(6, 'Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.', 'Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that...')" id="expand-btn-6">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16696v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16696v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="synthetic-powered multiple testing with fdr control yonghoon lee meshi bashari edgar dobriban yaniv romano multiple hypothesis testing with false discovery rate (fdr) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. in many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. we introduce synthbh, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. we prove that synthbh guarantees finite-sample, distribution-free fdr control under a mild prds-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. the proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the fdr at a user-specified level regardless of their quality. we demonstrate the empirical performance of synthbh on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16690v1" target="_blank" rel="noopener">Synthetic-Powered Multiple Testing with FDR Control</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">stat.ME</span><span class="paper-category">cs.LG</span><span class="paper-category">stat.ML</span>
                </div>
                <p class="paper-authors">Yonghoon Lee, Meshi Bashari, Edgar Dobriban, Yaniv Romano</p>
                <p class="paper-abstract" id="abstract-7">Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In...</p>
                <button class="expand-btn" onclick="toggleAbstract(7, 'Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.', 'Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In...')" id="expand-btn-7">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16690v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16690v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="are object-centric representations better at compositional generalization? ferdinand kapl amir mohammad karimi mamaghan maximilian seitzer karl henrik johansson carsten marr stefan bauer andrea dittadi compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. object-centric (oc) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. we introduce a visual question answering benchmark across three controlled visual worlds (clevrtex, super-clevr, and movi-c) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. to ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. we use dinov2 and siglip2, two widely used vision encoders, as the foundation models and their oc counterparts. our key findings reveal that (1) oc approaches are superior in harder compositional generalization settings; (2) original dense representations surpass oc only on easier settings and typically require substantially more downstream compute; and (3) oc models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16689v1" target="_blank" rel="noopener">Are Object-Centric Representations Better At Compositional Generalization?</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.CV</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Ferdinand Kapl, Amir Mohammad Karimi Mamaghan, Maximilian Seitzer, Karl Henrik Johansson, Carsten Marr <em>(+2 more)</em></p>
                <p class="paper-abstract" id="abstract-8">Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC)...</p>
                <button class="expand-btn" onclick="toggleAbstract(8, 'Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.', 'Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC)...')" id="expand-btn-8">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16689v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16689v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="on the hardness of approximation of the fair k-center problem suhas thejaswi in this work, we study the hardness of approximation of the fair $k$-center problem. here the data points are partitioned into groups and the task is to choose a prescribed number of data points from each group, called centers, while minimizing the maximum distance from any point to its closest center. although a polynomial-time $3$-approximation is known for this problem in general metrics, it has remained open whether this approximation guarantee is tight or could be further improved, especially since the unconstrained $k$-center problem admits a polynomial-time factor-$2$ approximation. we resolve this open question by proving that, for every $Îµ&gt;0$, achieving a $(3-Îµ)$-approximation is np-hard, assuming $\text{p} \neq \text{np}$. our inapproximability results hold even when only two disjoint groups are present and at least one center must be chosen from each group. further, it extends to the canonical one-per-group setting with $k$-groups (for arbitrary $k$), where exactly one center must be selected from each group. consequently, the factor-$3$ barrier for fair $k$-center in general metric spaces is inherent, and existing $3$-approximation algorithms are optimal up to lower-order terms even in these restricted regimes. this result stands in sharp contrast to the $k$-supplier formulation, where both the unconstrained and fair variants admit factor-$3$ approximation in polynomial time.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16688v1" target="_blank" rel="noopener">On the Hardness of Approximation of the Fair k-Center Problem</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.CC</span><span class="paper-category">cs.DS</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Suhas Thejaswi</p>
                <p class="paper-abstract" id="abstract-9">In this work, we study the hardness of approximation of the fair $k$-center problem. Here the data points are partitioned into groups and the task is to choose a prescribed number of data points from...</p>
                <button class="expand-btn" onclick="toggleAbstract(9, 'In this work, we study the hardness of approximation of the fair $k$-center problem. Here the data points are partitioned into groups and the task is to choose a prescribed number of data points from each group, called centers, while minimizing the maximum distance from any point to its closest center. Although a polynomial-time $3$-approximation is known for this problem in general metrics, it has remained open whether this approximation guarantee is tight or could be further improved, especially since the unconstrained $k$-center problem admits a polynomial-time factor-$2$ approximation. We resolve this open question by proving that, for every $Îµ&gt;0$, achieving a $(3-Îµ)$-approximation is NP-hard, assuming $\text{P} \neq \text{NP}$. Our inapproximability results hold even when only two disjoint groups are present and at least one center must be chosen from each group. Further, it extends to the canonical one-per-group setting with $k$-groups (for arbitrary $k$), where exactly one center must be selected from each group. Consequently, the factor-$3$ barrier for fair $k$-center in general metric spaces is inherent, and existing $3$-approximation algorithms are optimal up to lower-order terms even in these restricted regimes. This result stands in sharp contrast to the $k$-supplier formulation, where both the unconstrained and fair variants admit factor-$3$ approximation in polynomial time.', 'In this work, we study the hardness of approximation of the fair $k$-center problem. Here the data points are partitioned into groups and the task is to choose a prescribed number of data points from...')" id="expand-btn-9">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16688v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16688v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="retrieval-augmented foundation models for matched molecular pair transformations to recapitulate medicinal chemistry intuition bo pan peter zhiping zhang hao-wei pang alex zhu xiang yu liying zhang liang zhao matched molecular pairs (mmps) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ml approaches either operate at the whole-molecule level with limited edit controllability or learn mmp-style edits from restricted settings and small models. we propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale mmp transformations (mmpts) to generate diverse variables conditioned on an input variable. to enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. we further introduce mmpt-rag, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16684v1" target="_blank" rel="noopener">Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Bo Pan, Peter Zhiping Zhang, Hao-Wei Pang, Alex Zhu, Xiang Yu <em>(+2 more)</em></p>
                <p class="paper-abstract" id="abstract-10">Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with...</p>
                <button class="expand-btn" onclick="toggleAbstract(10, 'Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.', 'Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with...')" id="expand-btn-10">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16684v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16684v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="neighborhood stability as a measure of nearest neighbor searchability thomas vecchiato sebastian bruch clustering-based approximate nearest neighbor search (anns) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. despite its popularity, there are virtually no analytical tools to determine the suitability of clustering-based anns for a given dataset -- what we call &quot;searchability.&quot; to address that gap, we present two measures for flat clusterings of high-dimensional points in euclidean space. first is clustering-neighborhood stability measure (clustering-nsm), an internal measure of clustering quality -- a function of a clustering of a dataset -- that we show to be predictive of anns accuracy. the second, point-neighborhood stability measure (point-nsm), is a measure of clusterability -- a function of the dataset itself -- that is predictive of clustering-nsm. the two together allow us to determine whether a dataset is searchable by clustering-based anns given only the data points. importantly, both are functions of nearest neighbor relationships between points, not distances, making them applicable to various distance functions including inner product.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16673v1" target="_blank" rel="noopener">Neighborhood Stability as a Measure of Nearest Neighbor Searchability</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.IR</span>
                </div>
                <p class="paper-authors">Thomas Vecchiato, Sebastian Bruch</p>
                <p class="paper-abstract" id="abstract-11">Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its...</p>
                <button class="expand-btn" onclick="toggleAbstract(11, 'Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its popularity, there are virtually no analytical tools to determine the suitability of clustering-based ANNS for a given dataset -- what we call &quot;searchability.&quot; To address that gap, we present two measures for flat clusterings of high-dimensional points in Euclidean space. First is Clustering-Neighborhood Stability Measure (clustering-NSM), an internal measure of clustering quality -- a function of a clustering of a dataset -- that we show to be predictive of ANNS accuracy. The second, Point-Neighborhood Stability Measure (point-NSM), is a measure of clusterability -- a function of the dataset itself -- that is predictive of clustering-NSM. The two together allow us to determine whether a dataset is searchable by clustering-based ANNS given only the data points. Importantly, both are functions of nearest neighbor relationships between points, not distances, making them applicable to various distance functions including inner product.', 'Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its...')" id="expand-btn-11">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16673v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16673v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="sparc: scenario planning and reasoning for automated c unit test generation jaid monwar chowdhury chi-an fu reyhaneh jabbarvand automated unit test generation for c remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. while large language models (llms) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. this will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. we introduce sparc, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) control flow graph (cfg) analysis, (2) an operation map that grounds llm reasoning in validated utility helpers, (3) path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. we evaluate sparc on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool klee on complex subjects. sparc retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. by aligning llm reasoning with program structure, sparc provides a scalable path for industrial-grade testing of legacy c codebases.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16671v1" target="_blank" rel="noopener">SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.SE</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Jaid Monwar Chowdhury, Chi-An Fu, Reyhaneh Jabbarvand</p>
                <p class="paper-abstract" id="abstract-12">Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual...</p>
                <button class="expand-btn" onclick="toggleAbstract(12, 'Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.', 'Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual...')" id="expand-btn-12">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16671v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16671v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="towards a science of ai agent reliability stephan rabanser sayash kapoor peter kirgis kangheng liu saiteja utpala arvind narayanan ai agents are increasingly deployed to execute important tasks. while rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. this discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. by exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16666v1" target="_blank" rel="noopener">Towards a Science of AI Agent Reliability</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.AI</span><span class="paper-category">cs.CY</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-13">AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This...</p>
                <button class="expand-btn" onclick="toggleAbstract(13, 'AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.', 'AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This...')" id="expand-btn-13">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16666v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16666v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="optimizing p-spin models through hypergraph neural networks and deep reinforcement learning li zeng mutian shen tianle pu zohar nussinov qing feng chao chen zhong liu changjun fan p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pairwise case (p&gt;2), are prototypical disordered systems whose ground-state search is np-hard and computationally prohibitive for large instances. solving this problem is not only fundamental for understanding high-order disorder, structural glasses, and topological phases, but also central to a wide spectrum of hard combinatorial optimization tasks. despite decades of progress, there still lacks an efficient and scalable solver for generic large-scale p-spin models. here we introduce planck, a physics-inspired deep reinforcement learning framework built on hypergraph neural networks. planck directly optimizes arbitrary high-order interactions, and systematically exploits gauge symmetry throughout both training and inference. trained exclusively on small synthetic instances, planck exhibits strong zero-shot generalization to systems orders of magnitude larger, and consistently outperforms state-of-the-art thermal annealing methods across all tested structural topologies and coupling distributions. moreover, without any modification, planck achieves near-optimal solutions for a broad class of np-hard combinatorial problems, including random k-xorsat, hypergraph max-cut, and conventional max-cut. the presented framework provides a physics-inspired algorithmic paradigm that bridges statistical mechanics and reinforcement learning. the symmetry-aware design not only advances the tractable frontiers of high-order disordered systems, but also opens a promising avenue for machine-learning-based solvers to tackle previously intractable combinatorial optimization challenges.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16665v1" target="_blank" rel="noopener">Optimizing p-spin models through hypergraph neural networks and deep reinforcement learning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cond-mat.dis-nn</span><span class="paper-category">physics.comp-ph</span>
                </div>
                <p class="paper-authors">Li Zeng, Mutian Shen, Tianle Pu, Zohar Nussinov, Qing Feng <em>(+3 more)</em></p>
                <p class="paper-abstract" id="abstract-14">p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pairwise case (p&gt;2), are prototypical disordered systems whose ground-state search is NP-hard and...</p>
                <button class="expand-btn" onclick="toggleAbstract(14, 'p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pairwise case (p&gt;2), are prototypical disordered systems whose ground-state search is NP-hard and computationally prohibitive for large instances. Solving this problem is not only fundamental for understanding high-order disorder, structural glasses, and topological phases, but also central to a wide spectrum of hard combinatorial optimization tasks. Despite decades of progress, there still lacks an efficient and scalable solver for generic large-scale p-spin models. Here we introduce PLANCK, a physics-inspired deep reinforcement learning framework built on hypergraph neural networks. PLANCK directly optimizes arbitrary high-order interactions, and systematically exploits gauge symmetry throughout both training and inference. Trained exclusively on small synthetic instances, PLANCK exhibits strong zero-shot generalization to systems orders of magnitude larger, and consistently outperforms state-of-the-art thermal annealing methods across all tested structural topologies and coupling distributions. Moreover, without any modification, PLANCK achieves near-optimal solutions for a broad class of NP-hard combinatorial problems, including random k-XORSAT, hypergraph max-cut, and conventional max-cut. The presented framework provides a physics-inspired algorithmic paradigm that bridges statistical mechanics and reinforcement learning. The symmetry-aware design not only advances the tractable frontiers of high-order disordered systems, but also opens a promising avenue for machine-learning-based solvers to tackle previously intractable combinatorial optimization challenges.', 'p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pairwise case (p&gt;2), are prototypical disordered systems whose ground-state search is NP-hard and...')" id="expand-btn-14">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16665v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16665v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="align once, benefit multilingually: enforcing multilingual consistency for llm safety alignment yuyan bu xiaohao liu zhaoxing ren yaodong yang juntao dai the widespread deployment of large language models (llms) across linguistic communities necessitates reliable multilingual safety alignment. however, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. in this work, we propose a resource-efficient method for improving multilingual safety alignment. we introduce a plug-and-play multi-lingual consistency (mlc) loss that can be integrated into existing monolingual alignment pipelines. by improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. this allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. we validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16660v1" target="_blank" rel="noopener">Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Yuyan Bu, Xiaohao Liu, ZhaoXing Ren, Yaodong Yang, Juntao Dai</p>
                <p class="paper-abstract" id="abstract-15">The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other...</p>
                <button class="expand-btn" onclick="toggleAbstract(15, 'The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.', 'The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other...')" id="expand-btn-15">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16660v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16660v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="investigating nonlinear quenching effects on polar field buildup in the sun using physics-informed neural networks jithu j. athalathil mohammed h. talafha bhargav vaidya the solar dynamo relies on the regeneration of the poloidal magnetic field through processes strongly modulated by nonlinear feedbacks such as tilt quenching (tq) and latitude quenching (lq). these mechanisms play a decisive role in regulating the buildup of the sun&#x27;s polar field and, in turn, the amplitude of future solar cycles. in this work, we employ physics-informed neural networks (pinn) to solve the surface flux transport (sft) equation, embedding physical constraints directly into the neural network framework. by systematically varying transport parameters, we isolate the relative contributions of tq and lq to polar dipole buildup. we use the residual dipole moment as a diagnostic for cycle-to-cycle amplification and show that tq suppression strengthens with increasing diffusivity, while lq dominates in advection-dominated regimes. the ratio $Î´d_{\mathrm{lq}}/Î´d_{\mathrm{tq}}$ exhibits a smooth inverse-square dependence on the dynamo effectivity range, refining previous empirical fits with improved accuracy and reduced scatter. the results further reveal that the need for a decay term is not essential for pinn set-up due to the training process. compared with the traditional 1d sft model, the pinn framework achieves significantly lower error metrics and more robust recovery of nonlinear trends. our results suggest that the nonlinear interplay between lq and tq can naturally produce alternations between weak and strong cycles, providing a physical explanation for the observed even-odd cycle modulation. these findings demonstrate the potential of pinn as an accurate, efficient, and physically consistent tool for solar cycle prediction.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16656v1" target="_blank" rel="noopener">Investigating Nonlinear Quenching Effects on Polar Field Buildup in the Sun Using Physics-Informed Neural Networks</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">astro-ph.SR</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Jithu J. Athalathil, Mohammed H. Talafha, Bhargav Vaidya</p>
                <p class="paper-abstract" id="abstract-16">The solar dynamo relies on the regeneration of the poloidal magnetic field through processes strongly modulated by nonlinear feedbacks such as tilt quenching (TQ) and latitude quenching (LQ). These...</p>
                <button class="expand-btn" onclick="toggleAbstract(16, 'The solar dynamo relies on the regeneration of the poloidal magnetic field through processes strongly modulated by nonlinear feedbacks such as tilt quenching (TQ) and latitude quenching (LQ). These mechanisms play a decisive role in regulating the buildup of the Sun&#x27;s polar field and, in turn, the amplitude of future solar cycles. In this work, we employ Physics-Informed Neural Networks (PINN) to solve the surface flux transport (SFT) equation, embedding physical constraints directly into the neural network framework. By systematically varying transport parameters, we isolate the relative contributions of TQ and LQ to polar dipole buildup. We use the residual dipole moment as a diagnostic for cycle-to-cycle amplification and show that TQ suppression strengthens with increasing diffusivity, while LQ dominates in advection-dominated regimes. The ratio $Î”D_{\mathrm{LQ}}/Î”D_{\mathrm{TQ}}$ exhibits a smooth inverse-square dependence on the dynamo effectivity range, refining previous empirical fits with improved accuracy and reduced scatter. The results further reveal that the need for a decay term is not essential for PINN set-up due to the training process. Compared with the traditional 1D SFT model, the PINN framework achieves significantly lower error metrics and more robust recovery of nonlinear trends. Our results suggest that the nonlinear interplay between LQ and TQ can naturally produce alternations between weak and strong cycles, providing a physical explanation for the observed even-odd cycle modulation. These findings demonstrate the potential of PINN as an accurate, efficient, and physically consistent tool for solar cycle prediction.', 'The solar dynamo relies on the regeneration of the poloidal magnetic field through processes strongly modulated by nonlinear feedbacks such as tilt quenching (TQ) and latitude quenching (LQ). These...')" id="expand-btn-16">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16656v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16656v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="retrieval augmented generation of literature-derived polymer knowledge: the example of a biodegradable polymer expert system sonakshi gupta akhlak mahmood wei xiong rampi ramprasad polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. retrieval-augmented generation (rag) offers a promising way to overcome this limitation by combining large language models (llms) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. in this work, we develop two retrieval pipelines: a dense semantic vector-based approach (vectorrag) and a graph-based approach (graphrag). using over 1,000 polyhydroxyalkanoate (pha) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. we evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as gpt and gemini, and qualitative validation by a domain chemist. the results show that graphrag achieves higher precision and interpretability, while vectorrag provides broader recall, highlighting complementary trade-offs. expert validation further confirms that the tailored pipelines, particularly graphrag, produce well-grounded, citation-reliable responses with strong domain relevance. by grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. more broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16650v1" target="_blank" rel="noopener">Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.CE</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Sonakshi Gupta, Akhlak Mahmood, Wei Xiong, Rampi Ramprasad</p>
                <p class="paper-abstract" id="abstract-17">Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning...</p>
                <button class="expand-btn" onclick="toggleAbstract(17, 'Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale.', 'Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning...')" id="expand-btn-17">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16650v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16650v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="factorization machine with quadratic-optimization annealing for rna inverse folding and evaluation of binary-integer encoding and nucleotide assignment shuta kikuchi shu tanaka the rna inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. while various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. we propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (fmqa). fmqa is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. applying fmqa to the problem requires converting nucleotides into binary variables. however, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of fmqa has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. therefore, this study aims both to establish a novel fmqa framework for rna inverse folding and to analyze the effects of these assignments and encoding methods. we evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. in domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. in the rna inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16643v1" target="_blank" rel="noopener">Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cond-mat.stat-mech</span>
                </div>
                <p class="paper-authors">Shuta Kikuchi, Shu Tanaka</p>
                <p class="paper-abstract" id="abstract-18">The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have...</p>
                <button class="expand-btn" onclick="toggleAbstract(18, 'The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.', 'The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have...')" id="expand-btn-18">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16643v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16643v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="optimizer choice matters for the emergence of neural collapse jim zhao tin sum cheng wojciech masarczyk aurelien lucchi neural collapse (nc) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. despite its prevalence, the theoretical understanding of nc remains limited. existing analyses largely ignore the role of the optimizer, thereby suggesting that nc is universal across optimization methods. in this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of nc. the phenomenon is typically quantified through nc metrics, which, however, are difficult to track and analyze theoretically. to overcome this limitation, we introduce a novel diagnostic metric, nc0, whose convergence to zero is a necessary condition for nc. using nc0, we provide theoretical evidence that nc cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in adamw. concretely, we prove that sgd, signgd with coupled weight decay (a special case of adam), and signgd with decoupled weight decay (a special case of adamw) exhibit qualitatively different nc0 dynamics. also, we show the accelerating effect of momentum on nc (beyond convergence of train loss) when trained with sgd, being the first result concerning momentum in the context of nc. finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. this work provides the first theoretical explanation for optimizer-dependent emergence of nc and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.16642v1" target="_blank" rel="noopener">Optimizer choice matters for the emergence of Neural Collapse</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-18</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Jim Zhao, Tin Sum Cheng, Wojciech Masarczyk, Aurelien Lucchi</p>
                <p class="paper-abstract" id="abstract-19">Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the...</p>
                <button class="expand-btn" onclick="toggleAbstract(19, 'Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.', 'Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the...')" id="expand-btn-19">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.16642v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.16642v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

        </div>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2026 Coding Blog | BST 236 Computing I | Harvard University</p>
            <p class="footer-links">
                <a href="https://github.com">GitHub</a>
            </p>
        </div>
    </footer>

    <script src="js/main.js"></script>
    <script>
        function filterPapers() {
            const query = document.getElementById('searchInput').value.toLowerCase();
            const cards = document.querySelectorAll('.paper-card');
            
            cards.forEach(card => {
                const searchText = card.getAttribute('data-search');
                if (searchText.includes(query)) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });
        }
        
        function toggleAbstract(index, fullText, shortText) {
            const abstractEl = document.getElementById('abstract-' + index);
            const btnEl = document.getElementById('expand-btn-' + index);
            
            if (abstractEl.classList.contains('expanded')) {
                abstractEl.textContent = shortText;
                abstractEl.classList.remove('expanded');
                btnEl.textContent = 'Show more â–¼';
            } else {
                abstractEl.textContent = fullText;
                abstractEl.classList.add('expanded');
                btnEl.textContent = 'Show less â–²';
            }
        }
        
        // Client-side arXiv fetching for custom keywords
        // CORS proxies to try in order
        const corsProxies = [
            url => `https://corsproxy.io/?${encodeURIComponent(url)}`,
            url => `https://api.codetabs.com/v1/proxy?quest=${encodeURIComponent(url)}`,
            url => `https://api.allorigins.win/raw?url=${encodeURIComponent(url)}`
        ];
        
        async function tryFetchWithProxies(url, proxies) {
            for (let i = 0; i < proxies.length; i++) {
                const proxyUrl = proxies[i](url);
                try {
                    const response = await fetch(proxyUrl, { timeout: 10000 });
                    if (response.ok) {
                        return await response.text();
                    }
                } catch (e) {
                    console.log(`Proxy ${i + 1} failed:`, e.message);
                }
            }
            throw new Error('All CORS proxies failed. Please try again later or edit scripts/config.json directly.');
        }
        
        async function fetchCustomPapers() {
            const input = document.getElementById('customKeywords').value.trim();
            const btn = document.getElementById('fetchBtn');
            const grid = document.getElementById('papersGrid');
            const keywordsDisplay = document.getElementById('currentKeywords');
            
            if (!input) {
                alert('Please enter at least one keyword');
                return;
            }
            
            const keywords = input.split(',').map(k => k.trim()).filter(k => k);
            
            btn.disabled = true;
            btn.textContent = 'Fetching...';
            
            // Update displayed keywords
            keywordsDisplay.innerHTML = keywords.map(kw => 
                `<span class="keyword-tag">${escapeHtml(kw)}</span>`
            ).join('');
            
            // Build arXiv query
            const searchQuery = keywords.map(kw => `all:"${kw}"`).join(' OR ');
            const url = `https://export.arxiv.org/api/query?search_query=${encodeURIComponent(searchQuery)}&start=0&max_results=20&sortBy=submittedDate&sortOrder=descending`;
            
            try {
                const xmlText = await tryFetchWithProxies(url, corsProxies);
                
                // Parse XML
                const parser = new DOMParser();
                const xmlDoc = parser.parseFromString(xmlText, 'text/xml');
                const entries = xmlDoc.querySelectorAll('entry');
                
                if (entries.length === 0) {
                    grid.innerHTML = `
                        <div class="no-results" style="grid-column: 1 / -1;">
                            <h3>No papers found</h3>
                            <p>Try different keywords.</p>
                        </div>
                    `;
                } else {
                    let html = '';
                    entries.forEach((entry, i) => {
                        const title = entry.querySelector('title')?.textContent?.replace(/\s+/g, ' ').trim() || 'Untitled';
                        const abstract = entry.querySelector('summary')?.textContent?.replace(/\s+/g, ' ').trim() || 'No abstract';
                        const published = entry.querySelector('published')?.textContent?.substring(0, 10) || 'Unknown';
                        const id = entry.querySelector('id')?.textContent || '';
                        const pdfUrl = id.replace('/abs/', '/pdf/') + '.pdf';
                        
                        const authors = [];
                        entry.querySelectorAll('author name').forEach(n => authors.push(n.textContent));
                        const authorsStr = authors.length > 5 
                            ? authors.slice(0, 5).join(', ') + ` <em>(+${authors.length - 5} more)</em>`
                            : authors.join(', ');
                        
                        const categories = [];
                        entry.querySelectorAll('category').forEach(c => {
                            const term = c.getAttribute('term');
                            if (term && categories.length < 3) categories.push(term);
                        });
                        
                        const abstractShort = abstract.length > 200 
                            ? abstract.substring(0, 200).replace(/\s+\S*$/, '') + '...'
                            : abstract;
                        
                        html += `
                            <article class="paper-card" data-search="${escapeHtml(title.toLowerCase())} ${escapeHtml(authors.join(' ').toLowerCase())} ${escapeHtml(abstract.toLowerCase())}">
                                <h2 class="paper-title">
                                    <a href="${escapeHtml(id)}" target="_blank" rel="noopener">${escapeHtml(title)}</a>
                                </h2>
                                <div class="paper-meta">
                                    <span class="paper-date">ğŸ“… ${escapeHtml(published)}</span>
                                    ${categories.map(c => `<span class="paper-category">${escapeHtml(c)}</span>`).join('')}
                                </div>
                                <p class="paper-authors">${authorsStr}</p>
                                <p class="paper-abstract" id="abstract-dyn-${i}">${escapeHtml(abstractShort)}</p>
                                <button class="expand-btn" onclick="toggleAbstract('dyn-${i}', '${escapeHtml(abstract).replace(/'/g, "\\'")}', '${escapeHtml(abstractShort).replace(/'/g, "\\'")}')">Show more â–¼</button>
                                <div class="paper-actions">
                                    <a href="${escapeHtml(pdfUrl)}" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                                    <a href="${escapeHtml(id)}" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                                </div>
                            </article>
                        `;
                    });
                    grid.innerHTML = html;
                }
                
                // Update timestamp
                const now = new Date().toISOString().replace('T', ' ').substring(0, 19) + ' (live fetch)';
                document.querySelector('.update-info').textContent = 'Last updated: ' + now;
                
            } catch (error) {
                console.error('Fetch error:', error);
                grid.innerHTML = `
                    <div class="no-results" style="grid-column: 1 / -1;">
                        <h3>Error fetching papers</h3>
                        <p>${escapeHtml(error.message)}</p>
                        <p style="margin-top: 15px; font-size: 0.9rem;">
                            <strong>Alternative:</strong> Edit <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">scripts/config.json</code> 
                            and run <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">python scripts/fetch_arxiv.py</code> locally, 
                            or push to GitHub to trigger the auto-update.
                        </p>
                    </div>
                `;
            }
            
            btn.disabled = false;
            btn.textContent = 'Fetch Papers';
        }
        
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }
        
        // Allow Enter key to trigger fetch
        document.getElementById('customKeywords').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') fetchCustomPapers();
        });
    </script>
</body>
</html>
