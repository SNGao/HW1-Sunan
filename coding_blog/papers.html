<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Feed | Coding Blog</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <style>
        .papers-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 100px 20px 40px;
        }
        .papers-header {
            text-align: center;
            margin-bottom: 40px;
        }
        .papers-header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .update-info {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 8px;
        }
        .auto-update-info {
            color: #22c55e;
            font-size: 0.85rem;
            margin-bottom: 15px;
        }
        .config-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-bottom: 10px;
        }
        .config-hint code {
            background: #334155;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: var(--font-code);
            color: #f472b6;
        }
        .search-box {
            max-width: 500px;
            margin: 20px auto;
        }
        .search-box input {
            width: 100%;
            padding: 12px 20px;
            border: 2px solid var(--border);
            border-radius: 25px;
            background: var(--card-bg);
            color: var(--text-primary);
            font-size: 1rem;
            outline: none;
            transition: border-color 0.3s;
        }
        .search-box input:focus {
            border-color: var(--primary);
        }
        .search-box input::placeholder {
            color: var(--text-muted);
        }
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 25px;
        }
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 25px;
            transition: all 0.3s ease;
        }
        .paper-card:hover {
            border-color: var(--primary);
            transform: translateY(-3px);
            box-shadow: 0 10px 30px rgba(236, 72, 153, 0.15);
        }
        .paper-card.hidden {
            display: none;
        }
        .paper-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-bottom: 10px;
            line-height: 1.4;
        }
        .paper-title a {
            color: inherit;
            text-decoration: none;
            transition: color 0.3s;
        }
        .paper-title a:hover {
            color: var(--primary);
        }
        .paper-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 12px;
        }
        .paper-date {
            background: var(--primary);
            color: white;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
            font-weight: 500;
        }
        .paper-category {
            background: #334155;
            color: #e2e8f0;
            padding: 4px 10px;
            border-radius: 15px;
            font-size: 0.75rem;
        }
        .paper-authors {
            color: #cbd5e1;
            font-size: 0.9rem;
            margin-bottom: 12px;
            line-height: 1.5;
        }
        .paper-abstract {
            color: #94a3b8;
            font-size: 0.9rem;
            line-height: 1.7;
            margin-bottom: 15px;
        }
        .paper-abstract.expanded {
            max-height: none;
        }
        .expand-btn {
            background: none;
            border: none;
            color: var(--primary);
            cursor: pointer;
            font-size: 0.85rem;
            padding: 0;
            margin-bottom: 15px;
        }
        .expand-btn:hover {
            text-decoration: underline;
        }
        .paper-actions {
            display: flex;
            gap: 10px;
        }
        .pdf-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            transition: all 0.3s;
        }
        .pdf-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 15px rgba(236, 72, 153, 0.3);
        }
        .arxiv-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            background: var(--surface);
            color: var(--text-primary);
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            border: 1px solid var(--border);
            transition: all 0.3s;
        }
        .arxiv-btn:hover {
            border-color: var(--primary);
            color: var(--primary);
        }
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: var(--text-muted);
        }
        .no-results h3 {
            font-size: 1.5rem;
            margin-bottom: 10px;
            color: var(--text-secondary);
        }
        .keywords-info {
            background: #1e293b;
            padding: 20px;
            border-radius: 12px;
            margin-bottom: 30px;
            text-align: center;
            border: 1px solid #334155;
        }
        .keywords-info > span {
            color: #e2e8f0;
            font-size: 0.95rem;
            font-weight: 500;
        }
        .keyword-tag {
            display: inline-block;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: #ffffff;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin: 4px;
            text-shadow: 0 1px 2px rgba(0,0,0,0.2);
        }
        .keyword-editor {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #334155;
        }
        .keyword-input-group {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .keyword-input {
            padding: 10px 16px;
            border: 2px solid #334155;
            border-radius: 25px;
            background: #0f172a;
            color: #f8fafc;
            font-size: 0.9rem;
            width: 250px;
            outline: none;
            transition: border-color 0.3s;
        }
        .keyword-input:focus {
            border-color: #6366f1;
        }
        .keyword-input::placeholder {
            color: #64748b;
        }
        .fetch-btn {
            padding: 10px 24px;
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: white;
            border: none;
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .fetch-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 20px rgba(99, 102, 241, 0.4);
        }
        .fetch-btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }
        .editor-hint {
            color: #94a3b8;
            font-size: 0.8rem;
            margin-top: 8px;
        }
        @media (max-width: 768px) {
            .papers-grid {
                grid-template-columns: 1fr;
            }
            .papers-header h1 {
                font-size: 1.8rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="index.html" class="nav-logo">
                <span class="logo-icon">ğŸ’»</span>
                <span class="logo-text">Coding Blog</span>
            </a>
            <ul class="nav-menu">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li><a href="index.html#projects" class="nav-link">Projects</a></li>
                <li><a href="papers.html" class="nav-link active">Papers</a></li>
                <li><a href="index.html#about" class="nav-link">About</a></li>
            </ul>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <div class="papers-container">
        <div class="papers-header">
            <h1>ğŸ“š arXiv Paper Feed</h1>
            <p class="update-info">Last updated: 2026-02-18 01:47:51 UTC</p>
            <p class="auto-update-info">â° Auto-updates daily at midnight UTC via GitHub Actions</p>
            <div class="keywords-info">
                <span>Current keywords: </span>
                <div id="currentKeywords">
                    <span class="keyword-tag">large language model</span><span class="keyword-tag">machine learning</span><span class="keyword-tag">biostatistics</span><span class="keyword-tag">deep learning</span>
                </div>
                <div class="keyword-editor">
                    <p class="editor-hint">ğŸ”„ Try different keywords (fetches live from arXiv):</p>
                    <div class="keyword-input-group">
                        <input type="text" id="customKeywords" class="keyword-input" 
                               placeholder="e.g., reinforcement learning, NLP" 
                               value="large language model, machine learning, biostatistics, deep learning">
                        <button onclick="fetchCustomPapers()" class="fetch-btn" id="fetchBtn">
                            Fetch Papers
                        </button>
                    </div>
                    <p class="editor-hint">Separate multiple keywords with commas</p>
                </div>
            </div>
            <div class="search-box">
                <input type="text" id="searchInput" placeholder="ğŸ” Filter papers by title, author, or abstract..." oninput="filterPapers()">
            </div>
        </div>

        <div class="papers-grid" id="papersGrid">

            <article class="paper-card" data-search="symmetry in language statistics shapes the geometry of model representations dhruva karkada daniel j. korchinski andres nava matthieu wyart yasaman bahri although learned representations underlie neural networks&#x27; success, their fundamental properties remain poorly understood. a striking example is the emergence of simple geometric structures in llm representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities&#x27; latitudes and longitudes can be decoded by a linear probe. we show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. we show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. we empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15029v1" target="_blank" rel="noopener">Symmetry in language statistics shapes the geometry of model representations</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cond-mat.dis-nn</span><span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Dhruva Karkada, Daniel J. Korchinski, Andres Nava, Matthieu Wyart, Yasaman Bahri</p>
                <p class="paper-abstract" id="abstract-0">Although learned representations underlie neural networks&#x27; success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM...</p>
                <button class="expand-btn" onclick="toggleAbstract(0, 'Although learned representations underlie neural networks&#x27; success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities&#x27; latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.', 'Although learned representations underlie neural networks&#x27; success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM...')" id="expand-btn-0">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15029v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15029v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="long context, less focus: a scaling gap in llms revealed through privacy and personalization shangding gu large language models (llms) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. we introduce a large-scale benchmark, paperbench, to systematically study how increasing context length influences both personalization quality and privacy protection in llms. the benchmark comprises approximately 29,000 instances with context lengths ranging from 1k to 256k tokens, yielding a total of 377k evaluation questions. it jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. extensive evaluations across state-of-the-art llms reveal consistent performance degradation in both personalization and privacy as context length increases. we further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity transformers. the empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. we release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. code and data are available at https://github.com/saferl-lab/paperbench">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15028v1" target="_blank" rel="noopener">Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Shangding Gu</p>
                <p class="paper-abstract" id="abstract-1">Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization...</p>
                <button class="expand-btn" onclick="toggleAbstract(1, 'Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench', 'Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization...')" id="expand-btn-1">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15028v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15028v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="rethinking diffusion models with symmetries through canonicalization with applications to molecular graph generation cai zhou zijie chen zian li jike wang kaiyi jiang pan li rose yu muhan zhang stephen bates tommi jaakkola many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). a common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. in this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. we then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. we instantiate the framework for molecular graph generation under $s_n \times se(3)$ symmetries. by leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3d molecule generation tasks, with similar or even less computation. moreover, with a novel architecture canon, canonflow achieves state-of-the-art performance on the challenging geom-drug dataset, and the advantage remains large in few-step generation.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15022v1" target="_blank" rel="noopener">Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span><span class="paper-category">math.GR</span>
                </div>
                <p class="paper-authors">Cai Zhou, Zijie Chen, Zian Li, Jike Wang, Kaiyi Jiang <em>(+5 more)</em></p>
                <p class="paper-abstract" id="abstract-2">Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through...</p>
                <button class="expand-btn" onclick="toggleAbstract(2, 'Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.', 'Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through...')" id="expand-btn-2">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15022v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15022v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="generalization from low- to moderate-resolution spectra with neural networks for stellar parameter estimation: a case study with desi xiaosheng zhao yuan-sen ting rosemary f. g. wyse alexander s. szalay yang huang lÃ¡szlÃ³ dobos tamÃ¡s budavÃ¡ri viska wei cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. we investigate this problem using pre-trained models, focusing on simple neural networks such as multilayer perceptrons (mlps), with a case study transferring from lamost low-resolution spectra (lrs) to desi medium-resolution spectra (mrs). specifically, we pre-train mlps on either lrs or their embeddings and fine-tune them for application to desi stellar spectra. we compare mlps trained directly on spectra with those trained on embeddings derived from transformer-based models (self-supervised foundation models pre-trained for multiple downstream tasks). we also evaluate different fine-tuning strategies, including residual-head adapters, lora, and full fine-tuning. we find that mlps pre-trained on lamost lrs achieve strong performance, even without fine-tuning, and that modest fine-tuning with desi spectra further improves the results. for iron abundance, embeddings from a transformer-based model yield advantages in the metal-rich ([fe/h] &gt; -1.0) regime, but underperform in the metal-poor regime compared to mlps trained directly on lrs. we also show that the optimal fine-tuning strategy depends on the specific stellar parameter under consideration. these results highlight that simple pre-trained mlps can provide competitive cross-survey generalization, while the role of spectral foundation models for cross-survey stellar parameter estimation requires further exploration.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15021v1" target="_blank" rel="noopener">Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">astro-ph.SR</span><span class="paper-category">astro-ph.GA</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Xiaosheng Zhao, Yuan-Sen Ting, Rosemary F. G. Wyse, Alexander S. Szalay, Yang Huang <em>(+3 more)</em></p>
                <p class="paper-abstract" id="abstract-3">Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. We investigate this problem...</p>
                <button class="expand-btn" onclick="toggleAbstract(3, 'Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. We investigate this problem using pre-trained models, focusing on simple neural networks such as multilayer perceptrons (MLPs), with a case study transferring from LAMOST low-resolution spectra (LRS) to DESI medium-resolution spectra (MRS). Specifically, we pre-train MLPs on either LRS or their embeddings and fine-tune them for application to DESI stellar spectra. We compare MLPs trained directly on spectra with those trained on embeddings derived from transformer-based models (self-supervised foundation models pre-trained for multiple downstream tasks). We also evaluate different fine-tuning strategies, including residual-head adapters, LoRA, and full fine-tuning. We find that MLPs pre-trained on LAMOST LRS achieve strong performance, even without fine-tuning, and that modest fine-tuning with DESI spectra further improves the results. For iron abundance, embeddings from a transformer-based model yield advantages in the metal-rich ([Fe/H] &gt; -1.0) regime, but underperform in the metal-poor regime compared to MLPs trained directly on LRS. We also show that the optimal fine-tuning strategy depends on the specific stellar parameter under consideration. These results highlight that simple pre-trained MLPs can provide competitive cross-survey generalization, while the role of spectral foundation models for cross-survey stellar parameter estimation requires further exploration.', 'Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. We investigate this problem...')" id="expand-btn-3">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15021v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15021v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="neurosim: a fast simulator for neuromorphic robot perception richeek das pratik chaudhari neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, rgb cameras, depth sensors, and inertial sensors. it can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. neurosim can achieve frame rates as high as ~2700 fps on a desktop gpu. neurosim integrates with a zeromq-based communication library called cortex to facilitate seamless integration with machine learning and robotics workflows. cortex provides a high-throughput, low-latency message-passing system for python and c++ applications, with native support for numpy arrays and pytorch tensors. this paper discusses the design philosophy behind neurosim and cortex. it demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. neurosim and cortex are available at https://github.com/grasp-lyrl/neurosim .">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15018v1" target="_blank" rel="noopener">Neurosim: A Fast Simulator for Neuromorphic Robot Perception</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.RO</span><span class="paper-category">cs.CV</span>
                </div>
                <p class="paper-authors">Richeek Das, Pratik Chaudhari</p>
                <p class="paper-abstract" id="abstract-4">Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of...</p>
                <button class="expand-btn" onclick="toggleAbstract(4, 'Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .', 'Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of...')" id="expand-btn-4">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15018v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15018v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="scaling beyond masked diffusion language models subham sekhar sahoo jean-marie lemercier zhihan yang justin deschenaux jingyu liu john thickstun ante jukic diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. among discrete diffusion approaches, masked diffusion currently dominates, largely driven by strong perplexity on language modeling benchmarks. in this work, we present the first scaling law study of uniform-state and interpolating discrete diffusion methods. we also show that masked diffusion models can be made approximately 12% more flops-efficient when trained with a simple cross-entropy objective. we find that perplexity is informative within a diffusion family but can be misleading across families, where models with worse likelihood scaling may be preferable due to faster and more practical sampling, as reflected by the speed-quality pareto frontier. these results challenge the view that masked diffusion is categorically the future of diffusion language modeling and that perplexity alone suffices for cross-algorithm comparison. scaling all methods to 1.7b parameters, we show that uniform-state diffusion remains competitive on likelihood-based benchmarks and outperforms autoregressive and masked diffusion models on gsm8k, despite worse validation perplexity. we provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/scaling-dllms">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15014v1" target="_blank" rel="noopener">Scaling Beyond Masked Diffusion Language Models</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Subham Sekhar Sahoo, Jean-Marie Lemercier, Zhihan Yang, Justin Deschenaux, Jingyu Liu <em>(+2 more)</em></p>
                <p class="paper-abstract" id="abstract-5">Diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. Among discrete diffusion approaches, Masked diffusion currently dominates,...</p>
                <button class="expand-btn" onclick="toggleAbstract(5, 'Diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. Among discrete diffusion approaches, Masked diffusion currently dominates, largely driven by strong perplexity on language modeling benchmarks. In this work, we present the first scaling law study of uniform-state and interpolating discrete diffusion methods. We also show that Masked diffusion models can be made approximately 12% more FLOPs-efficient when trained with a simple cross-entropy objective. We find that perplexity is informative within a diffusion family but can be misleading across families, where models with worse likelihood scaling may be preferable due to faster and more practical sampling, as reflected by the speed-quality Pareto frontier. These results challenge the view that Masked diffusion is categorically the future of diffusion language modeling and that perplexity alone suffices for cross-algorithm comparison. Scaling all methods to 1.7B parameters, we show that uniform-state diffusion remains competitive on likelihood-based benchmarks and outperforms autoregressive and Masked diffusion models on GSM8K, despite worse validation perplexity. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/scaling-dllms', 'Diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. Among discrete diffusion approaches, Masked diffusion currently dominates,...')" id="expand-btn-5">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15014v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15014v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="text style transfer with parameter-efficient llm finetuning and round-trip translation ruoxi liu philipp koehn this paper proposes a novel method for text style transfer (tst) based on parameter-efficient fine-tuning of large language models (llms). addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. this approach creates &#x27;neutralized&#x27; text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot icl techniques measured by bleu scores and style accuracy scores across four investigated domains. furthermore, the integration of retrieval-augmented generation (rag) for terminology and name knowledge enhances robustness and stylistic consistency.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15013v1" target="_blank" rel="noopener">Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.CL</span>
                </div>
                <p class="paper-authors">Ruoxi Liu, Philipp Koehn</p>
                <p class="paper-abstract" id="abstract-6">This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between...</p>
                <button class="expand-btn" onclick="toggleAbstract(6, 'This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. This approach creates &#x27;neutralized&#x27; text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. Experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot ICL techniques measured by BLEU scores and style accuracy scores across four investigated domains. Furthermore, the integration of retrieval-augmented generation (RAG) for terminology and name knowledge enhances robustness and stylistic consistency.', 'This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between...')" id="expand-btn-6">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15013v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15013v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="cold-start personalization via training-free priors from structured world models avinandan bose shuyue stella li faeze brahman pang wei koh simon shaolei du yulia tsvetkov maryam fazel lin xiao asli celikyilmaz cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. the core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. with a limited question budget, asking without structure will miss the dimensions that matter. reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. we propose decomposing cold-start elicitation into offline structure learning and online bayesian inference. pep (preference elicitation with priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. the framework is modular across downstream solvers and requires only simple belief models. across medical, mathematical, social, and commonsense reasoning, pep achieves 80.8% alignment between generated responses and users&#x27; stated preferences versus 68.5% for rl, with 3-5x fewer interactions. when two users give different answers to the same question, pep changes its follow-up 39-62% of the time versus 0-28% for rl. it does so with ~10k parameters versus 8b for rl, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15012v1" target="_blank" rel="noopener">Cold-Start Personalization via Training-Free Priors from Structured World Models</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Avinandan Bose, Shuyue Stella Li, Faeze Brahman, Pang Wei Koh, Simon Shaolei Du <em>(+4 more)</em></p>
                <p class="paper-abstract" id="abstract-7">Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens...</p>
                <button class="expand-btn" onclick="toggleAbstract(7, 'Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users&#x27; stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.', 'Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens...')" id="expand-btn-7">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15012v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15012v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="bpp: long-context robot imitation learning by focusing on key history frames max sobol mark jacky liang maria attarian chuyuan fu debidatta dwibedi dhruv shah aviral kumar many robot tasks require attending to the history of past observations. for example, finding an item in a room requires remembering which places have already been searched. however, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. we analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. motivated by these findings, we propose big picture policies (bpp), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. by projecting diverse rollouts onto a compact set of task-relevant events, bpp substantially reduces distribution shift between training and deployment, without sacrificing expressivity. we evaluate bpp on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. bpp achieves 70% higher success rates than the best comparison on real-world evaluations.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15010v1" target="_blank" rel="noopener">BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.RO</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Max Sobol Mark, Jacky Liang, Maria Attarian, Chuyuan Fu, Debidatta Dwibedi <em>(+2 more)</em></p>
                <p class="paper-abstract" id="abstract-8">Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the...</p>
                <button class="expand-btn" onclick="toggleAbstract(8, 'Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.', 'Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the...')" id="expand-btn-8">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15010v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15010v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="efficient sampling with discrete diffusion models: sharp and adaptive guarantees daniil dmitriev zhihan huang yuting wei diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. in this paper, we study the sampling efficiency of score-based discrete diffusion models under a continuous-time markov chain (ctmc) formulation, with a focus on $Ï„$-leaping-based samplers. we establish sharp convergence guarantees for attaining $\varepsilon$ accuracy in kullback-leibler (kl) divergence for both uniform and masking noising processes. for uniform discrete diffusion, we show that the $Ï„$-leaping algorithm achieves an iteration complexity of order $\tilde o(d/\varepsilon)$, with $d$ the ambient dimension of the target distribution, eliminating linear dependence on the vocabulary size $s$ and improving existing bounds by a factor of $d$; moreover, we establish a matching algorithmic lower bound showing that linear dependence on the ambient dimension is unavoidable in general. for masking discrete diffusion, we introduce a modified $Ï„$-leaping sampler whose convergence rate is governed by an intrinsic information-theoretic quantity, termed the effective total correlation, which is bounded by $d \log s$ but can be sublinear or even constant for structured data. as a consequence, the sampler provably adapts to low-dimensional structure without prior knowledge or algorithmic modification, yielding sublinear convergence rates for various practical examples (such as hidden markov models, image data, and random graphs). our analysis requires no boundedness or smoothness assumptions on the score estimator beyond control of the score entropy loss.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15008v1" target="_blank" rel="noopener">Efficient Sampling with Discrete Diffusion Models: Sharp and Adaptive Guarantees</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.IT</span><span class="paper-category">math.ST</span>
                </div>
                <p class="paper-authors">Daniil Dmitriev, Zhihan Huang, Yuting Wei</p>
                <p class="paper-abstract" id="abstract-9">Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of...</p>
                <button class="expand-btn" onclick="toggleAbstract(9, 'Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of score-based discrete diffusion models under a continuous-time Markov chain (CTMC) formulation, with a focus on $Ï„$-leaping-based samplers. We establish sharp convergence guarantees for attaining $\varepsilon$ accuracy in Kullback-Leibler (KL) divergence for both uniform and masking noising processes. For uniform discrete diffusion, we show that the $Ï„$-leaping algorithm achieves an iteration complexity of order $\tilde O(d/\varepsilon)$, with $d$ the ambient dimension of the target distribution, eliminating linear dependence on the vocabulary size $S$ and improving existing bounds by a factor of $d$; moreover, we establish a matching algorithmic lower bound showing that linear dependence on the ambient dimension is unavoidable in general. For masking discrete diffusion, we introduce a modified $Ï„$-leaping sampler whose convergence rate is governed by an intrinsic information-theoretic quantity, termed the effective total correlation, which is bounded by $d \log S$ but can be sublinear or even constant for structured data. As a consequence, the sampler provably adapts to low-dimensional structure without prior knowledge or algorithmic modification, yielding sublinear convergence rates for various practical examples (such as hidden Markov models, image data, and random graphs). Our analysis requires no boundedness or smoothness assumptions on the score estimator beyond control of the score entropy loss.', 'Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of...')" id="expand-btn-9">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15008v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15008v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="distributed quantum gaussian processes for multi-agent systems meet gandhi george p. kontoudis gaussian processes (gps) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. quantum computing offers the potential to overcome this limitation by embedding data into exponentially large hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. in this paper, we propose a distributed quantum gaussian process (dqgp) method in a multiagent setting to enhance modeling capabilities and scalability. to address the challenging non-euclidean optimization problem, we develop a distributed consensus riemannian alternating direction method of multipliers (dr-admm) algorithm that aggregates local agent models into a global model. we evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. we use real-world, non-stationary elevation datasets of nasa&#x27;s shuttle radar topography mission and synthetic datasets generated by quantum gaussian processes. beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in gaussian processes and distributed optimization.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15006v1" target="_blank" rel="noopener">Distributed Quantum Gaussian Processes for Multi-Agent Systems</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.MA</span><span class="paper-category">cs.LG</span><span class="paper-category">math.DG</span>
                </div>
                <p class="paper-authors">Meet Gandhi, George P. Kontoudis</p>
                <p class="paper-abstract" id="abstract-10">Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of...</p>
                <button class="expand-btn" onclick="toggleAbstract(10, 'Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. Quantum computing offers the potential to overcome this limitation by embedding data into exponentially large Hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. In this paper, we propose a Distributed Quantum Gaussian Process (DQGP) method in a multiagent setting to enhance modeling capabilities and scalability. To address the challenging non-Euclidean optimization problem, we develop a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model. We evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. We use real-world, non-stationary elevation datasets of NASA&#x27;s Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes. Beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization.', 'Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of...')" id="expand-btn-10">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15006v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15006v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="learning user interests via reasoning and distillation for cross-domain news recommendation mengdan zhu yufan zhao tao di yulan yan liang zhao news recommendation plays a critical role in online news platforms by helping users discover relevant content. cross-domain news recommendation further requires inferring user&#x27;s underlying information needs from heterogeneous signals that often extend beyond direct news consumption. a key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. in this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. we formulate query-list generation as a policy optimization problem and employ grpo with multiple reward signals. we systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. extensive offline experiments, ablation studies and large-scale online a/b tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15005v1" target="_blank" rel="noopener">Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.CL</span><span class="paper-category">cs.IR</span>
                </div>
                <p class="paper-authors">Mengdan Zhu, Yufan Zhao, Tao Di, Yulan Yan, Liang Zhao</p>
                <p class="paper-abstract" id="abstract-11">News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user&#x27;s underlying...</p>
                <button class="expand-btn" onclick="toggleAbstract(11, 'News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user&#x27;s underlying information needs from heterogeneous signals that often extend beyond direct news consumption. A key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. In this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. We formulate query-list generation as a policy optimization problem and employ GRPO with multiple reward signals. We systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. Finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. Extensive offline experiments, ablation studies and large-scale online A/B tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.', 'News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user&#x27;s underlying...')" id="expand-btn-11">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15005v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15005v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="pde foundation models are skillful ai weather emulators for the martian atmosphere johannes schmude sujit roy liping wang theodore van kessel levente klein marcus freitag eloisa bentivegna robert manson-sawko bjorn lutjens manil maskey campbell watson rahul ramachandran juan bernabe-moreno we show that ai foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the martian atmosphere. we base our work on the poseidon pde foundation model for two-dimensional systems. we develop a method to extend poseidon from two to three dimensions while keeping the pretraining information. moreover, we investigate the performance of the model in the presence of sparse initial conditions. our results make use of four martian years (approx.~34 gb) of training data and a median compute budget of 13 gpu hours. we find that the combination of pretraining and model extension yields a performance increase of 34.4\% on a held-out year. this shows that pdes-fms can not only approximate solutions to (other) pdes but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15004v1" target="_blank" rel="noopener">PDE foundation models are skillful AI weather emulators for the Martian atmosphere</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">physics.ao-ph</span>
                </div>
                <p class="paper-authors">Johannes Schmude, Sujit Roy, Liping Wang, Theodore van Kessel, Levente Klein <em>(+8 more)</em></p>
                <p class="paper-abstract" id="abstract-12">We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather...</p>
                <button class="expand-btn" onclick="toggleAbstract(12, 'We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.', 'We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather...')" id="expand-btn-12">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15004v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15004v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="boundary point jailbreaking of black-box llms xander davies giorgi giglemiani edmund lau eric winsor geoffrey irving yarin gal frontier llms are safeguarded against attempts to extract harmful information via adversarial prompts known as &quot;jailbreaks&quot;. recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. we introduce boundary point jailbreaking (bpj), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, bpj is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. to achieve this, bpj addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. instead of directly trying to learn an attack for a target harmful string, bpj converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength (&quot;boundary points&quot;). we believe bpj is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against constitutional classifiers, as well as the first automated attack algorithm that succeeds against gpt-5&#x27;s input classifier without relying on human attack seeds. bpj is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.15001v1" target="_blank" rel="noopener">Boundary Point Jailbreaking of Black-Box LLMs</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Xander Davies, Giorgi Giglemiani, Edmund Lau, Eric Winsor, Geoffrey Irving <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-13">Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as &quot;jailbreaks&quot;. Recently, defenders have developed classifier-based systems that have...</p>
                <button class="expand-btn" onclick="toggleAbstract(13, 'Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as &quot;jailbreaks&quot;. Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength (&quot;boundary points&quot;). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5&#x27;s input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.', 'Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as &quot;jailbreaks&quot;. Recently, defenders have developed classifier-based systems that have...')" id="expand-btn-13">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.15001v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.15001v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="spectral convolution on orbifolds for geometric deep learning tim mangliers bernhard mÃ¶ssner benjamin himpel geometric deep learning (gdl) deals with supervised learning on data domains that go beyond euclidean structure, such as data with graph or manifold structure. due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. there are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-euclidean data. in this paper, the concept of spectral convolution on orbifolds is introduced. this provides a building block for making learning on orbifold structured data accessible using gdl. the theory discussed is illustrated using an example from music theory.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.14997v1" target="_blank" rel="noopener">Spectral Convolution on Orbifolds for Geometric Deep Learning</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.LG</span><span class="paper-category">cs.AI</span>
                </div>
                <p class="paper-authors">Tim Mangliers, Bernhard MÃ¶ssner, Benjamin Himpel</p>
                <p class="paper-abstract" id="abstract-14">Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from...</p>
                <button class="expand-btn" onclick="toggleAbstract(14, 'Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.', 'Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from...')" id="expand-btn-14">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.14997v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.14997v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="thermeval: a structured benchmark for evaluation of vision-language models on thermal imagery ayush shrivastava kirtan gangani laksh jain mayank goel nipun batra vision language models (vlms) achieve strong performance on rgb imagery, but they do not generalize to thermal images. thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. unlike rgb imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing rgb-centric benchmarks do not evaluate. we introduce thermeval-b, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. thermeval-b integrates public datasets with our newly collected thermeval-d, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. evaluating 25 open-source and closed-source vlms, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. these results demonstrate that thermal understanding requires dedicated evaluation beyond rgb-centric assumptions, positioning thermeval as a benchmark to drive progress in thermal vision language modeling.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.14989v1" target="_blank" rel="noopener">ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.CV</span><span class="paper-category">cs.AI</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Ayush Shrivastava, Kirtan Gangani, Laksh Jain, Mayank Goel, Nipun Batra</p>
                <p class="paper-abstract" id="abstract-15">Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails,...</p>
                <button class="expand-btn" onclick="toggleAbstract(15, 'Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.', 'Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails,...')" id="expand-btn-15">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.14989v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.14989v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="orthogonalized multimodal contrastive learning with asymmetric masking for structured representations carolin cissee raneen younis zahra ahmadi multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. while self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. we introduce \textbf{coral}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. coral employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. to promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. extensive experiments on synthetic benchmarks and diverse multibench datasets demonstrate that coral consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. these results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.14983v1" target="_blank" rel="noopener">Orthogonalized Multimodal Contrastive Learning with Asymmetric Masking for Structured Representations</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Carolin Cissee, Raneen Younis, Zahra Ahmadi</p>
                <p class="paper-abstract" id="abstract-16">Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their...</p>
                <button class="expand-btn" onclick="toggleAbstract(16, 'Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. While self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. We introduce \textbf{COrAL}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. To promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. These results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.', 'Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their...')" id="expand-btn-16">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.14983v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.14983v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="block empirical likelihood inference for longitudinal generalized partially linear single-index models tianni zhang yuyao wang yu lu and mengfei ran generalized partially linear single-index models (gplsims) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. for repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. we propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (bel) for joint inference on the parametric coefficients and the single-index direction. the resulting bel ratio statistic enjoys a wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. we also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.14981v1" target="_blank" rel="noopener">Block Empirical Likelihood Inference for Longitudinal Generalized Partially Linear Single-Index Models</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">stat.ME</span><span class="paper-category">math.ST</span><span class="paper-category">stat.CO</span>
                </div>
                <p class="paper-authors">Tianni Zhang, Yuyao Wang, Yu Lu, and Mengfei Ran</p>
                <p class="paper-abstract" id="abstract-17">Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component...</p>
                <button class="expand-btn" onclick="toggleAbstract(17, 'Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. For repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. We propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (BEL) for joint inference on the parametric coefficients and the single-index direction. The resulting BEL ratio statistic enjoys a Wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. We also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. Simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance.', 'Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component...')" id="expand-btn-17">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.14981v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.14981v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="macroguide: topological guidance for macrocycle generation alicja maksymiuk alexandre duplessis michael bronstein alexander tong fernanda duarte iÌ‡smail iÌ‡lkan ceylan macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. despite their chemical value, they remain underexplored in generative modeling, likely owing to their scarcity in public datasets and the challenges of enforcing topological constraints in standard deep generative models. we introduce macroguide: topological guidance for macrocycle generation, a diffusion guidance mechanism that uses persistent homology to steer the sampling of pretrained molecular generative models toward the generation of macrocycles, in both unconditional and conditional (protein pocket) settings. at each denoising step, macroguide constructs a vietoris-rips complex from atomic positions and promotes ring formation by optimizing persistent homology features. empirically, applying macroguide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics such as chemical validity, diversity, and posebusters checks.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.14977v1" target="_blank" rel="noopener">MacroGuide: Topological Guidance for Macrocycle Generation</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">Alicja Maksymiuk, Alexandre Duplessis, Michael Bronstein, Alexander Tong, Fernanda Duarte <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-18">Macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. Despite their...</p>
                <button class="expand-btn" onclick="toggleAbstract(18, 'Macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. Despite their chemical value, they remain underexplored in generative modeling, likely owing to their scarcity in public datasets and the challenges of enforcing topological constraints in standard deep generative models. We introduce MacroGuide: Topological Guidance for Macrocycle Generation, a diffusion guidance mechanism that uses Persistent Homology to steer the sampling of pretrained molecular generative models toward the generation of macrocycles, in both unconditional and conditional (protein pocket) settings. At each denoising step, MacroGuide constructs a Vietoris-Rips complex from atomic positions and promotes ring formation by optimizing persistent homology features. Empirically, applying MacroGuide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics such as chemical validity, diversity, and PoseBusters checks.', 'Macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. Despite their...')" id="expand-btn-18">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.14977v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.14977v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

            <article class="paper-card" data-search="faster molecular dynamics with neural network potentials via distilled multiple time-stepping and non-conservative forces nicolaÃ¯ gouraud cÃ´me cattin thomas plÃ© olivier adjoua louis lagardÃ¨re jean-philip piquemal following our previous work (j. phys. chem. lett., 2026, 17, 5, 1288-1295), we propose the dmts-nc approach, a distilled multi-time-step (dmts) strategy using non conservative (nc) forces to further accelerate atomistic molecular dynamics simulations using foundation neural network models. there, a dual-level reversible reference system propagator algorithm (respa) formalism couples a target accurate conservative potential to a simplified distilled representation optimized for the production of non-conservative forces. despite being non-conservative, the distilled architecture is designed to enforce key physical priors, such as equivariance under rotation and cancellation of atomic force components. these choices facilitate the distillation process and therefore improve drastically the robustness of simulation, significantly limiting the &quot;holes&quot; in the simpler potential, thus achieving excellent agreement with the forces data. overall, the dmts-nc scheme is found to be more stable and efficient than its conservative counterpart with additional speedups reaching 15-30% over dmts. requiring no finetuning steps, it is easier to implement and can be pushed to the limit of the systems physical resonances to maintain accuracy while providing maximum efficiency. as for dmts, dmts-nc is applicable to any neural network potential.">
                <h2 class="paper-title">
                    <a href="https://arxiv.org/abs/2602.14975v1" target="_blank" rel="noopener">Faster Molecular Dynamics with Neural Network Potentials via Distilled Multiple Time-Stepping and Non-Conservative Forces</a>
                </h2>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… 2026-02-16</span>
                    <span class="paper-category">physics.chem-ph</span><span class="paper-category">cs.LG</span>
                </div>
                <p class="paper-authors">NicolaÃ¯ Gouraud, CÃ´me Cattin, Thomas PlÃ©, Olivier Adjoua, Louis LagardÃ¨re <em>(+1 more)</em></p>
                <p class="paper-abstract" id="abstract-19">Following our previous work (J. Phys. Chem. Lett., 2026, 17, 5, 1288-1295), we propose the DMTS-NC approach, a distilled multi-time-step (DMTS) strategy using non conservative (NC) forces to further...</p>
                <button class="expand-btn" onclick="toggleAbstract(19, 'Following our previous work (J. Phys. Chem. Lett., 2026, 17, 5, 1288-1295), we propose the DMTS-NC approach, a distilled multi-time-step (DMTS) strategy using non conservative (NC) forces to further accelerate atomistic molecular dynamics simulations using foundation neural network models. There, a dual-level reversible reference system propagator algorithm (RESPA) formalism couples a target accurate conservative potential to a simplified distilled representation optimized for the production of non-conservative forces. Despite being non-conservative, the distilled architecture is designed to enforce key physical priors, such as equivariance under rotation and cancellation of atomic force components. These choices facilitate the distillation process and therefore improve drastically the robustness of simulation, significantly limiting the &quot;holes&quot; in the simpler potential, thus achieving excellent agreement with the forces data. Overall, the DMTS-NC scheme is found to be more stable and efficient than its conservative counterpart with additional speedups reaching 15-30% over DMTS. Requiring no finetuning steps, it is easier to implement and can be pushed to the limit of the systems physical resonances to maintain accuracy while providing maximum efficiency. As for DMTS, DMTS-NC is applicable to any neural network potential.', 'Following our previous work (J. Phys. Chem. Lett., 2026, 17, 5, 1288-1295), we propose the DMTS-NC approach, a distilled multi-time-step (DMTS) strategy using non conservative (NC) forces to further...')" id="expand-btn-19">Show more â–¼</button>
                <div class="paper-actions">
                    <a href="https://arxiv.org/pdf/2602.14975v1" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                    <a href="https://arxiv.org/abs/2602.14975v1" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                </div>
            </article>

        </div>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2026 Coding Blog | BST 236 Computing I | Harvard University</p>
            <p class="footer-links">
                <a href="https://github.com">GitHub</a>
            </p>
        </div>
    </footer>

    <script src="js/main.js"></script>
    <script>
        function filterPapers() {
            const query = document.getElementById('searchInput').value.toLowerCase();
            const cards = document.querySelectorAll('.paper-card');
            
            cards.forEach(card => {
                const searchText = card.getAttribute('data-search');
                if (searchText.includes(query)) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });
        }
        
        function toggleAbstract(index, fullText, shortText) {
            const abstractEl = document.getElementById('abstract-' + index);
            const btnEl = document.getElementById('expand-btn-' + index);
            
            if (abstractEl.classList.contains('expanded')) {
                abstractEl.textContent = shortText;
                abstractEl.classList.remove('expanded');
                btnEl.textContent = 'Show more â–¼';
            } else {
                abstractEl.textContent = fullText;
                abstractEl.classList.add('expanded');
                btnEl.textContent = 'Show less â–²';
            }
        }
        
        // Client-side arXiv fetching for custom keywords
        // CORS proxies to try in order
        const corsProxies = [
            url => `https://corsproxy.io/?${encodeURIComponent(url)}`,
            url => `https://api.codetabs.com/v1/proxy?quest=${encodeURIComponent(url)}`,
            url => `https://api.allorigins.win/raw?url=${encodeURIComponent(url)}`
        ];
        
        async function tryFetchWithProxies(url, proxies) {
            for (let i = 0; i < proxies.length; i++) {
                const proxyUrl = proxies[i](url);
                try {
                    const response = await fetch(proxyUrl, { timeout: 10000 });
                    if (response.ok) {
                        return await response.text();
                    }
                } catch (e) {
                    console.log(`Proxy ${i + 1} failed:`, e.message);
                }
            }
            throw new Error('All CORS proxies failed. Please try again later or edit scripts/config.json directly.');
        }
        
        async function fetchCustomPapers() {
            const input = document.getElementById('customKeywords').value.trim();
            const btn = document.getElementById('fetchBtn');
            const grid = document.getElementById('papersGrid');
            const keywordsDisplay = document.getElementById('currentKeywords');
            
            if (!input) {
                alert('Please enter at least one keyword');
                return;
            }
            
            const keywords = input.split(',').map(k => k.trim()).filter(k => k);
            
            btn.disabled = true;
            btn.textContent = 'Fetching...';
            
            // Update displayed keywords
            keywordsDisplay.innerHTML = keywords.map(kw => 
                `<span class="keyword-tag">${escapeHtml(kw)}</span>`
            ).join('');
            
            // Build arXiv query
            const searchQuery = keywords.map(kw => `all:"${kw}"`).join(' OR ');
            const url = `https://export.arxiv.org/api/query?search_query=${encodeURIComponent(searchQuery)}&start=0&max_results=20&sortBy=submittedDate&sortOrder=descending`;
            
            try {
                const xmlText = await tryFetchWithProxies(url, corsProxies);
                
                // Parse XML
                const parser = new DOMParser();
                const xmlDoc = parser.parseFromString(xmlText, 'text/xml');
                const entries = xmlDoc.querySelectorAll('entry');
                
                if (entries.length === 0) {
                    grid.innerHTML = `
                        <div class="no-results" style="grid-column: 1 / -1;">
                            <h3>No papers found</h3>
                            <p>Try different keywords.</p>
                        </div>
                    `;
                } else {
                    let html = '';
                    entries.forEach((entry, i) => {
                        const title = entry.querySelector('title')?.textContent?.replace(/\s+/g, ' ').trim() || 'Untitled';
                        const abstract = entry.querySelector('summary')?.textContent?.replace(/\s+/g, ' ').trim() || 'No abstract';
                        const published = entry.querySelector('published')?.textContent?.substring(0, 10) || 'Unknown';
                        const id = entry.querySelector('id')?.textContent || '';
                        const pdfUrl = id.replace('/abs/', '/pdf/') + '.pdf';
                        
                        const authors = [];
                        entry.querySelectorAll('author name').forEach(n => authors.push(n.textContent));
                        const authorsStr = authors.length > 5 
                            ? authors.slice(0, 5).join(', ') + ` <em>(+${authors.length - 5} more)</em>`
                            : authors.join(', ');
                        
                        const categories = [];
                        entry.querySelectorAll('category').forEach(c => {
                            const term = c.getAttribute('term');
                            if (term && categories.length < 3) categories.push(term);
                        });
                        
                        const abstractShort = abstract.length > 200 
                            ? abstract.substring(0, 200).replace(/\s+\S*$/, '') + '...'
                            : abstract;
                        
                        html += `
                            <article class="paper-card" data-search="${escapeHtml(title.toLowerCase())} ${escapeHtml(authors.join(' ').toLowerCase())} ${escapeHtml(abstract.toLowerCase())}">
                                <h2 class="paper-title">
                                    <a href="${escapeHtml(id)}" target="_blank" rel="noopener">${escapeHtml(title)}</a>
                                </h2>
                                <div class="paper-meta">
                                    <span class="paper-date">ğŸ“… ${escapeHtml(published)}</span>
                                    ${categories.map(c => `<span class="paper-category">${escapeHtml(c)}</span>`).join('')}
                                </div>
                                <p class="paper-authors">${authorsStr}</p>
                                <p class="paper-abstract" id="abstract-dyn-${i}">${escapeHtml(abstractShort)}</p>
                                <button class="expand-btn" onclick="toggleAbstract('dyn-${i}', '${escapeHtml(abstract).replace(/'/g, "\\'")}', '${escapeHtml(abstractShort).replace(/'/g, "\\'")}')">Show more â–¼</button>
                                <div class="paper-actions">
                                    <a href="${escapeHtml(pdfUrl)}" class="pdf-btn" target="_blank" rel="noopener">ğŸ“„ View PDF</a>
                                    <a href="${escapeHtml(id)}" class="arxiv-btn" target="_blank" rel="noopener">ğŸ”— arXiv</a>
                                </div>
                            </article>
                        `;
                    });
                    grid.innerHTML = html;
                }
                
                // Update timestamp
                const now = new Date().toISOString().replace('T', ' ').substring(0, 19) + ' (live fetch)';
                document.querySelector('.update-info').textContent = 'Last updated: ' + now;
                
            } catch (error) {
                console.error('Fetch error:', error);
                grid.innerHTML = `
                    <div class="no-results" style="grid-column: 1 / -1;">
                        <h3>Error fetching papers</h3>
                        <p>${escapeHtml(error.message)}</p>
                        <p style="margin-top: 15px; font-size: 0.9rem;">
                            <strong>Alternative:</strong> Edit <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">scripts/config.json</code> 
                            and run <code style="background: #334155; padding: 2px 6px; border-radius: 4px;">python scripts/fetch_arxiv.py</code> locally, 
                            or push to GitHub to trigger the auto-update.
                        </p>
                    </div>
                `;
            }
            
            btn.disabled = false;
            btn.textContent = 'Fetch Papers';
        }
        
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }
        
        // Allow Enter key to trigger fetch
        document.getElementById('customKeywords').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') fetchCustomPapers();
        });
    </script>
</body>
</html>
